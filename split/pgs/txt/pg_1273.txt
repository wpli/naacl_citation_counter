Negatives Asthma pranayama, sterilizing, block, yougurt, medcine, expectorants, inhalable, exertion, hate, virally sanitizers, ayurvedic Diabetes quinoa, vinegars, vegat- nicely, chiropracter, exables, threadmill, pos- hales, paralytic, metabosilbe, asanas, omegas lize, fluffy

Positives

A. Carlson, J. Betteridge, R. C. Wang, Jr. E. R. Hruschka, and T. M. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Web Search and Data Mining (WSDM), pages 101­110. L. Chiticariu, Y. Li, and F. R. Reiss. 2013. Rule-based information extraction is dead! long live rule-based information extraction systems! In Empirical Methods in Natural Language Processing (EMNLP), pages 827­832. M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Empirical Methods in Natural Language Processing (EMNLP). O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91­134. A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying relations for open information extraction. In Empirical Methods in Natural Language Processing (EMNLP). S. Gupta and C. D. Manning. 2014. Improved pattern learning for bootstrapped entity extraction. In Computational Natural Language Learning (CoNLL). A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In North American Association for Computational Linguistics (NAACL), pages 320­327. M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Interational Conference on Computational linguistics, pages 539­545. P. Liang. 2005. Semi-supervised learning for natural language. Master's thesis, Massachusetts Institute of Technology. T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space. Technical Report 1301.3781, arXiv. T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS). J. Pennington, R. Socher, and C. D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Computational Natural Language Learning (CoNLL). E. Riloff. 1996. Automatically generating extraction patterns from untagged text. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1044­1049.

Table 2: Examples of unlabeled entities that were expanded into the training sets. Gray colored entities were judged by the authors as falsely labeled. using a large corpus perform better than those trained using the smaller in-domain corpus. For the Acne forum, where brand name DT entities are more frequent, the entities expanded by MedHelp vectors had fewer false positives than those expanded by Wiki+Twit+MedHelp. Table 2 shows some examples of unlabeled entities that were included as positive/negative entities in the entity classifiers. Even though some entities were included in the training data with wrong labels, overall the classifiers benefited from the expansion.

7

Conclusion

We improve entity classifiers in bootstrapped entity extraction systems by enhancing the training set using unsupervised distributed representations of words. The classifiers learned using the expanded seed sets extract entities with better F1 score. This supports our hypothesis that generalizing labels to entities that are similar according to unsupervised methods of word vector learning is effective in improving entity classifiers, notwithstanding that the label generalization is quite noisy.

References
S. Abney. 2004. Understanding the Yarowsky algorithm. Computational Linguistics, 30:365­395. A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Conference on Learning Theory (COLT). P. F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467­ 479.

1219

