testing competing segment-level metrics, at: https://github.com/ygraham/ segment-mteval

2

WMT-style Evaluation of Segment-level MT Metrics

Since 2008, the WMT workshop series has included a shared task for automatic metrics, and as with the translation shared task, human evaluation remains the official gold standard for evaluation. In order to minimize the amount of annotation work and enforce consistency between the primary shared tasks in WMT, the same evaluations are used to evaluate MT systems in the shared translation task, as well as MT evaluation metrics in the document-level metrics and segment-level metrics tasks. Although WMT have trialled several methods of human evaluation over the years, the prevailing method takes the form of ranking a set of five competing translations for a single source language (SL) input segment from best to worst. A total of ten pairwise human relative preference judgments can be extracted from each set of five translations. Performance of a segment-level metric is assessed by the degree to which it corresponds with human judgment, measured by the number of metric scores for pairs of translations that are either concordant (Con ) or discordant (Dis ) with those of a human assessor, which the organizers describe as "Kendall's  ":  = |Con | - |Dis | |Con | + |Dis |

of five competing translations and not a single ranking of all translations, it is not possible to compute a single Kendall's  correlation.1 The formula used to assess the performance of a metric in the task, therefore, is not what is ordinarily understood to be a Kendall's  coefficient, but, in fact, equivalent to a weighted average of all Kendall's  for each humanranked set of five translations. A more significant problem, however, lies in the inconsistency of human relative preference judgments within data sets. Since overall scores for metrics are described as correlations, possible values achievable by any metric could be expected to lie in the range [-1, 1] (or "±1"). This is not the case, and achievements of metrics are obscured by contradictory human judgments. Before any metric has provided scores for segments, for example, the maximum and minimum correlation achievable by a participating metric can be computed as, in the case of W MT-13: · Russian-to-English: ±0.92 · Spanish-to-English: ±0.90 · French-to-English: ±0.90 · German-to-English: ±0.92 · Czech-to-English: ±0.89 · English-to-Russian: ±0.90 · English-to-Spanish: ±0.90 · English-to-French: ±0.91 · English-to-German: ±0.90 · English-to-Czech: ±0.87 If we are interested in the relative performance of metrics and take a closer look at the formula used to contribute a score to metrics, we can effectively ignore the denominator (|Con | + |Dis |), as it is constant for all metrics. The numerator (|Con | - |Dis |) is what determines our evaluation of the relative performance of metrics, and although the formula appears to be a straightforward subtraction of counts of concordant and discordant pairs, due to the large numbers of contradictory human relative preference judgments in data sets, what this number actually represents is not immediately obvious. If, for example, translations A and B were scored by a metric such that metric score(A) > metric score(B ), one
This would in fact require all (|MT systems| × |distinct segments|) translations included in the evaluation to be placed in a single rank order.
1

Pairs of translations deemed equally good by a human assessor are omitted from evaluation of segment-level metrics (Bojar et al., 2014). There is a mismatch between the human judgments data used to evaluate segment-level metrics and the standard conditions under which Kendall's  is applied, however: Kendall's  is used to measure the association between a set of observations of a single pair of joint random variables, X (e.g. the human rank of a translation) and Y (e.g. the metric score for the same translation). A conventional application of Kendall's  would be comparison of all pairs of values within X with each corresponding pair within Y . Since the human assessment data is, however, a large number of separately ranked sets 1184

