Model PM'13 (S) FCM (S) LRFCM (S) BASELINE (ST) FCM (ST) LRFCM (ST)

ACE-bc (|L|=11) P R F1 55.3 43.1 48.5 62.3 45.1 52.3 58.5 46.8 52.0 72.2 52.0 60.5 66.2 54.2 59.6 65.1 54.7 59.4

ACE-bc (|L|=32) P R F1 59.7 41.6 49.0 57.4 46.2 51.2 60.2 51.2 55.3 62.9 49.6 55.4 63.5 51.1 56.6

ERE (|L|=9) P R F1 68.3 52.6 59.4 65.1 56.1 60.3 76.2 64.0 69.5 73.0 65.4 69.0 75.0 65.7 70.0

ERE (|L|=18) P R F1 67.1 51.5 58.2 65.4 55.3 59.9 73.5 62.1 67.3 74.0 60.1 66.3 73.2 63.2 67.8

Table 1: Results on test for ACE and ERE where only the entity spans (S) are known (top) and where both the entity
spans and types are known (ST). PM'13 is an embedding method. The sizes of relation sets are indicated by |L|.

of LRFCM to scale and generalize. As is standard, we report precision, recall, and F1 for all tasks. ACE 2005 We use the English portion of the ACE 2005 corpus (Walker et al., 2006). Following Plank and Moschitti (2013), we train on the union of the news domains (Newswire and Broadcast News), hold out half of the Broadcast Conversation (bc) domain as development data, and evaluate on the remainder of bc. There are 11 coarse types and 32 fine (sub-)type classes in total. In order to compare with traditional feature-based methods (Sun et al., 2011), we report results in which the gold entity spans and types are available at both train and test time. We train the models with all pairs of entity mentions in the training set to yield 43,518 classification instances. Furthermore, for comparison with prior work on embeddings for relation extraction (Plank and Moschitti, 2013), we report results using gold entity spans but no types, and generate negative relation instances from all pairs of entities within each sentence with three or fewer intervening entities. ERE We use the third release of the ERE annotations from Phase 1 of DEFT (LDC, 2013) . We divided the proxy reports summarizing news articles (pr) into training (56,889 relations), development (6,804 relations) and test data (6,911 relations). We run experiments under both the settings with and without gold entity types, while generating negative relation instances just as in ACE with the gold entity types setting. To the best of our knowledge, we are the first to report results on this task. Following the annotation guidelines of ERE relations, we treat all relations, except for "social.business", "social.family" and "social.unspecified", as asymmetric relations. For 1377

coarse relation task, we treat all relations as asymmetric, including the "social" relation. The reason is that the asymmetric subtype, "social.role", dominates the class: 679 of 834 total "social" relations. Setup We randomly initialize the feature embeddings Wf and pre-train 200-dimensional word embeddings on the NYT portion of Gigaword 5.0 (Parker et al., 2011) with word2vec (default setting of the toolkit) (Mikolov et al., 2013). Dependency parses are obtained from the Stanford Parser (De Marneffe et al., 2006). We use the same feature templates as Yu et al. (2014). When gold entity types are unavailable, we replace them with WordNet tags annotated by Ciaramita and Altun (2006). Learning rates, weights of L2-regularizations, the number of iterations and the size of the feature embeddings d are tuned on dev sets. We selected d from {12, 15, 20, 25, 30, 40}. We used d=30 for feature embeddings for fine-grained ACE without gold types, and d=20 otherwise. For ERE, we have d=15. The weights of L2  was selected from {1e3, 5e-4, 1e-4}. As in prior work (Yu et al., 2014), regularization did not significantly help FCM. However for LRFCM, =1e-4 slightly helps. We use a learning rate of 0.05. We compare to two baselines. First, we use the features of Sun et al. (2011), who build on Zhou et al. (2005) with additional highly tuned features for ACE-style relation extraction from years of research. We implement these in a logistic regression model BASELINE, excluding country gazetteer and WordNet features. This baseline includes gold entity types and represents a high quality feature rich model. Second, we include results from Plank and Moschitti (2013) (PM'13), who obtained improve-

