been translated with the subject pronouns still absent. This is partially due to the casual nature of the text, and partially because the quality (fluency) of English translations in this data is at times dubious. While there may be a systematicity to this kind of subject omission on the English side, this was not a factor taken into account by our human annotators. So while our own model may stand a chance at predicting "hidden" labels (no overt pronoun on English side) in such instances, the annotators will never assign a label of "none" to a location at which a pronoun could reasonably have been inserted. 4.3 Overall system efficacy In this section we discuss the overall efficacy of our proposed method in comparison to a few alternatives. These alternatives are: Random guessing baseline. A na¨ ive system that makes predictions uniformly at random. Subject continuation baseline. This is a rulebased approach that mimics the intuitions described in Section 2. In particular, for a Chinese utterance, we check whether the current utterance has an overt pronominal subject. If so, we assign a label of 1v, 2v or 3v depending on the person of this subject. If the current utterance has a non-pronominal subject, we assign 3h. Otherwise we "carry forward" the subject from the previous utterance, flipping the 1p/2p as necessary when the speaker changes; these are labeled as 1h, 2h or 3h. Minimal model baseline. In the minimal model, we restrict our model to use just three features: participant index, participant switch and subject continuation feature. This is a machine learning variant of the rule-based subject continuation baseline. Oracle upper bound. None of the proposed models can hope to achieve 100% accuracy on this task because the gold annotation data consists of 26% "no pronoun" cases. Since all of our approaches must predict a pronoun when a zero pronoun has been identified, their performance (namely, their precision) is upper-bounded away from 100%. The summary of results (micro-averaged across 1p, 2p and 3p) are shown in Table 5. These results show that on both the SMS data (on which the model was developed) and the OntoNotes data (on which 500

System Random Minimal SubjCont Full Model Upper Bound

SMS/chat Micro-average Pre Rec F 0.24 0.25 0.24 0.42 0.23 0.30 0.32 0.42 0.36 0.59 0.31 0.41 0.74 1.00 0.85

OntoNotes Micro-average Pre Rec F 0.18 0.26 0.22 0.30 0.07 0.11 0.31 0.15 0.20 0.30 0.36 0.33 0.55 1.00 0.71

Table 5: Summary of results for different comparator models against the gold standard labels from SMS data (left) and OntoNotes (right).

the model was applied blindly), our full model is able to substantially outperform the baselines. In fact, on OntoNotes, despite a potential domain mismatch (from SMS/chat to telephone conversations), our full model was the only baseline to beat random guessing! Across both data sets, the minimal model tends to have high precision and low recall; the behavior of the other approaches varies across the tasks. On the SMS/chat data, our model achieves a 14% relative improvement over the best baseline; on an OntoNotes data, a 50% relative improvement. More specific breakdowns of performance by different pronouns (1p, 2p and 3p) are shown for the subject continuation baseline and the full model in Table 4. In these tables, we also report results when evaluated on the OntoNotes test set in these Tables. As we can see, the subject continuation baseline massively overpredicts third person pronouns in the SMS data, leading to an overall low score. In comparison, our model tends to have much higher precision (at the expense of recall) across the board on the SMS data, leading to a 14% relative improvement over the subject continuation baseline. Since, to our knowledge, no prior work (see Section 5) has focused on deictic pronoun restoration, it is not possible to directly compare our results to previously published results. Although it is an apples-to-oranges comparison, a state-of-theart anaphoric zero pronoun resolution system (Chen and Ng, 2014) achieves a precision of 13.3, a recall of 32.2 and an F-measure of 18.8 on the telephone conversation part of the OntoNotes data, but does so addressing the complementary problem of correctly choosing antecedents from previous overt noun phrases. Another reasonable comparison would be with a

