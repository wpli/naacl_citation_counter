3.1

Spoken Language Features

ASR transcription errors pose a challenge for standard text processing features, which rely on textual similarity to measure relatedness of both context and entity mention strings. However, ASR errors are not random; incorrectly decoded words may be phonetically similar to the original spoken words. This suggests that similarity can still be captured by considering phonetic similarity. We experiment with four feature types.  Text: Our baseline system uses features based on the text of the mention string and document. We used the feature set presented by McNamee et al. (2012) and used in Benton et al. (2014), which was the best performing submission in the TAC-KBP 2009 entity linking task. These features include, among others, features that compare the candidate entity name to the mention string as well as the document's terms to those stored in the candidate's description in the KB. These include the dice coefficient, cosine similarity (boolean and weighted), and proportion of candidate tokens in the query document.  Phone: Words in the document, mention string as well as the knowledge base are represented as phone sequences instead of text. We convert all words to phones using a grapheme string to phone (G2P) system.  Metaphones: Two distinct phones can sound similar, yet still appear different when matching phones. Metaphones (Philips, 1990), a more recent version of Soundex, map similar sounding phones to the same representation. We convert the phones used in the previous paragraph to metaphones.  Lattice: Expected word counts of the query document from the ASR lattice. Extracted unigrams are treated as a weighted bag-of-words for the query document. We compute all the features that use the query document's content and weigh them by the term's expectation. The features in each of the above sets depend on their representation of the text (e.g. text, phone, metaphone, lattice). Additionally, we include the following features in all experiments: Bias features that fire for all candidates, only non-NIL candidates, 227

and only NIL candidates; NIL features indicative of being linked to no article in the knowledge base such as the mention string is only 1 or 2 characters/tokens, the number of candidates emitted by the triager; and the Popularity (number of Wikipedia in-links) of the candidate. Triage The above feature sets change the ranking of candidates. We also modified the triage methods that produce Cq based on these new features. For Text we used the triage methods of Mcnamee et al. (2009): string similarity based on character/token n-gram overlap, same acronym, exact match, etc. Phone triage used the same heuristics but based on phone representations of the mention strings and candidate names. Metaphone triage worked as in phone, but used metaphones. When two representations are used we take the union of their candidates. G2P System For phone features we use a G2P system based on the Sequitur G2P grapheme-phoneconverter (Bisani and Ney, 2008). We trained the system on version 0.7a of CMUdict1 (stress omitted from phone sequences, case-insensitive for graphemes), by predicting the phoneme sequence of an English token given its string (G2P). The language model was a 9-gram model with modified Kneser-Ney smoothing applied. For Phone features we converted each token to its best phone representation, where each phone, as well as diphthong, is represented by a single character for similarity matching.

4

Experiments

We evaluate reference transcripts and output from two ASR systems run on our dataset (HUB4). We use Kaldi (Povey et al., 2011) trained on the spoken version of the Wall Street Journal corpus (Paul and Baker, 1992).2 The first system (mono) relies on an HMM whose hidden states are context-independent phones. The second system (tri4b) uses an HMM that outputs phones dependent on their immediate left and right contexts. These systems respectively achieve 70.6% and 50.7% WER over our training set, where high error rates are likely due to a shift in domain from primarily financial news to the wider
1 2

http://www.speech.cs.cmu.edu/cgi-bin/cmudict Linguistic Data Consortium number LDC94S13A.

