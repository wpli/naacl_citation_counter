els (S) treat nested pseudo-terms with varying levels of synonymy. Unstructured models (U ) treat nested pseudo-terms as independent. Selective models retain only a subset (1 or n) of the pseudo-terms from each nested region; inclusive models retain them all (a). Finally, weighted models (W ) include a heuristic adjustment to give some pseudo-terms (in our experiments, longer ones) greater influence; unweighted models treat each pseudo-term in the same manner. Table 1 illustrates the weights given to each term by each of the retrieval models defined below. Unweighted models implicitly take a binary approach to term weighting--with unweighted selective models omitting many pseudo-terms--while structured and weighted models yield real values between zero and one. Note that both weighted and unweighted models reward term repetition (term frequency) and term specificity (inverse collection frequency). 4.2 Bag-of-Words Baseline (Ua) Our first set of experiments had three goals: 1) to serve as a dry run for system development, as we had no prior experience with indexing or ranked retrieval based on pseudo-terms; 2) to gain experience with performing relevance judgments using only the audio responses; and 3) to understand the feasibility of speech retrieval based on pseudo-terms. For these initial experiments, each pseudo-term was treated as a "word" in a bag-of-words representation (coded Ua). No consideration was given to term length or nesting. Although this set of runs was largely exploratory, it provided a good baseline for comparison to other methods considered. 4.3 Terms as Synonyms (Sa, U1) Moving beyond the bag of words method of term selection involves various forms of term analysis within an overlapping region. The first family of methods treats terms in each overlapping group as synonymous. Aside from being straightforward, treating terms as unweighted synonyms has been a successful technique in cross-language IR. There are generally two methods that can be used in such cases. The first is to treat all overlapping pseudoterms as synonyms of a single term. This is accomplished in Indri by placing each pseudo-term in an overlapping region within the syn operator. This 593

Retrieval Model P. Term P21 P20 P19 P18 P17 P16 P15 P14 P13 P12 P11 P10 P9 P8 P7 P6 P5 P4 P3 P2 P1 Ua 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 Sa 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 U1 Un 1.00 UaW SaW 0.45 0.43 0.48 0.36 0.45 0.53 0.48 0.37 0.48 0.36 0.41 0.43 0.54 0.45 0.39 0.37 0.40 0.41 0.47 0.40 0.46 0.45 0.22 0.48 0.36 0.06 0.53 0.11 0.12 0.22 0.02 0.22 0.24 0.54 0.45 0.04 0.03 0.13 0.08 0.47 0.22 0.46

1.00

1.00

1.00

Table 1: Weights assigned to pseudo-terms in Figure 3 by each retrieval model (zero values shown as blank). model is coded Sa. One risk with the Sa model is that including shorter terms may add more noise than signal. Another method of dealing with alternatives in the cross-language IR literature is to somehow select a single term from the set. For our experiments with this technique, only the longest pseudo-term from an overlapping set is retained; all other ("nested") pseudo-terms are simply deleted from the query. The thinking behind this is that the longest term should contain the greatest amount of information. This method is coded U1. 4.4 Length Measure of Specificity (UaW, SaW)

The U1 and Sa models are two extremes on a spectrum of possibilities; thus, models in which some pseudo-terms receive less weight, rather than being ignored entirely, were also explored. Care must be

