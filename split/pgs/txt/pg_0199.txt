tion label is the lemmatized version of the head verb of the recipe step. We look at all chunked noun entities in the step which are the direct object of the action (either directly or via the preposition "of", as in "Add a cup of flour"). We canonicalize these entities by computing their similarity to the list of ingredients associated with this recipe. If an ingredient is sufficiently similar, that ingredient is added to this step's entity list. Otherwise, the stemmed entity is used. For example, consider the step "Mix tomato sauce and pasta"; if the recipe has a known ingredient called "spaghetti", we would label the action as "mix" and the entities as "tomato sauce" and "spaghetti", because of its high semantic similarity to "pasta". (Semantic similarity is estimated based on Euclidean distance between word embedding vectors computed using the method of (Mikolov et al., 2013) trained on general web text.) In many cases, the direct object of a transitive verb is elided (not explicitly stated); this is known as the "zero anaphora" problem. For example, the text may say "Add eggs and flour to the bowl. Mix well.". The object of the verb "mix" is clearly the stuff that was just added to the bowl (namely the eggs and flour), although this is not explicitly stated. To handle this, we use a simple recency heuristic, and insert the entities from the previous step to the current step. 2.3 Processing the speech transcript

Figure 1: Graphical model representation of the factored
HMM. See text for details.

of error, we also collected a much smaller set of 480 cooking videos (with corresponding recipe text) for which the video creator had uploaded a manually curated speech transcript; this has no transcription errors, it contains sentence boundary markers, and it also aligns whole phrases with the video (instead of just single tokens). We applied the same NLP pipeline to these manual transcripts. In the results section, we will see that the accuracy of our end-toend system is indeed higher when the speech transcript is error-free and well-formed. However, we can still get good results using noisier, automatically produced transcripts.

3

Methods

In this section, we describe our system for aligning instructional text and video. 3.1 HMM to align recipe with ASR transcript We align each step of the recipe to a corresponding sequence of words in the ASR transcript by using the input-output HMM shown in Figure 1. Here X (1 : K ) represents the textual recipe steps (obtained using the process described in Section 2.2); Y (1 : T ) represent the ASR tokens (spoken words); R(t)  {1, . . . , K } is the recipe step number for frame t; and B (t)  {0, 1} represents whether timestep t is generated by the background (B = 1) or foreground model (B = 0). This background variable is needed since sometimes sequences of spoken words are unrelated to the content of the recipe, especially at the beginning and end of a video. The conditional probability distributions (CPDs) for the Markov chain is as follows: 
p(R(t) = r|R(t - 1) = r ) p(B (t) = b|B (t - 1) = b) = =   1-  0.0 if r = r +1 if r = r otherwise

The output of Youtube's ASR system is a sequence of time-stamped tokens, produced by a standard Viterbi decoding system. We concatenate these tokens into a single long document, and then apply our NLP pipeline to it. Note that, in addition to errors introduced by the ASR system2 , the NLP system can introduce additional errors, because it does not work well on text that may be ungrammatical and which is entirely devoid of punctuation and sentence boundary markers. To assess the impact of these combined sources
2 According to (Liao et al., 2013), the Youtube ASR system we used, based on using Gaussian mixture models for the acoustic model, has a word error rate of about 52% (averaged over all English-language videos; some genres, such as news, had lower error rates). The newer system, which uses deep neural nets for the acoustic model, has an average WER of 44%; however, this was not available to us at the time we did our experiments.

.

145

