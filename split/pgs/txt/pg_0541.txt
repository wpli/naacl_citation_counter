Figure 1: Comparison of CBOW and skip-gram for clustering of adjectives.

port a best F1 score of 48% but note that such score does not seem to reflect the quality of the clustering. We observe the same problem: our automatically derived clusters have a different organization for words that belong to the same cluster than the gold standard clusters, but in a way that seems intuitive. Some differences between our automatically derived clusters and the gold standard clusters are illustrated in Table 1. For example, the adjectives false and misleading belong to the same cluster in both the gold standard as well as the automatic clustering output. However, the automatic clustering groups the adjectives false and misleading together with unreliable and wrong, whereas the gold standard groups false and misleading with deceptive and fraudulent. Both clusterings are plausible, though. The adjectives fraudulent and deceptive become part of new clusters in our automatic clustering. It could be argued that the gold standard cluster "deceptive, false, fraudulent, misleading" represents different degrees of "trickery," whereas the automatic cluster "false, misleading, unreliable, wrong" represent different degrees of "wrongness." Thus, although both clusters contain different adjectives, they group adjectives that are on the same scale of a different meaning. Therefore, to evaluate the quality of the automatic clustering, we sampled 50 clusters containing three or more adjectives (corresponding to a total of 190 adjectives) from all the generated clusters and obtained annotations using Amazon Mechanical Turk 487

(AMT), a crowdsourcing platform that has been shown useful for a number of NLP tasks (Snow et al., 2008). Annotators (workers, in AMT parlance) were presented with 15 clusters in each worker session, whose members were each associated with a checkbox. For each cluster, workers had to uncheck the adjectives that did not belong to the same scale. The nature of the annotation task does involve inherent subjectivity which cannot be avoided. We tried to minimize this by giving detailed instructions with accompanying examples to achieve coherent annotations. To make sure workers were paying attention to the task, 2 clusters among the 15 clusters they saw were clusters for which we a priori knew which adjectives should be removed (e.g., beautiful, pretty, and rainy where rainy had to be unchecked). Most workers did the task well: we only had to discard annotations from 4 worker sessions (out of 140). We ended up with annotations from 8 to 10 workers per cluster. To create a gold standard, we retained in each cluster only those words that were ascertained to be in the same cluster by 6 or more annotators. For each cluster, we calculated an accuracy score equivalent to the number of correct adjectives (determined to be on the same scale by the annotators) divided by the total number of adjectives in the generated cluster. This accuracy was averaged across all 50 clusters, and yielded a final micro-averaged accuracy of 74.36% as seen in Table 2. 4.2 Ranking Since our end goal is to establish an ordering among scalar adjectives, we use the automatically derived clusters (rather than the WordNet dumbbells) as input to the MILP algorithm. To determine the performance of the ranking produced by the MILP algorithm, we use AMT to obtain pairwise ranking annotations for all unique adjective pairs within a cluster. Workers were presented with 15 word pairs in each worker session. For each pair (a1, a2), the worker had to pick one of four options: (1) a1 is stronger than a2, (2) a2 is stronger than a1, (3) both are equally strong, and (4) a1 and a2 are not comparable. Option (4) was present because our clusters possibly contained adjectives that are not on the same scale. As in the previous task for getting annotations for clusters, we inserted two items with a clear ranking (e.g., hot, hotter) for every set of 15

