Unsupervised Dependency Parsing: Let's Use Supervised Parsers
Phong Le and Willem Zuidema Institute for Logic, Language, and Computation University of Amsterdam, the Netherlands {p.le,zuidema}@uva.nl

Abstract
We present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms. Our approach, called `iterated reranking' (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees. Our system achieves 1.8% accuracy higher than the stateof-the-part parser of Spitkovsky et al. (2013) on the WSJ corpus.

1

Introduction

Unsupervised dependency parsing and its supervised counterpart have many characteristics in common: they take as input raw sentences, produce dependency structures as output, and often use the same evaluation metric (DDA, or UAS, the percentage of tokens for which the system predicts the correct head). Unsurprisingly, there has been much more research on supervised parsing ­ producing a wealth of models, datasets and training techniques ­ than on unsupervised parsing, which is more difficult, much less accurate and generally uses very simple probability models. Surprisingly, however, there have been no reported attempts to reuse supervised approaches to tackle the unsupervised parsing problem (an idea briefly mentioned in Spitkovsky et al. (2010b)). There are, nevertheless, two aspects of supervised parsers that we would like to exploit in an unsupervised setting. First, we can increase the model ex651

pressiveness in order to capture more linguistic regularities. Many recent supervised parsers use thirdorder (or higher order) features (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014) to reach state-of-the-art (SOTA) performance. In contrast, existing models for unsupervised parsing limit themselves to using simple features (e.g., conditioning on heads and valency variables) in order to reduce the computational cost, to identify consistent patterns in data (Naseem, 2014, page 23), and to avoid overfitting (Blunsom and Cohn, 2010). Although this makes learning easier and more efficient, the disadvantage is that many useful linguistic regularities are missed: an upper bound on the performance of such simple models ­ estimated by using annotated data ­ is 76.3% on the WSJ corpus (Spitkovsky et al., 2013), compared to over 93% actual performance of the SOTA supervised parsers. Second, we would like to make use of information available from lexical semantics, as in Bansal et al. (2014), Le and Zuidema (2014), and Chen and Manning (2014). Lexical semantics is a source for handling rare words and syntactic ambiguities. For instance, if a parser can identify that "he" is a dependent of "walks" in the sentence "He walks", then, even if "she" and "runs" do not appear in the training data, the parser may still be able to recognize that "she" should be a dependent of "runs" in the sentence "she runs". Similarly, a parser can make use of the fact that "sauce" and "John" have very different meanings to decide that they have different heads in the two phrases "ate spaghetti with sauce" and "ate spaghetti with John". However, applying existing supervised parsing

Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 651­661, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

