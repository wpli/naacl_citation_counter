semantics are created. For instance, given an input example (1)
NL: mr1 : mr2 : mr3 : purple eight kicks to purple seven playmode(play on) pass(purple8, purple7) pass(purple2, purple5)

the following alignment might be created:
NL mr nl  ref X1 kicks to X2 pass(ARG1 , ARG2 ) Mapping: X1  ARG1 , X2  ARG2 purple eight  purple8 purple seven  purple7

(2)

That is, assuming that the algorithm has learned in step 1 that "purple eight" and "purple seven" refer to purpl8 and purple7, respectively, it may use this knowledge to hypothesize that NL is an instantiation of a pattern "X1 kicks to X2 ", which in turn refers to the predicate pass(ARG1 , ARG2 ) with a mapping X1  ARG1 , X2  ARG2 . Alignments are rated, and for each example only those having maximal scores are used for parser induction. By this, lexical knowledge is used to predisambiguate the training data. Given the alignments induced in step 3, a parser is estimated by computing co-occurrence frequencies at different levels. In particular, association scores are computed at three different levels, i.e. 1. nl  ref : between all lexical units, e.g. "purple eight", and semantic referents, e.g. purple8, appearing in alignments, 2. NL  mr: between all syntactic patterns, e.g. "X1 kicks to X2 ", and semantic frames, e.g. pass(ARG1 , ARG2 ), appearing in alignments, and 3. mapping: between all slots in a syntactic pattern, e.g. X1 , and argument slots, e.g. ARG1 , specific for each pattern and semantic frame. Then, the parser's lexicon consists of rules of the form nl  ref , while the syntactic constructions have the form NL  mr, each coupled with its individual mapping. Parsing is performed by searching for an appropriate syntactic construction given an input NL, and the arguments matching the elements at the slots in

the syntactic pattern are inserted into the appropriate argument slots in the associated semantic frame. Approximate matching can be applied during parsing of NLs for which no pattern can be found otherwise. In this paper, we always perform approximate matching by searching for a matching syntactic pattern with a Levenshtein distance of 1 if no matching pattern can be found directly, which allows us to parse utterances containing a recognition error. Even though ­ of course ­ more than one recognition error might be contained in a given utterance, we do not use greater distance values because this would likely yield parsing errors, as utterances are rather short and most of the words are important for detecting the meaning; leaving out too many words will in general increase the likelihood of matching wrong patterns, thus yielding spurious interpretations. Using the algorithm, for each fold of the RoboCup data set we created a semantic parser using the written training data of three games. 3.3 Creation of language models Based on the written training data we created different LMs. In particular, we created rule-based recognition grammars using the algorithm and further LMs, such as trigram models, for comparison. 3.3.1 Recognition grammars We built semantic speech recognition grammars given a semantic parser by transforming all rules with an occurrence greater than one into JSpeech Grammar Format (JSGF)2 . The resulting grammars consisted of rules representing the parser's inventory of syntactic constructions as well as its lexicon. In case of the inventory of syntactic constructions, alternative expansions of learned syntactic patterns were defined, and in case of the lexicon, alternative expansions of learned lexical units were defined. In particular, with respect to the lexicon we defined a rule <ref> which comprises the learned lexical units. With respect to syntactic constructions we defined a rule <utterance> which comprises the patterns. Further, syntactic slots in patterns were replaced by <ref>, allowing lexical units to appear at those positions. In grammar creation, we also investigated the influence of occurrence frequencies of
2

http://www.w3.org/TR/jsgf/

876

