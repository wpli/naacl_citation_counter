(T3) do these women talk to each other about something besides a man? seems to be one that linguistic features should be able to answer. A closer analysis suggests why this may be the case. Consider the screenplay excerpt in Figure 1 (on the next page). This excerpt is from the movie Hannah and Her Sisters, which passes the Bechdel test. Even though the conversation between named women Mickey and Gail mentions a man (He), the conversation is not about a man. The conversation is about Mickey's brain tumor. Now consider the following (contrived) conversation between the same characters: Mickey: Ssssss, if i'm in love, I don't know what I'm gonna do. Gail: You're not in love. Didn't he tell you that it was over. Mickey: No, naturally This conversation is clearly about a man (or being in love with a man). Much like the original conversation, this conversation mentions a man only once. The linguistic phenomena that allows us to infer that this contrived conversation is about a man is quite complex; it requires a deeper semantic analysis and world knowledge. First, we need to infer that it being over refers to a relationship. Relationships typically have two participants. In order to identify the participants, we need to use world knowledge that relationships can end and that the person ending the relationship was once part of the relationship, and so on. Eventually, we are able to conclude that one of the main participants of the conversation or the event being discussed is a man. As a first attempt to automate the test, we only experiment with basic linguistic features. However, we believe that the task itself offers an opportunity for the development of-- and subsequent evaluation of-- rich linguistic features that may be better equipped for determining the aboutness of conversations. The rest of the paper is structured as follows. Section 2 reviews the related literature. Section 3 introduces the terminology regarding movie screenplays that we use throughout the paper. Section 4 describes the data and gold standard used for the purposes of automating the test. Sections 5, 6, and 7 present our approach, evaluation and results for the 831

three Bechdel tests, respectively. We conclude and present future direction for research in Section 8.

2

Related

There has been much work in the computational sciences community on studying gender differences in the way language is used by men versus women (Peersman et al., 2011; Mohammad and Yang, 2011; Bamman et al., 2012; Schwartz et al., 2013; Bamman et al., 2014; Prabhakaran et al., 2014). In fact, researchers have proposed linguistic features for supervised classifiers that predict the gender of authors given their written text (Koppel et al., 2002; Corney et al., 2002; Cheng et al., 2011). There has also been a growth in research that utilizes computational techniques and big data for quantifying existing gender biases in society (Sugimoto et al., 2013; Garcia et al., 2014; Wagner et al., 2015). More closely related to our application is the ongoing work in the social sciences community regarding the study of gender biases in movie scripts and books (Weitzman et al., 1972; Clark et al., 2003; Gooden and Gooden, 2001; McCabe et al., 2011; Chick and Corle, 2012; Smith et al., 2013). This work has largely depended on manual effort. McCabe et al. (2011) analyzed the presence of male and female characters in titles, and their centralities, in 5,618 children's books. The authors employed multiple human coders for obtaining the relevant annotations. Smith et al. (2013) employed 71 research assistants to evaluate 600 films to study gender prevalence in their scripts. Our work offers computational techniques that may help reduce the manual effort involved in carrying out similar social science studies. Recently, Garcia et al. (2014) used 213 movie screenplays for evaluating the correlation of two novel scores with whether or not movies passed the Bechdel test. However, the main focus of their work was not to automate the test. The focus of their work was to study gender biases in MySpace and Twitter (using these scores). Nonetheless, we experiment with these scores and in fact they provide a strong baseline for automating the task. Furthermore, we use our previous work (Agarwal et al., 2014b) to clean noisy screenplays found on the web and carry out the study on a larger data-set of 457 screenplays.

