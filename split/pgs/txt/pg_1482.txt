Embedding a Semantic Network in a Word Space
~ Richard Johansson and Luis Nieto Pina Spr° akbanken, Department of Swedish, University of Gothenburg Box 200, SE-40530 Gothenburg, Sweden {richard.johansson, luis.nieto.pina}@svenska.gu.se

Abstract
We present a framework for using continuousspace vector representations of word meaning to derive new vectors representing the meaning of senses listed in a semantic network. It is a post-processing approach that can be applied to several types of word vector representations. It uses two ideas: first, that vectors for polysemous words can be decomposed into a convex combination of sense vectors; secondly, that the vector for a sense is kept similar to those of its neighbors in the network. This leads to a constrained optimization problem, and we present an approximation for the case when the distance function is the squared Euclidean. We applied this algorithm on a Swedish semantic network, and we evaluate the quality of the resulting sense representations extrinsically by showing that they give large improvements when used in a classifier that creates lexical units for FrameNet frames.

while the latter has seen much interest lately, their respective strengths and weaknesses are still being debated (Baroni et al., 2014; Levy and Goldberg, 2014). The most important relation defined in a vector space between the meaning of two words is similarity: a mouse is something quite similar to a rat. Similarity of meaning is operationalized in terms of geometry, by defining a distance metric. Symbolic representations seem to have an advantage in describing word sense ambiguity: when a surface form corresponds to more than one concept. For instance, the word mouse can refer to a rodent or an electronic device. Vector-space representations typically represent surface forms only, which makes it hard to search e.g. for a group of words similar to the rodent sense of mouse or to reliably use the vectors in classifiers that rely on the semantics of the word. There have been several attempts to create vectors representing senses, most of them based on some variant of the idea first proposed by Sch¨ utze (1998): that senses can be seen as clusters of similar contexts. Recent examples in this tradition include the work by Huang et al. (2012) and Neelakantan et al. (2014). However, because sense distributions are often highly imbalanced, it is not clear that context clusters can be reliably created for senses that occur rarely. These approaches also lack interpretability: if we are interested in the rodent sense of mouse, which of the vectors should we use? In this work, we instead derive sense vectors by embedding the graph structure of a semantic network in the word space. By combining two complementary sources of information ­ corpus statistics and network structure ­ we derive useful vectors also for concepts that occur rarely. The method, which can be applied to context-counting as well as context-predicting spaces, works by decompos-

1

Introduction

Representing word meaning computationally is central in natural language processing. Manual, knowledge-based approaches to meaning representation maps word strings to symbolic concepts, which can be described using any knowledge representation framework; using the relations between concepts defined in the knowledge base, we can infer implicit facts from the information stated in a text: a mouse is a rodent, so it has prominent teeth. Conversely, data-driven meaning representation approaches rely on cooccurrence patterns to derive a vector representation (Turney and Pantel, 2010). There are two classes of methods that compute word vectors: context-counting and context-predicting; 1428

Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1428­1433, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

