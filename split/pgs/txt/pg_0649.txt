swered. To this end, farmers would call into an Interactive Voice Response (IVR) system and peruse answers to existing questions, or would pose their own questions for the community. Other farmers would call into the system to leave answers to those questions. On occasion, there were also a small group of system administrators who would periodically call in to leave announcements that they expected would be of interest to the broader farming community. The system was completely automated--no human intervention or call center was involved. Avaj Otalo's recorded speech was divided into 50 queries and 2,999 responses. Queries were statements on a particular topic, sometimes phrased as a question, sometimes phrased as an announcement. Responses were sometimes answers to questions, sometimes they were related announcements, and sometimes they were questions on a similar topic. This represented approximately two-thirds of the total audio present in the system. Very short recordings were omitted, as were those in which little speech activity was automatically detected. The average length of a query is approximately 70 seconds (SD = 14.40s), or approximately 61 seconds (SD = 15.76s) after automated silence removal. Raw response lengths averaged 110 seconds (SD = 88.80s), and 96.52 seconds (SD = 82.75s) after silence was removed. 5.1 Relevance Judgments and Evaluation Pools for judgment were formed by combining the results from every system reported in our results section below, along with several other systems that yielded less interesting results that we omit for space reasons. Three native speakers of Gujarati performed relevance assessment; none of the three had any role in system development. Relevance assessment was performed by listening to the audio and making a graded relevance judgment. Assessors could assign one of the following judgments for each response: 1) unable to assess, 2) not relevant, 3) relevant, and 4) highly relevant. For evaluation measures that require binary judgments, and for computing inter-annotator agreement, the relevance judgments were subsequently binarized by removing all the unassessable cases. Highly relevant and relevant responses were then collapsed into a single relevant category. To com595

Retrieval Model U1 MRR Un Ua UaW Sa SaW 0.447 0.281 0.169 0.204 0.235 0.432 0.139 0.071 0.081 0.089 0.242 0.075 0.188 0.104 0.109 0.193 0.252 0.105 0.106 0.057 0.047 0.060 0.058 0.111 0.023 0.011 0.015 0.018 0.050 0.010 0.045 0.013 0.018 0.050 0.058 0.022 0.237 0.216 0.206 0.219 0.214 0.284 0.122 0.098 0.187 0.195 0.243 0.194 0.142 0.089 0.219 0.191 0.285 0.230

MAP

NDCG

Table 2: Results for pure (top), medium (middle) and noisy (bottom) clustering for the 10 queries for which more than one relevant response is known. Shaded cells are best-performers, per measure; starred values indicate NDCG or MAP is significantly better or worse than same-row Ua (two-sided paired t-test, p < 0.05). pute NDCG, relevant and highly relevant categories were assigned the scores 1 and 2, respectively, while non-relevant judgments retained a score of 0. Three rounds of relevance assessments were conducted as query models were developed and assessor agreement was characterized.

6

Results

Each retrieval model was run for each of the three clustering results. For each method, there were three metrics of interest: normalized discounted cumulative gain (NDCG), mean reciprocal rank (MRR), and mean average precision (MAP). Results are outlined in Table 2. To limit the effect of quantization noise on the evaluation measures, results are reported for queries having three or more relevant documents. There were a total of 10 such queries, having a total of 61 relevant documents and yielding an average of 6.10 documents per query (SD = 2.13). Low baselines for each evaluation were established--as there were none in prior existence-- by randomly sampling 60 documents from the test collection. For each of the six randomly selected topics, 10 of the 60 randomly selected documents were add to the judgment pool without replacement.

