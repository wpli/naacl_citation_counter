Figure 5: Precision (top) and recall (bottom) over binned HEAD distance of iterated reranking (IR) and its initializer (MS) on the training sentences in phase 1 ( 15 words). 2013), the parser used in the previous experiments, (ii) GGGPT (Gillenwater et al., 2011)7 employing an extension of the DMV model and posterior regularization framework for training, and (iii) Harmonic, the harmonic initializer proposed by Klein and Manning (2004). Figure 4 shows DDAs of phase 1 (MaxEnc) on training sentences up to length 15 with three starting-points given by those parsers. Starting point is clearly very important to the performance of the iterated reranking: the better the starting point is, the higher performance phase 1 has. However, a remarkable point here is that the iterated reranking of phase 1 always finds out more useful patterns for parsing whatever the starting point is in this experiment. It is certainly due to the high order features and lexical semantics, which are not exploited in those parsers. The contribution of Iterated Reranking We compare the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by the initialier Marecek and Straka (2013). Figure 5 shows precision (top) and recall (bottom)
7

over binned HEAD distance. IR helps to improve the precision on all distance bins, especially on the bins corresponding to long distances ( 3). The recall is also improved, except on the bin corresponding to  7 (but the F1-score on this bin is increased). We attribute this improvement to the -order model which uses very large fragments as contexts thus be able to capture long dependencies. Figure 6 shows the correct-head accuracies over POS-tags. IR helps to improve the accuracies over almost all POS-tags, particularly nouns (e.g. NN, NNP, NNS), verbs (e.g. VBD, VBZ, VBN, VBG) and adjectives (e.g. JJ, JJR). However, as being affected by the initializer, IR performs poorly on conjunction (CC) and modal auxiliary (MD). For instance, in the treebank given by the initializer, almost all modal auxilaries are dependents of their verbs instead of the other way around.

7 Discussion
Our system is different from the other systems shown in Table 1 as it uses an extremely expressive model, the -order generative model, in which conditioning contexts are very large fragments. Only the work of Blunsom and Cohn (2010), whose resulting grammar rules can contain large tree fragments, shares this property. The difference is that their work needs a pre-defined prior, namely hierarchical non-parametric Pitman-Yor process prior, to avoid large, rare fragments and for smoothing. The IORNN of our system, in contrast, does that automatically. It learns by itself how to deal with distant conditioning nodes, which are often less informative than close conditioning nodes on computing P (x|C  (u)). In addition, smoothing is given free: recursive neural nets are able to map `similar' fragments onto close points (Socher et al., 2010) thus an unseen fragment tends to be mapped onto a point close to points corresponding to `similar' seen fragments. Another difference is that our system exploits lexical semantics via word embeddings, which were learnt unsupervisedly. By initialising the IORNN with these embeddings, the use of this knowledge turns out easy and transparent. Spitkovsky et al. (2013) also exploit lexical semantics but in a limited way, using a context-based polysemous unsuper-

code.google.com/p/pr-toolkit

658

