Figure 2: An example of a commercial real estate flyer and manually entered listing information c ProMaker Commercial Real Estate LLC, c BrokerSavant Inc.

try staff employed by the listing service7 . Figure 2 shows an example of a real estate flyer and the corresponding manually entered listing data. To generate a dataset for the text categorization tasks we assigned the list of manually entered labels for transaction and property types to each flyer. For example, the flyer from Figure 2 was assigned to transaction type sale and property type industrial. To generate annotated data for the NER task, we had to convert the stand-alone listing information to annotated text in which each occurrence of the field values was marked with the corresponding entity type via string matching. The manually entered listing data, however, introduced some text variations and did not always match the text in the corresponding flyer. For example, the same street and intersection address could be expressed in a variety of ways (e.g. `Westpark Drive and Main Street' vs `Westpark Dr & Main St'; `123 North Main Road' vs `123 N Main', etc.). Similarly, broker names, phones, and company names could have a variety of alternative representations (e.g. `Michael R. Smith' vs `Mike Smith CCIM'; `Lee and Associates' vs `Lee & Associates of IL LLC'; `773-777-0000 ext 102' vs `773.777.0000x102', etc.). Lastly, space size information was always entered in square feet, while at the same time it could be expressed as both square feet and acres (with various precision points) in the corresponding flyer (e.g. 53796 sf, 1.235 acres, 1.23 acres, etc.). To account for the various ways in which an attribute value can be expressed in the text we hand7

crafted a small set of rules and regular expressions that allowed us to find most of its alternative representations. In some cases, however, the listing value was not found in the corresponding flyer text. In the case of such a discrepancy, the flyer was simply discarded from the training set used for the corresponding named entity type. Such discrepancies could occur for several reasons. In some cases, the manually hand-crafted rules and regular expressions did not cover all possible variants in which the value could be expressed. On occasion, the text containing the attribute value was in image format (inside embedded images). We also noted a few instances of incorrectly entered manual data. As a result, only a portion of the training data (a total of 1,361 flyers) was used for the training of individual named entity types. We were able to automatically annotate 878 flyers used for training the address named entity recognizer (street or intersection, city, state, zip), 1145 flyers used for training the broker information named entity recognizer (broker name, phone, email, company) and 1242 flyers for training the space named entity recognizer (size and space type). 3.3 Data Pre-processing

As mentioned earlier, all flyers (PDF and HTML) were converted to a common format (HTML). An HTML parser was then used to create a text representation of the flyer. The text was tokenized and tokens were normalized (all tokens were converted to lower case and digits were converted to a common format). As noted previously, data entry staff were able to quickly spot listing attributes of interest solely because of their visual characteristics. To account for such visual characteristics we included typographic and other visual features associated with tokens or text chunks for both the text categorization and NER tasks. Typographic and visual features were based on the computed HTML style attributes for each DOM element containing text. Computing the HTML style attributes is a complex task since they are typically defined by a combination of CSS8 files, in-lined HTML style attributes, and browser defaults. The complexities of style definition, inheritance, and overwriting are handled by
8

c BrokerSavant Inc.

Cascading Style Sheets.

286

