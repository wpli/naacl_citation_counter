tion that not all keywords are of equal importance, so a good summariser should favour sentences covering the most important ones. Intuitively, the keyword profile of a paper containing valuable information on words' salience in characterising the paper's main contributions should be utilised to drive such a discriminative sentence selection process. Based on the previous considerations, we use a paper's keyword profile to build a discriminative unigram language model that directly encodes words' salience as pseudo generative probabilities to facilitate the seamless incorporation of such information into a generic probabilistic framework. More specifically, we directly translate words' salience (in the form of p-values) into a discriminative unigram language model of a paper that assigns high probabilities to its characterising keywords. The pseudo generative probability of word W according to a paper D's keyword profile language model Mkp is:
P (W |Mkp )=
1 Z

a highly adaptive data driven term weighting framework. For brevity, from now on, we use KPLM to refer to keyword profile language model.
Word W1 W2 W3 W4 W5 Salience S (W ) 0.01 0.10 0.50 0.99 1.00 Pseudo count log(S (W )) 4.61 2.30 0.69 0.01 0.00 P (W |Mkp ) 0.605 0.303 0.091 0.001 0.000

Table 3: Keyword profile language model built for an imaginary document consists of only 5 distinct words.

log(S (W ))

(3)

where S (W ) denotes the salience of word W in characterising paper D calculated using (2), and Z is a normalisation factor. An intuitive interpretation of (3) is to deem log(S (W )) a pseudo word count of W , where more salient words have higher pseudo counts; this makes Z the total length of the pseudo document generated from the paper's keyword profile. We disregard actual word counts to make the keyword profile language model directly encode words' salience. Also, in the previous step, keyword profiling had already implicitly taken such information into account, providing another justification for this design decision. Table 3 shows a miniature example to illustrate how a keyword profile language model is built. In this example, W5 is automatically eliminated from the resulting language model because it has lowest salience in characterising the imaginary document. Any word S with salience value S (W ) close to but strictly less than 1.0 would still have a tiny pseudo probability in the resulting keyword profile language model (e.g., W4 ). Words with low salience are not necessarily stop words (e.g., W4 and W5 ), and neither is the reverse true: a content word can possibly be used across the document collection and thus evaluate to a very low salience (and so have a nul or low pseudo generative probability in the resulting keyword profile language model) for the document under consideration. For example, "parsing" would have a low salience for any paper in a collection on Dependency Parsing. It can be seen that our method amounts to 127

Although implicitly conveyed in the formulation of KPLM above, it should be made clear that the KPLM is a pseudo language model that encodes words' salience in the form of pseudo generative probabilities, which functions as a language model, yet should not be interpreted as a true language model under the traditional definition. A traditional unigram language model is constructed using the actual term frequencies in the document, the resulting model capturing generative probabilities. In contrast, the KPLM of a document is built using pseudo term frequencies that directly encode words' salience in characterising a document's contents, measured using a sophisticated quantitative statistical procedure. It can thus be interpreted as a probabilistic description of the document's keywords with significantly boosted discriminative power. Having clarified the nature of KPLM, we treat it as a language model in the rest of the paper. 3.3 KPLM Based Summarisation 3.3.1 Sentence Selection The KPLM of a paper is a discriminative generative model that incorporates words' salience in characterising a paper's main contributions. It thus represents an effective language model from which a model citing sentence covering the paper's main contributions could be sampled from3 . So by measuring the statistical surprise between the realistic language model estimated from each citing sentence with the KPLM of a paper, we can select the set of citing sentences that conform best to the optimal model given by the the KPLM and build a summary that well captures keywords. More specifically, we adopt the negative cross entropy retrieval model (Zhai, 2008), use the KPLM of a paper as the
A pseudo citing sentence sampled from KPLM in this manner would simply be a bag of words, not a grammatical sentence. So here "model" has the favour of keywords coverage.
3

