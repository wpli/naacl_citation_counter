all cases. Meanwhile, given in advance the domain information for a relatively small sample of 2.5% of the heterogeneous data, the results are mixed. We obtain a good performance/slightly better performance/worse performance with the case of Hardware/Legal/Pharmacy respectively. What do domain-conditioned statistics look like? To have an idea what the induced statistics look like, we investigate their conditional entropy. Here, we present the conditional entropy for the domain-confused/-conditioned word translation statistics induced from the HMM alignment model/its latent domain model. Note that similar results are observed for transition tables.
Model Baseline Prior Hardware Latent Legal Pharmacy Statistics Domain-confused D1 -conditioned D2 -conditioned D1 -conditioned D2 -conditioned D1 -conditioned D2 -conditioned H (F| E) 1348.53 1124.43 1354.58 1104.58 1385.35 1115.52 1342.54

statistics simultaneously. Under this setting, we obtain good results, as described in Table 1. For the two cases with the training corpora of 2M and 4M sentence pairs respectively, learning with the combining domain prior knowledge produces the best word alignment accuracy compared to the rest. In the last case with the training corpus of 1M sentence pairs, learning with the combining domain prior knowledge produces compatible with the case of Hardware, i.e., the best binary domain case. Table 1 also reveals that the performance of our model approaches Model 4, even though Model 4 is much more complex and computationally expensive. Domain-conditioned statistics combination We also investigate the relation between the number of domain-conditioned statistics "involved" in the Viterbi decoding (Eq. 5) and the word alignment accuracy. Table 3 presents the results in case of using only the induced D1 -/, D2 -/, D3 -/, D4 conditioned statistics separately, and also using their different combinations. Interestingly, we observe that using more domain-conditioned statistics for decoding incrementally improves the word alignment accuracy over the heterogeneous data. While the domain-conditioned statistics are very different in their characteristics from each other, the results reveal how they are complementary to the others, conveying a mix of domains for each sentence pair.
Decoding's Statistics Hard Decision (ref.) D1 (Pharmacy) D2 (Legal) D3 (Hardware) D4 (OUT) D1 + D2 D1 + D2 + D3 D1 + D2 + D3 + D4 Prec. 68.49 64.78 66.54 66.98 68.46 66.80 68.54 69.64 Rec. 62.80 59.86 61.15 61.36 63.01 61.72 62.80 63.30 AER 34.48 37.78 36.27 35.95 34.38 35.84 34.46 33.68

Table 2: Conditional entropy of the statistics. Formally, for a translation table, F, E , its conditional entropy, H (F | E ) can be estimated from its possible word pairs, e, f : H (F | E ) = - e P (e) f P (f | e) log P (f | e). Table 2 reveals that the induced D1 -conditioned statistics need much less bits to represent than the induced domain-confused statistics, e.g., 1124.43, 1104.58, 1115.52 vs. 1348.53. This implies the induced D1 conditioned statistics are much more predictable compared to the domain-confused statistics. Meanwhile, the induced D2 -conditioned statistics are similar to the domain-confused statistics in terms of the conditional entropy, e.g., 1354.58, 1385.35, 1342.54 vs. 1348.53. 7.2 Learning with Multiple Domains

It would be more interesting to learn the latent domain alignment model for multiple domains, rather than learning with each of them separately. In detail, using all the seed samples from different domains, we aim to learn four different domain-conditioned 404

Table 3: Domain-conditioned statistics combination for Viterbi decoding. The reported results are for the heterogeneous corpus of 1M sentence pairs. Similar results are observed for other training data. Finally, it is also tempting to make a comparison between the hard vs. soft domain assignment in Viterbi decoding. Here, for hard domain decision we simply do decoding with the following objec-

