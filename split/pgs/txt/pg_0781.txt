correlated depends on which topic they appear in. There are several works utilizing knowledge with more complex structure to improve topic modeling. Boyd-Graber et al. (2007) incorporate the synset structure in WordNet (Miller, 1995b) into LDA for word sense disambiguation, where each topic is a random process defined over the synsets. Hu et al. (2011) proposed interactive topic modeling, which allows users to iteratively refine the discovered topics by adding constraints such as certain set of words must appear together in the same topic. Andrzejewski et al. (2011) proposed a general framework which uses first order logic to encode various domain knowledge regarding documents, topics and side information into LDA. The vast generality and expressivity of this model makes its inference to be very hard. Chen et al. (2013) proposed a topic model to model multi-domain knowledge, where each document is an admixture of latent topics and each topic is a probability distribution over domain knowledge. Jagarlamudi et al. (2012) proposed to guide topic modeling by setting a set of seed words in the beginning that user believes could represent certain topics. While these knowledge are rich in structure, they are hard to acquire in the real world applications. In this paper, we focus on pairwise word correlation knowledge which are widely attainable in many scenarios. In the domain of computer vision, the idea of using MRF to enforce topical coherence between neighboring patches or superpixels has been exploited by several works. Verbeek and Triggs (2007) proposed Markov field aspect model where each image patch is modeled using PLSA (Hofmann, 1999) and a Potts model is imposed on the hidden topic layer to enforce spatial coherence. Zhao et al. (2010) proposed topic random field model where each superpixel is modeled using a combination of LDA and mixture of Gaussian model and a Potts model is defined on the topic layer to encourage neighboring superpixels to share the same topic. Similarly, Zhu and Xing (2010) proposed a conditional topic random field to incorporate features about words and documents into topic modeling. In their model, the MRF is restricted to be a linear chain, which can only capture the dependencies between neighboring words and is unable to incorporate long range word correlations. Different from these works, the MRF in our model is not restricted to Potts or chain struc727

ture. Instead, its structure is decided by the word correlation knowledge and can be arbitrary.

3

Markov Random Field Regularized Latent Dirichlet Allocation

In this section, we present the MRF-LDA model and the variational inference technique. 3.1 MRF-LDA

We propose the MRF-LDA model to incorporate word similarities into topic modeling. As shown in Figure 1, MRF-LDA extends the standard LDA model by imposing a Markov Random Field on the latent topic layer. Similar to LDA, we assume a document possesses a topic proportion vector  sampled from a Dirichlet distribution. Each topic  k is a multinomial distribution over words. Each word w has a topic label z indicating which topic w belongs to. In many scenarios, we have access to external knowledge regarding the correlations between words, such as apple and orange are similar, church and bible are semantically related. These similarity relationships among words can be leveraged to improve the coherence of learned topics. To do this, we define a Markov Random Field over the latent topic layer. Given a document d containing N words {wi }N i=1 , we examine each word pair (wi , wj ). If they are correlated according to the external knowledge, we create an undirected edge between their topic labels (zi , zj ). In the end, we obtain an undirected graph G where the nodes are latent topic labels {zi }N i=1 and edges connect topic labels of correlated words. In the example shown in Figure 1, G contains five nodes z1 , z2 , z3 , z4 , z5 and four edges connecting (z1 , z3 ), (z2 , z5 ), (z3 , z4 ), (z3 , z5 ). Given the undirected graph G, we can turn it into a Markov Random Field by defining unary potentials over nodes and binary potentials over edges. We define the unary potential for zi as p(zi | ), which is a multinomial distribution parameterized by  . In standard LDA, this is how a topic is sampled from the topic proportion vector. For binary potential, with the goal to encourage similar words to have similar topic assignments, we define the edge potential between (zi , zj ) as exp{I(zi = zj )}, where I(·) is the indicator function. This potential func-

