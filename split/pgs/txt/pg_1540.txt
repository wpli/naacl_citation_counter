closely as possible2 . All datasets are built around the core idea of collecting pairs consisting of a persuasive sentence (P ) and a non-persuasive counterpart (¬P ), where P and ¬P are structurally very similar and controlled for the above mentioned confounding factors. Twitter. A set of 11,404 tweet pairs, where each pair comes from the same user (author control) and contains the same URL (topic control). P and ¬P are determined based on their retweet counts (Tan et al., 2014). It is worth noting that in our experiments we were able to collect only 11,019 of such tweet pairs since some of them were deleted in the meanwhile. Movie. A set of 2,198 single-sentence memorable movie quotes (P ) paired with non-memorable quotes (¬P ). For each P , the dataset contains a contrasting quote ¬P from the same movie such that (i) P and ¬P are uttered by the same speaker, (ii) P and ¬P have the same number of words, (iii) ¬P does not occur in the IMDb list of memorable quotes and (iv) P and ¬P are as close as possible to each other in the script (Danescu-Niculescu-Mizil et al., 2012). CORPS. A set of 2,600 sentence pairs uttered by various politicians. We collected these pairs from CORPS, a freely available corpus of political speeches tagged with audience reactions (Guerini et al., 2013). The methodology that we used to build the pairs is very similar to Danescu-NiculescuMizil et al. (2012): for each P , where P is the sentence preceding an audience reaction (e.g. APPLAUSE, LAUGHTER), we selected a contrasting single-sentence ¬P from the same speech. We required ¬P to be close to P in the speech transcription, subject to the conditions that (i) P and ¬P are uttered by the same speaker - which is trivial since these are monologues, where a single speaker is addressing the audience - (ii) P and ¬P have the same number of words, and (iii) ¬P is 5 to 15 sentences away from P . This last condition had to be imposed since, differently from movie quotes, we do not have the evidence of which fragment of the speech exactly provoked the audience reaction (i.e. it could be the combination of more than one sentence). Slogan. A set of 1,533 slogans taken from onCORPS and Slogans datasets can be downloaded at the following link: https://github.com/marcoguerini/ paired_datasets_for_persuasion/
2

line resources paired with non-slogans that are similar in content. We collected the non-slogans from the subset of the New York Times articles in English GigaWord ­ 5th Edition ­ released by Linguistic Data Consortium (LDC)3 . For each slogan, we picked the most similar sentence in the New York Times articles having the same length and the highest LSA similarity (Deerwester et al., 1990) with the slogan. The LSA similarity approach that we used to collect the non-slogans is very similar to the approach used by Louis and Nenkova (2013) to collect the non-persuasive counterparts of successful news articles. In Table 1, we sum up the criteria used in the construction of each dataset. As can be observed from the table, each dataset satisfies at least two of the three criteria described above. In the last two
DATASET CORPS Movie Slogan Twitter Author     Criterion Length     Topic     Length P ¬P 14.0 9.7 5.0 16.2 14.0 9.7 5.0 15.4

Table 1: Criteria used in the construction of each dataset and average token length of persuasive and nonpersuasive pairs

columns of the table, we also provide the average token length of the persuasive and non-persuasive sentences in each dataset. Finally, in Table 2 we provide examples of euphonic and persuasive sentences for each dataset together with their phonetic scores.

5

Data Analysis

To provide a first insight on the data, in Table 3 we report the average phonetic scores for each data set (Mann-Whitney U Test is used for statistical significance between P and ¬P samples, with Bonferroni correction to ameliorate issues with multiple comparisons). The results are partially in line with our expectations of the euphony phenomena being more relevant in the persuasive sentences across the datasets. As can be observed from the table, the average rhyme scores are higher in persuasive sentences and
http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2011T07
3

1486

