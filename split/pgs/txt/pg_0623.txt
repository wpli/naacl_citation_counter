word removal. We use lexical specificity in order to extract the most representative words (Section 2.2.1) and synsets (Section 2.2.2) of Wb . Lexical specificity. Lexical specificity (Lafon, 1980) is a statistical measure that has been used in a wide range of NLP applications, such as textual data analysis (Lebart et al., 1998), term extraction (Drouin, 2003), and domain disambiguation (Camacho Collados et al., 2014). However, to our knowledge, it has never heretofore been used to calculate weights in a vector-based representation (Turney and Pantel, 2010). Lexical specificity is based on the hypergeometric distribution over word frequencies. This statistical measure is particularly suitable for extracting an accurate set of representative terms for a given subcorpus of a reference corpus (Lafon, 1980). Unlike the conventional term frequency-inverse document frequency weighting scheme (Jones, 1972, tf-idf ), lexical specificity is not especially sensitive to different text lengths. Assume a reference corpus of T words and a twords subcorpus of that corpus. The goal is to find a set of terms that are peculiar to the subcorpus, but not to the whole reference corpus. Formally, given a word w that occurs f and k times in the corpus and subcorpus, respectively, positive specificity computes the relevance of w to the subcorpus t as P (X  k ) if k  f T , where X is a random variable following a hypergeometric distribution with t parameters f , t and T , and f T is the expected value of X . In our setting we are only interested in the positive specificity, i.e., the set of most relevant words appearing in the contextual information. We apply the standard procedure of applying log10 and then inverting the sign of the specificity probabilities in order to re-scale them to the real line, which is more easily interpretable (Drouin, 2003; Camacho Collados et al., 2014). We only retain words with specificity greater than two, which is equal to -log10 (0.01). This threshold leads to a set of representative words that are relevant to the context with a confidence of at least 99%, i.e., P (X  k )  0.01 (Billami et al., 2014). 2.2.1 Word-based representation

specificity to compute a weighted set of most representative words for Wb with respect to the reference corpus, i.e., the whole Wikipedia. As an example, the obtained word-based vector for the edge of water sense of shore has water, ocean, lake, beach and sea among its most relevant dimensions. 2.2.2 Synset-based representation Given that the amount of contextual information gathered for a concept can be small, the resulting word-based vector can be sparse and as a consequence prone to noise, especially in the case of less frequent concepts. Therefore, we put forward a method that tackles the issue, providing rich semantically-aware representations. To this end, we group - and thereby generalize - similar dimensions in the obtained word-based vector, to produce a smaller vector in which dimensions are WordNet synsets and weights are computed on the basis of the combined information of the individual words in the group. The generalization procedure can be summarized in two main steps. First, for each word w in Wb , we obtain from WordNet the set Hw of all the direct hypernyms of all the synsets containing w. For each synset h  Hw we check whether there exists another word w from the contextual information that is a hyponym of h, i.e., h  Hw  Hw . When such is the case, letting Yh be the set of all words in the hyponym synsets of h, we combine w, w and all the other words in Yh into a single dimension represented by their common hypernym h. Thus for our earlier example, the three words ocean, lake, and sea are grouped into a single dimension represented by their hypernym, i.e., the synset containing the body 3 of water sense of water (water2 n in WordNet 3.0) . Then, we compute the weight associated with the new dimension by calculating the lexical specificity of the word cluster. Formally, we calculate the lexical specificity of h by setting the parameters k and f as the total number of times the words in Yh occur in Wb and the whole Wikipedia, respectively. The values of t and T remain unchanged. Our generalization procedure is similar to the dimensionality reduction that is performed using singular value decomposition in Latent Semantic Analysis (Landauer and Dumais, 1997, LSA). However,
3 i We denote the ith sense of word w with POS p as wp .

This word-based representation models the concept b in a conventional semantic space whose dimensions are individual words. We leverage lexical 569

