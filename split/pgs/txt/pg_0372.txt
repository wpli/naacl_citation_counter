used to train a model for each target word. In the test phase, the model is used to classify test samples and assign a sense tag to each sample. This supervised framework with separate feature extraction and classification phases makes it easy to add any number of features and in our case, word embeddings. In order to make use of word embeddings trained by a neural network, we follow the approach of (Turian et al., 2010) and include word embeddings for all words in the surrounding window of text, given a target ambiguous word. We use the words from immediately adjacent sentences if the window falls beyond the current sentence boundaries. Since each word type has its own classification model, we do not add the word embeddings for the target word because the same vector will be used in all training and test samples and will be useless. After extraction of the three mentioned types of features, d.(w - 1) features will be added to each sample, where d is the word embeddings dimension and w is the number of words in the window of text surrounding the target word (window size). w is one of the hyper-parameters of our system that can be tuned for each word type separately. However, since the training sets for some of the benchmark tasks are small, tuning the window size will not be consistent over different tuning sets. Thus, we decided to select the same window size for all words in a task and tune this parameter on the whole tuning set instead. After augmenting the features, a model is trained for the target word and then the classifier can be used to assign the correct sense to each test sample. However, since the original three types of features are binary, newly added real-valued word embeddings do not fit well into the model and they tend to decrease performance. This problem is addressed in (Turian et al., 2010) and a simple solution is to scale word embeddings. The following conversion is suggested by (Turian et al., 2010): E   · E/stddev (E ) (4)

Equation 4 may not work well. In this case, perdimension scaling will make more sense. In order to scale the word embeddings matrix, we use Equation 5 in our experiments: Ei   · Ei /stddev (Ei ), i : 1, 2, ..., d (5)

where Ei denotes the ith dimension of word embeddings. Like (Turian et al., 2010), we also found that  = 0.1 is a good choice for the target standard deviation and works well.

4

Results and Discussion

We evaluate our word sense disambiguation system experimentally by using standard benchmarks. The two major tasks in word sense disambiguation are lexical sample task and all-words task. For each task, we explain our experimental setup first and then present the results of our experiments for the two mentioned tasks. Although most benchmarks are general domain test sets, a few domain-specific test sets also exist (Koeling et al., 2005; Agirre et al., 2010). 4.1 Lexical Sample Tasks We have evaluated our system on SensEval-2 (SE2) and SensEval-3 (SE3) lexical sample tasks and also the domain-specific test set (we call it DS05) published by (Koeling et al., 2005). This subsection describes our experiments and presents the results of these tasks. 4.1.1 Experimental Setup Most lexical sample tasks provide separate training and test sets. Some statistics about these tasks are given in Table 1. #Word types #Training samples #Test samples SE2 73 8,611 4,328 SE3 57 8,022 3,944 DS05 41 10,272

where  is a scalar hyper-parameter denoting the desired standard deviation, E is the word embeddings matrix and stddev (.) is the standard deviation function, which returns a scalar for matrix E . However, different dimensions of word embedding vectors may have different standard deviations and 318

Table 1: Statistics of lexical sample tasks

The DS05 dataset does not provide any training instances. In order to train models for DS05 (and later for the SE3 all-words task), we generated training samples for the top 60% most frequently occurring polysemous content words in Brown Corpus,

