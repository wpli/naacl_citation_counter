able's outgoing messages are pointwise products of the incoming ones, so they become simple too. Past work has used approximations with a similar flavor. Hall and Klein (2011) heuristically predetermine a short, fixed list of plausible values for V that were observed elsewhere in their dataset. This list is analogous to our  V . After updating µF V , they force µF V (v ) to 0 for all v outside the list, yielding a finite message that is analogous to our  F V . Our own past papers are similar, except they adaptively set the "plausible values" list to F N (V ) k - BEST (µF V ). These strings are favored by at least one of the current messages to V (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011; Cotterell et al., 2015). Thus, simplifying one of V 's incoming messages considers all of them, as in EP. The above methods prune each message, so may prune correct values. Hall and Klein (2010) avoid this: they fit a full bigram model by inclusive KL divergence, which refuses to prune any values (see section 3). Specifically, they minimized D(µF V  || q  ), where  was a simple fixed function (a 0-gram model) included so that they were working with distributions (see section 4.3). This is very similar to our (7). Indeed, Hall and Klein (2010) found their procedure "reminiscent of EP," hinting that  was a surrogate for a real µV F term. Dreyer and Eisner (2009) had also suggested EP as future work. EP has been applied only twice before in the NLP community. Daumé III and Marcu (2006) used EP for query summarization (following Minka and Lafferty (2003)'s application to an LDA model with fixed topics) and Hall and Klein (2012) used EP for rich parsing. However, these papers inferred a single structured variable connected to all factors (as in the traditional presentation of EP--see Appendix A), rather than inferring many structured variables connected in a sparse graphical model. We regard EP as a generalization of loopy BP for just this setting: graphical models with large or unbounded variable domains. Of course, we are not the first to use such a scheme; e.g., Qi (2005, chapter 2) applies EP to linear-chain models with both continuous and discrete hidden states. We believe that EP should also be broadly useful in NLP, since it naturally handles joint distributions over the kinds of structured variables that arise in NLP. 937

We now fill in details. If the feature set is defined by an unambiguous FSA A (section 3.4), two methods exist to max Evp [log q (v )] as section 3.1 requires. Closed-form. Determine how often A would traverse each of its arcs, in expectation, when reading a random string drawn from p. We would obtain an optimal E NCODE( ) by, at each state of A, setting the weights of the arcs from that state to be proportional to these counts while summing to 1.6 Thus, the logs of these arc weights give an optimal  . For example, in a trigram model, the probability of the c arc from the ab state is the expected count of abc (according to p) divided by the expected count of ab. Such expected substring counts can be found by the method of Allauzen et al. (2003). For general A, we can use the method sketched by Li et al. (2009, footnote 9): intersect the WFSA for p with the unweighted FSA A, and then run the forwardbackward algorithm to determine the posterior count of each arc in the result. This tells us the expected total number of traversals of each arc in A, if we have kept track of which arcs in the intersection of p with A were derived from which arcs in A. That bookkeeping can be handled with an expectation semiring (Eisner, 2002), or simply with backpointers. Gradient ascent. For any given  , we can use the WFSAs p and E NCODE( ) to exactly compute Evp [log q (v )] = -H (p, q ) (Cortes et al., 2006). We can tune  to globally maximize this objective. The technique is to intersect p with E NCODE( ), after lifting their weights into the expectation semiring via the mappings k  k, 0 and k  0, log k respectively. Summing over all paths of this intersection via the forward algorithm yields Z, r where Z is the normalizing constant for p. We also sum over paths of E NCODE( ) to get the normalizing constant Z . Now the desired objective is r/Z - log Z . Its gradient with respect to  can be found by back-propagation, or equivalently by the forward-backward algorithm (Li and Eisner, 2009). An overlarge gradient step can leave the feasible space (footnote 1) by driving ZV to  and thus driving (2) to  (Dreyer, 2011, section 2.8.2). In this case, we try again with reduced stepsize.
This method always yields a probabilistic FSA, i.e., the arc weights are locally normalized probabilities. This does not sacrifice any expressiveness; see Appendix B.7 for discussion.
6

6

Two Methods for Optimizing 

