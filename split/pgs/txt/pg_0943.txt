early steep slope of the generative model is more desirable in this setting than the eventually superior performance of the discriminative model given large numbers of annotations. Figure 4 additionally plots M OM R ESPA, a variant of M OM R ESP deprived of all unannotated documents, showing that the early generative advantage is not attributable entirely to semi-supervision. The generative model is more robust to annotation noise than the discriminative model, seen by comparing the LOW, MED, and HIGH columns in Figure 3. This robustness is significant because crowdsourcing tends to yield noisy annotations, making the LOW and MED annotator pools of greatest practical interest. This assertion is borne out by an experiment with CrowdFlower, reported in Section 6. To validate that L OG R ESP does, indeed, asymptotically surpass M OM R ESP we ran inference on datasets with increasing annotation depths. Crossover does not occur until 20 Newsgroups is annotated nearly 12-deep for LOW, 5-deep for MED, and 3.5-deep (on average) for HIGH. Additionally, for each combination of dataset and annotator pool except those involving CONFLICT, by the time L OG R ESP surpasses M OM R ESP, the majority vote baseline is extremely competitive with L OG R ESP. The CONFLICT setting is the exception to this rule: CONFLICT annotators are particularly challenging for majority vote since they violate the implicit assumption that annotators are basically aligned with the truth. The CONFLICT setting is of practical interest only when annotators have dramatic deepseated differences of opinion about what various labels should mean. For most crowdsourcing projects this issue may be avoided with sufficient up-front orientation of the annotators. For reference, in Figure 4 we show that a less extreme variant of CONFLICT behaves more similarly to LOW. Table 2 reports the percent of the dataset that must be annotated three-deep before L OG R ESP's inferred label accuracy surpasses that of M OM R ESP. Crossover tends to happen later when annotation quality is low and earlier when annotator quality is high. Cases reported as NA were too close to call; that is, the dominating algorithm changed depending on the random run. Unsurprisingly, M OM R ESP is not well suited to all classification datasets. The 0% entries in Table 889

20 Newsgroups
CONFLICT_MILD algorithm MomResp MomRespA LogResp 0 5 10 15 Majority

Number of annotated instances x 1,000

Figure 4: Inferred label accuracy for a variant of the CONFLICT annotator pool in which the offdiagonals of each annotator confusion matrix are drawn from a Dirichlet parameterized by 1 rather than 0.1. Also adds the algorithm M OM R ESPA to show the effect of removing M OM R ESP's access to unannotated documents. 2 mean that L OG R ESP dominates the learning curve for that annotator pool and dataset. These cases are likely the result of the M OM R ESP model making the same strict inter-feature independence assumptions as na¨ ive Bayes, rendering it tractable and effective for many classification tasks but ill-suited for datasets where features are highly correlated or for tasks in which class identity is not informed by document vocabulary. The CADE12 dataset, in particular, is known to be challenging. A supervised na¨ ive Bayes classifier achieves only 57% accuracy on this dataset (Cardoso-Cachopo, 2007). We would expect M OM R ESP to perform similarly poorly on sentiment classification data. Although we assert that generative models are inherently better suited to crowdsourcing than discriminative models, a sufficiently strong mismatch between model assumptions and data can negate this advantage.

6

In the previous section we used simulations to control annotator error. In this section we relax that control. To assess the effect of real-world annotation error on M OM R ESP and L OG R ESP, we selected 1000 instances at random from 20 Newsgroups and paid annotators on CrowdFlower to annotate them with the 20 Newsgroups categories, presented as humanreadable names (e.g., "Atheism" for alt.atheism). Annotators were allowed to express uncertainty by

Accuracy

1.0 0.8 0.6 0.4

Experiments with Human Annotators

