The results reveal that in case of applying trigram LMs F1 degrades about 9% absolute compared to parsing written text (reference), yielding 78.36% in F1 , even though the WER is rather low with a value of 7.1%. Thus, the trigram seems to "destroy" semantically meaningful sequences while restoring sequences that contain no meaning. By contrast, in case of applying a semantically motivated recognition grammar including weights, performance improves by 6% absolute over the trigram model, even though the WER is higher in this case. Moreover, including weights in the recognition grammars yields improved performance compared to using unweighted grammars. Notably, the decrease in performance in case of applying a weighted semantically motivated recognition grammar (+ trigram back off) compared to performance on the reference data is mainly due to a decrease in precision. Here it must be noted that the high values in F1 are achieved without performing any optimization of (recognition) parameters. In the performed experiments, the probability for OOG utterances was rather low, and thus utterances were matched incorrectly by the ASR which were actually not covered by the grammar, yielding both recognition and subsequent parsing errors. However, these parameters can be tuned, likely increasing precision and F1 even further (and probably also the WER). Applying a back off trigram model yields only little improvement in parsing performance, although this may to some extent be due to not tuning recognition parameters. That is, if the ASR would be tuned to reject more OOG utterances correctly, these utterances might instead be recognized by a trigram model and probably parsed correctly by applying approximate matching.

5

Discussion

When applying trigram models, even with a rather low error rate of 7.1%, semantic parsing performance degraded about 9% absolute in F1 . Here it must be noted that due to the evaluation schema a single recognition error can yield a completely incorrect parse. Recall that evaluation is performed on the basis of fully correct mrs, i.e. all referents and the predicate must be determined correctly in order to yield a correct parse. For instance, if any

of the words "purple", "pink", "two" or "five" is deleted or substituted in an utterance "purple two passes to pink five", one of the referents may not be identified (correctly). Similarly, deleting or substituting "passes" may yield an incorrect predicate or no parse at all. Hence, parsing performance can degrade rapidly even on ASR transcriptions containing only few recognition errors. The results show that, in line with previous research (Wang et al., 2003; Bayer and Riccardi, 2012), a lower WER may not yield better understanding results, i.e. in our case parsing performance is not directly dependent on the WER but rather on the type of errors made. In particular, with respect to semantic parsing it is important that words carrying important meaning are recognized correctly. For instance, a spoken utterance "pink nine passes the ball to pink seven" in which "seven" is incorrectly recognized as "eleven" likely yields a parsing error while a recognition error which substitutes "backward" by "forward" may not prevent correct parsing. This is the case because "forward" and "backward" do not carry any semantics in the data set at hand, while correct identification of numbers is in most cases essential for detecting the correct semantic referents. Applying the semantically motivated grammars may have been beneficial in recognizing the semantic referents correctly because the system can explicitly learn them and their appearances in certain patterns in contrast to the trigram model. In particular, if an utterance "pink nine passes the ball to pink seven" appears during recognition and "pink seven" has not been observed in the context of the preceding words during training, then the n-gram model would assign a low probability, likely leading to a recognition error such as "pink eleven". By contrast, in case of semantic grammars the system can learn that the utterance is an instantiation of a pattern "player passes the ball to player" and that all players can appear at the contained slots, thus making the appearance of the example utterance more likely. Notably, semantic classes such as player can in principle also be modeled in stochastic language models, in particular by applying class-based models, or in syntactically motivated grammars. Recall that we also experimented with syntactically motivated grammars and with class-based models and that neither classes which were auto-induced on the raw text data nor

879

