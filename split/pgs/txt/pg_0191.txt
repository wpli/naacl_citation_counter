The final conclusion of these observations is as follows: since any solution can be modelled as a chain of state sequences, the global optimum can be found by generating all possible states and fetching the one that, when reached from the initial state, yields the maximum score. This task is achievable with a number of operations linearly proportional to the number of possible states, which at the same time is polynomial with respect to the number of tokens in the document. In conclusion, the model can be decoded in quadratic time. The pseudo-code in algorithm 1 gives a sketch of a O(H × n2 × |P oS |) bottom-up implementation.

Algorithm 1 Sketch of a bottom-up algorithm for find-

ing the top-scoring state s that leads to the global optimum of our model's objective function. It iteratively computes two functions:  (i, l), which returns the set of all reachable states that correspond to headlines having token-length l and finishing with token xi , and (s), which returns the maximum score that can be obtained by following a chain of state sequences that ends in the provided state, and starts on s0 .
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27:

6

Training the model: learning what human-generated headlines look like

The global feature function F is responsible for taking a document and a bitmap, and producing a vector that describes the candidate headline in an abstract feature space. We defined the feature function so it only focuses on evaluating how a series of tokens that comprise a headline relate to each other and to the document as a whole. This implies that if h = {h1 , h2 , ..., hk } is the tokenized form of any arbitrary headline consisting of k tokens, and we define vectors a  X k+n and b  Y k+n as: a = {h1 , h2 , ..., hk , x1 , x2 , ..., xn } b = {11 , 12 , ..., 1k , 01 , 02 , ..., 0n } where a is the concatenation of h and x, and b is a bitmap for only selecting the actual headline tokens, it follows that the feature vector that results from calling the global feature function, which we define as u = F (a, b) (2) is equivalent to a description of how headline h relates to document x. This observation is the core of our learning algorithm, because it implies that it is possible to "insert" a human-generated headline in the text and get its description in the abstract feature space induced by F . The objective of the learning process will consist in molding a weight vector w, such that it makes the decoding algorithm favor headlines whose descriptions in feature space resemble the characteristics of human-generated titles. For training the model we follow the on-line learning schemes presented by Collins (2002) and 137

//Constants. H  Max. number of allowed tokens in headlines. n  Number of tokens in the document. x  List of n tokens (document). w  Weight vector. g  State transition function. f  Local feature function. s0  Init state. //Variables.   new Set<State>[n + 1][H + 1]({})   new Float[|State|](-) s  s0 //Base cases. (s0 )  0 for i in {0, ..., n} do  (i, 0)  {s0 } //Bottom-up fill of  and . for l in {1, ..., H } do for i in {l, ..., n} do for j in {l - 1, ..., i - 1} do for z in  (j, l - 1) do s  g (z, x, i, 1) sscore  (z ) + w · f (x, i, z, 1)  (i, l)   (i, l)  {s} (s)  max((s), sscore ) if (s) > (s ) then s  s

applied in studies that deal with CRF models with state sequences, such as the dependency parsing model of McDonald et al. (2005). The learning framework consists of an averaged perceptron that iteratively sifts through the training data and performs the following error-correction update at each step: ^ ), v ^ = F (x, y ^) w   w +  × (u - v ^ is the where u is the vector defined in equation (2), y result of solving equation (1) with the current weight

