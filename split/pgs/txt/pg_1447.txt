Insertion: Add current mention with probability pi . Removal: If a mention was inserted choose a mention uniformly at random to remove. Setting pi = k/N where k is the sample size and N is the number of items seen ensures the sample is uniform (Vitter, 1985).  Biased Reservoir Sampling (Biased-R): To resolve both distant and recent references we should store recent mentions and some older mentions. An uninformed approach from randomized algorithms is to use an exponentially biased reservoir sample (Aggarwal, 2006; Osborne et al., 2014). It will store mostly recent mentions but will probabilistically allow older mentions to stay in the sample. For example this technique will sample lots of mentions from yesterday and less from the day before yesterday. Insertion: Add current mention with probability pi Removal: If a mention was inserted choose a mention uniformly at random to remove. Unlike uniform reservoir sampling pi is constant. A higher value puts more emphasis on the recent past. [pi ]  Cache*: We should keep past mentions critical to resolving current references (an informed implementation of recency) and allow mentions to stay in the sample for an arbitrary period of time to help resolve distance references. For example if the same mention is used to resolve a reference on Saturday and Sunday it should be in the sample on Monday. Insertion: Add current mention to sample. Removal: Choose a mention that was not recently used to resolve a reference uniformly at random and remove it. When a mention is resolved we find its most similar mention in the sample, recording its use in a first in, first out cache of size n. The mention to be removed is chosen from the set of mentions not in the cache. We set n equal to a proportion of the sample size. [Proportion of mentions to keep] 1393

 Diversity*: If we store fewer mentions about each distinct entity we can represent more entities in the sample. For example if news breaks that a famous person died yesterday the sample should not be full of mentions about that entity at the expense of other entities mentioned today. Insertion: Add current mention to sample. Removal: If there is a sufficiently similar mention in the sample remove it else choose uniformly at random to be removed. We remove the past mention most similar to the current mention, but only if the similarity exceeds a threshold. [Replacement Threshold]  Diversity-Cache (D-C)*: We combine Diversity and Cache sampling. Insertion: Add current mention to sample. Removal: If there is a sufficiently similar mention in the sample remove it else remove a mention that has not recently been used to resolve a reference chosen uniformly at random. For this technique we first choose the replacement threshold then the proportion of mentions to keep. [Replacement threshold and proportion of mentions to keep]

6

Dataset

We collected 52 million English tweets from the 1% sample of all tweets sent over a 77 day period. We performed named entity recognition using Ritter et al. (2011). It is clearly infeasible for us to annotate all the mentions in the dataset. Hence we annotated a sample of the entities. As with most prior work we focused on person named entity mentions (of which there is approximately 6 million in the dataset). To select the entities we first sampled two names based on how frequently they occur in the dataset: `Roger' was chosen randomly from the low frequency names (between 1,000 and 10,000 occurrences) and `Jessica' was chosen similarly from medium frequency names (10,000 to 100,0000 occurrences). We first annotated all mentions of the names `Roger' and `Jessica' discarding entities mentioned once. For the remaining entities we annotated all their mentions (not restricting to mentions that contained the words `Roger' or `Jessica').

