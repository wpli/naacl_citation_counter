word is represented by a d-dimensional word embedding (d = 100 in our experiments). The next level feature matrix is the short ngram ^ sn . It has size (|S1 | + msn - 1) × similarity matrix F (|S2 | + msn - 1) where msn = 3 is the filter width in this convolution layer and |Si | + msn - 1 is the num^ sn ij is ber of short ngrams in Si . The feature entry F the similarity between two d/2-dimensional vectors representing two short ngrams from S1 and S2 . We use multiple feature maps to improve the system performance. Different feature maps are expected to extract different kinds of sentence features, and can be implemented in the same convolution layer in parallel. Specifically, we use f sn = 6 feature maps on this level following Kalchbrenner et al. (2014). Thus, we actually compute six feature ma^ sn, i (i = 1, · · · , f sn ), one for each pair of trices F feature maps that share convolution weights while derived from S1 and S2 respectively. (Figure 1 only shows one of those six matrices.) The next level feature matrix is the long ngram ^ ln . It has size (k dy, 1 + mln - 1) × similarity matrix F (k dy, 2 + mln - 1) where k dy, i (Equation 3) is the k value in dynamic k-max pooling for sentence i, k dy, i + mln - 1 is the number of long ngrams in Si and mln = 5 is the filter width in this convolu^ ln ij is the similarity tion layer. The feature entry F between two d/4-dimensional vectors representing two long ngrams from S1 and S2 . We use f ln = 14 feature maps on this level following Kalchbrenner et al. (2014). Thus, we com^ ln, i (i = 1, · · · , f ln ), in a pute 14 feature matrices F ^ sn, i . way analogous to the f sn = 6 feature maps F The last feature matrix is the sentence similarity ^ s . It has size k top × k top where k top = 4 matrix F is the parameter in k-max pooling at the last max ^ s ij is the similarity pooling layer. The feature entry F between two d/4-dimensional vectors computed by max pooling from S1 and S2 . For l = s, there are also f ln = 14 feature matrices ^ Fs, i (i = 1, · · · , f ln ), analogous to the f ln = 14 ^ ln, i . feature matrices F A general design principle of the architecture is that we compute each interaction feature matrix between two feature maps that share the same convolution weights. Two feature maps learned with the same filter will contain the same kinds of features derived from the input.

Dynamic pooling of feature matrices ^ l have As sentence lengths vary, feature matrices F different sizes, which makes it impossible to use them directly as input of the last layer. ^ l  Rr×c into a That means we need to map F matrix Fl of fixed size r × c (l  {u, sn, ln, s}; r , c are parameters and are the same for all sentence pairs while r, c depend on |S1 | and |S2 |). Dy^ l into r × c nonoverlapping namic pooling divides F (dynamic) pools and copies the maximum value in each dynamic pool to Fl . Our method is similar to (Socher et al., 2011), but preserves locality better. ^ l can be split into equal regions only if r (resp. F c) is divisible by r (resp. c ). Otherwise, for r > r and if r mod r = b, the dynamic pools in the first r r - b splits each have r rows while the remaining r b splits each have r + 1 rows. In Figure 2, a r × c = 4 × 5 matrix (left) is split into r × c = 3 × 3 dynamic pools (middle): each row is split into [1, 1, 2] and each column is split into [1, 2, 2]. If r < r , we first repeat all rows until no fewer than r rows remain. Then first r rows are kept and split into r dynamic pools. The same principle applies to the partitioning of columns. In Figure 2 (right), the areas with dashed lines and dotted lines are repeated parts for rows and columns, respectively; each cell is its own dynamic pool.

4.2

5
5.1

Training
Supervised training

Dynamic pooling gives us fixed size interaction feature matrices for sentence, ngram and unigram levels. As shown in Figure 1, the concatenation of these features (Fs , Fln , Fsn and Fu ) is the input to a logistic regression layer for paraphrase classification. We have now described all three parts of Bi-CNN-MI: CNN-SM, CNN-IM and logistic regression. Bi-CNN-MI with all its parameters ­ including word embeddings and convolution weights ­ is trained on MSRP. We initialize embeddings with those provided by Turian et al. (2010)1 (based on Collobert and Weston (2008)). For layer sn, we have f sn = 6 feature maps and set filter width msn = 3. For layer ln, we have f ln = 14 feature maps and set filter width mln = 5 and k top = 4. Dynamic pooling
1

metaoptimize.com/projects/wordreprs/

906

