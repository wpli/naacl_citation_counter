Edit Insertion Punctuation Subj. Word Be Det. Other Replacement Punctuation Slang Contraction Word Capitalization Other Removal Punctuation Word Twitter Other

Figure 1: Taxonomy of normalization edits

taxonomy of normalization token or edit types, different analyses often look at different edit types and at different levels of granularity. In an attempt to help future work converge on a common understanding of normalization edits, in this section we present our taxonomy of normalization edits at several different levels of granularity. While it would be difficult for a taxonomy of normalization edits to be universal enough to be appropriate over all datasets and domains, we attempt to provide a taxonomy general enough to give future work a meaningful initial point of reference. 3.1 Methodology

are most centrally thought of as part of the normalization task from other instances that may require additional pragmatic inference. Specifically, we separate edits coarsely into three categories:  Token Replacements. Replacing one or more existing tokens with one or more new tokens (e.g., replacing wanna with want to).  Token Additions. Adding a token that does not replace an existing token (e.g., adding in missing subjects).  Token Removals. Removing a token without replacing it with an equivalent (e.g., removing laughter words such as lol and hahaha). Level Two. The next level of granularity separates normalization edits over word tokens from those over punctuation:  Word. Replacing, adding, or removing word tokens (depending on parent).  Punctuation. Replacing, adding, or removing punctuation tokens (depending on parent). Level Three. At the final level, we subdivide word edits into groups as appropriate for the edit type. Rather than attempting to keep consistent groups across all leaf nodes, we selected the grouping based on the data distribution. For instance, Twitterspecific tokens (e.g., retweets) are often removed during normalization, so examining the removal of these words as a group is warranted. In contrast, these tokens are never added, so different segmentation is appropriate when examining word addition. At the lowest level of the taxonomy, word replacements were subdivided as follows:

Our taxonomy draws inspiration from both previous work and an examination of our own dataset (Section 3.3). In doing so, it attempts to cover normalization edits broadly, including cases that are universally understood to be important, such as slang replacement, as well as cases that are frequently ignored, such as capitalization correction. One of the guiding principles in the design of our taxonomy was that categories should not be divided so narrowly such that the phenomenon they capture appeared very infrequently in the data. One example of this is our decision not to divide punctuation edits at the lowest level of granularity. While certain clear categories exist (e.g., emoticons), these cases appeared in a small enough percentage of tokens that they would be difficult to examine and likely have a negligible effect on overall performance. 3.2 Taxonomy

Our taxonomy of normalization edits is shown in Figure 1. As can be seen, we categorize edits at three levels of granularity. Level One. The primary goal of the level one segmentation is to separate token replacements which 422

