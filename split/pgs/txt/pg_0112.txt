which in return combines with NP:John and forms back S:likes. The dependency graph is updated with a dependency between `likes' and `madly'. Note that the final output is a standard CCG tree. Figure 4(b) shows this LRev action. 3.2.3 Analysis Our incremental algorithm uses a combination of the CCG derivation and a dependency graph that helps to `reveal' unbuilt structure in the CCG derivation by identifying heads of the revealed categories. For example in Figure-4a, in RRev action, S:likes is split into S/NP:likes and NP:mangoes. The splitting of categories is deterministic but the right periphery of the dependency graph helps in identifying the head, which is `mangoes'. The theoretical idea of `revealing' is from Pareschi and Steedman (1987), but they used only a toy grammar without a model or empirical results. Checking the right periphery is similar to Sartorio et al. (2013) and abstracting over the left or right argument is similar to Dalrymple et al. (1991). Currently, we abstract only over arguments. Adding a new action to abstract over the verb as well will make our algorithm handle ellipses in the sentences like `John likes mangoes and Mary too' similar to Dalrymple et al. (1991) but we leave that for future work. Our system is monotonic in the sense that the set of dependency relationships grows monotonically during the parsing process. Our algorithm gives derivations almost as incremental as Hassan et al. (2009) but without changing the lexical categories and without backtracking. The only change we made to the CCGbank is making the main verb the head of the auxiliary rather than the reverse as in CCGbank derivations. In the right derivational trees of CCGbank, the main verb is the head for its right side arguments and the auxiliary verb is the head for the left side arguments in the derivation. Not changing the head rule would make our algorithm use the costly reveal actions significantly more, which we avoid by changing the head direction. 3% of the total dependencies are affected by this modification. Though our algorithm can be completely incremental, we currently compromise incrementality in the following cases: (a) no dependency between the nodes in the stack (b) unary type-changing and non-standard binary rules

(c) adjuncts like VP modifiers and coordinate constructions like VP, sentential coordination. We find empirically that extending incrementality to cover these cases actually reduces parsing performance significantly. It also violates the Strict Competence Hypothesis (SCH) (Steedman, 2000), which argues on evolutionary and developmental grounds that the parser can only build constituents that are typable by the competence grammar. We explored the adjunct case of attaching only the preposition first rather than creating a complete prepositional phrase and then attaching it to correct parent. In our example sentence, this would be the case of attaching the preposition `from' to its parent using RRev and then combining the NP `India' accordingly as opposed to creating the preposition phrase `from India' first and then using RRev action to attach it to the correct parent. Though the former is more incremental, it is inconsistent with the SCH. The latter analysis is consistent with strict competence and also gave better parsing performance while compromising incrementality only slightly. The empirical impact of these differing degrees of incrementality on extrinsic evaluation of our algorithm in terms of language modeling for SMT or ASR is left for future work. Using our incremental algorithm, we converted the CCGbank derivations into a sequence of shiftreduce actions. We could convert around 98% of the derivations, which is the coverage of our algorithm, recovering around 99% dependencies. Problematic cases are mainly the ones which involve nonstandard binary rules, and punctuations with lexical CCG categories other than `conj', used as a conjunction, or `,' which is treated as a punctuation mark.

4 Experiments and Results
We re-implemented Zhang and Clark (2011)'s model for our experiments. We used their global linear model trained with the averaged perceptron (Collins, 2002). We applied the early-update strategy of Collins and Roark (2004) while training. In this strategy, when we don't use a beam, decoding is stopped when the predicted action is different from the gold action and weights are updated accordingly. We use the feature set of Zhang and Clark (2011) (Z&C) for the NonInc algorithm. This feature set comprises of features over the top four nodes in the

58

