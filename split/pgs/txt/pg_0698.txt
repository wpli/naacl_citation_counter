due to missed argument mentions. Note that the cost of annotation was around 75 hours, confirming the cost-saving properties of distant supervision. 2.3 Dataset and experiment organization We sorted the list of earthquakes in the KB chronologically, and chose the earliest 75% of the earthquakes as the training dataset, and the most recent (25%) for testing. The training set contained 81 earthquakes and their corresponding 6078 tweets, while the testing set contained 27 earthquakes and 1763 tweets. All development experiments were performed using 5-fold cross-validation over the training partition, where the folds were organized randomly by earthquake. Each fold contained tweets for around 15 earthquakes, but the number of tweets varied widely, with one fold having 585 tweets and another 2229. The evaluation compares the argument values induced by our system with those in the gold KB, and computes precision, recall and F1 using the official scorer from the Knowledge Base Population (KBP) Slot Filling (SF) shared task (Surdeanu, 2013). We also incorporated the notion of equivalence classes proposed in the SF task. For instance, if the system predicted Guerrero State for the argument region, when the KB contains just Guerrero, we consider this result correct because the two strings are equivalent in this context. Our equivalence classes also include countries, regions, and cities with hashtags, unnormalized temporal expressions, etc. Where applicable, we checked statistical significance of performance differences using the bootstrap resampling technique proposed in (BergKirkpatrick et al., 2012), in which we draw many simulated test sets by sampling with replacement from the set of earthquakes in the test partition. 2.4 Distant supervision for event extraction For the initial extraction experiment, we followed a traditional distant supervision approach (Mintz et al., 2009), which has four steps: the KB of past events is aligned to the text; a supervised system is trained on the resulting annotated text; the system is run on test data; and the output slot values are inferred from the annotations produced by the system. We thus started by aligning the information in the KB to the training tweets using strict match644

ing8 . Table 2 compares the number of mentions automatically generated through DS against the number of manually annotated mentions. As expected, the strict matching criterion yields fewer mentions than the manual annotation. As an example of this process, given the Honduras earthquake in Table 2, this procedure will annotate two argument mentions in the first tweet from Table 1, country and affected-country, as follows:
Earthquake in <country>Honduras</country>. So strong it was felt in <affectedcountry>Guatemala</affected-country> as well. 7.1 offshore atlantic.

Note that the magnitude in the tweet is different from the one reported in the KB and it will thus be left unmarked (we revisit this issue in Section 5). Using this automatically-generated data, we trained a sequential tagger based on Conditional Random Fields (CRF)9 . Based on the output of the CRF, we inferred the arguments values using noisyor (Surdeanu et al., 2012), which selects the value with the largest probability for each single-valued argument by aggregating the individual mention probabilities produced by the CRF.10 In the case of multi-valued arguments (affected-country and affected-region) we choose all values that had been annotated by the sequential tagger.

3

Initial results and analysis

The left block in Table 3 reports the results on development (5-fold cross-validation) of the initial event
We identified two types of arguments: those that have binary (yes/no) values (tsunami and landslides) and those having other values. For the first type, we search the tweets corresponding to the target earthquake for a small number of strings (e.g., tsunami and tsunamis), and annotate all matches (e.g., <tsunami> tsunami </tsunami>). For nonbinary valued arguments, we searched the tweets for exact occurrences of the corresponding values, and annotated all matching strings. When the same value appears in more than one argument for the same earthquake (e.g., 7 as both magnitude and number of dead people), we choose the most common label (e.g., magnitude cf. Table 2). 9 We used the linear CRF in Stanford's CoreNLP package, with the default features (word form, PoS, lemma, NERC) for the macro configuration: http://nlp.stanford.edu/ software/corenlp.shtml. 10 For multi-token mentions (e.g. New Zealand) we use the average of the token probabilities.
8

