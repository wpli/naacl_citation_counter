For our new method, it is interesting to measure the improvement on the subset of those MSRP sentences that contain at least one phrase. In the standard MSRP corpus, 3027 training pairs (2123 true, 904 false) and 1273 test pairs (871 true, 402 false) contain phrases; we denote this subset as subset. We carry out experiments on overall (all MSRP sentences) as well as subset cases. We compare six methods for paraphrase identification.  NOWEIGHT. Following Blacoe and Lapata (2012), we simply represent a sentence as the unweighted sum of the embeddings of all its units.  MT is the method proposed by Madnani et al. (2012): the sentence pair is represented as a vector of eight different machine translation metrics.  Ji and Eisenstein (2013). We reimplemented their "inductive" setup which is based on matrix factorization and is the top-performing system in paraphrasing task.3 The following three methods not only use this vector of eight MT metrics, but use three kinds of additional features given two sentence representations s1 and s2 : cosine similarity, element-wise sum s1 + s2 and element-wise absolute difference |s1 - s2 |. We now describe how each of the three methods computes the sentence vectors.  WORD. The sentence is represented as the sum of all single-word embeddings, weighted by TF-KLD-KNN.  WORD+PHRASE. The sentence is represented as the sum of the embeddings of all its units (including phrases), weighted by TFKLD-KNN.  WORD+GOOGLE. Mikolov et al. (2013) use a data-driven method to detect statistical phrases which are mostly continuous bigrams.
They report even better performance in a "transductive" setup that makes use of test data. We only address paraphrase identification for the case that the test data are not available for training the model in this paper.
3

We implement their system by first exploiting word2phrase4 to reformat Wikipedia, then using word2vec skip-gram model to train phrase embeddings. We use the same weighting scheme TF-KLDKNN for the three weighted sum approaches: WORD, WORD+PHRASE and WORD+GOOGLE. Note however that there is an interaction between representation space and nearest neighbor search. We limit the neighbor range of unknown words for WORD to single words; in contrast, we search the space of all single words and linguistic (resp. Google) phrases for WORD+PHRASE (resp. WORD+GOOGLE). We use LIBLINEAR (Fan et al., 2008) as our linear SVM implementation. 20% training data is used as development data. Parameter k is fine-tuned on development set and the best value 3 is finally used in following reported results. 5.2 Experimental results Table 1 shows performance for the six methods as well as for the majority baseline. In the overall (resp. subset) setup, WORD+PHRASE performs best and outperforms (Ji and Eisenstein, 2013) by .009 (resp. .052) on accuracy. Interestingly, Ji and Eisenstein (2013)'s method obtains worse performance on subset. This can be explained by the effect of matrix factorization in their work: it works less well for smaller datasets like subset. This is a shortcoming of their approach. WORD+GOOGLE has a slightly worse performance than WORD+PHRASE; this suggests that linguistic phrases might be more effective than statistical phrases in identifying paraphrases. Cases overall and subset both suggest that phrase embeddings improve sentence representations. The accuracy of WORD+PHRASE is lower on overall than on subset because WORD+PHRASE has no advantage over WORD for sentences without phrases. 5.3 Effectiveness of TF-KLD-KNN The key contribution of TF-KLD-KNN is that it achieves full coverage of feature weights in the face of data sparseness. We now compare four weighting methods on overall corpus and with the combi4

https://code.google.com/p/word2vec/

1371

