De-stem 2 medium plum tomatoes.

Cut them in half lengthwise and

remove the seeds.

Finely chop the tomatoes,

combining them with 1/4 cup of finely chopped red onion, 2 minced cloves of garlic, 1 tablespoon of olive oil, 2 tablespoons of fresh lime juice, and 1/8 teaspoon of black pepper

Cut an avocado into chunks and mash until it's smooth with just a few pieces intact.

Stir the mashed avocados into the other mixture for a homemade guacamole recipe that 's perfect for any occasion!

Use this easy guacamole for parties,

or serve chips with guacamole for an easy appetizer.

You could even add some cayenne, jalapenos, or ancho chili for even more kick to add to your Mexican food night!

Figure 7: Automatically illustrating a Guacamole recipe from https://www.youtube.com/watch?v=H7Ne3s202lU.

to briefly mention a few key papers. (Rohrbach et al., 2012b) describes the MPII Cooking Composite Activities dataset, which consists of 212 videos collected in the lab of people performing various cooking activities. (This extends the dataset described in their earlier work, (Rohrbach et al., 2012a).) They also describe a method to recognize objects and actions using standard vision features. However, they do not leverage the speech signal, and their dataset is significantly smaller than ours. (Guadarrama et al., 2013) describes a method for generating subject-verb-object triples given a short video clip, using standard object and action detectors. The technique was extended in (Thomason et al., 2014) to also predict the location/ place. Furthermore, they use a linear-chain CRF to combine the visual scores with a simple (s,v,o,p) language model (similar to our affordance model). They applied their technique to the dataset in (Chen and Dolan, 2011), which consists of 2000 short video clips, each described with 1-3 sentences. By contrast, we focus on aligning instructional text to the video, and our corpus is significantly larger. (Yu and Siskind, 2013) describes a technique for estimating the compatibility between a video clip and a sentence, based on relative motion of the objects (which are tracked using HMMs). Their method is tested on 159 video clips, created under carefully controlled conditions. By contrast, we focus on aligning instructional text to the video, and our corpus is significantly larger.

have used this to align 180k recipe-video pairs, from which we have extracted a corpus of 1.4M labeled video clips ­ a small but crucial step toward building a multimodal procedural knowlege base. In the future, we hope to use this labeled corpus to train visual action detectors, which can then be combined with the existing visual object detectors to interpret novel videos. Additionally, we believe that combining visual and linguistic cues may help overcome longstanding challenges to language understanding, such as anaphora resolution and word sense disambiguation. Acknowledgments. We would like to thank Alex Gorban and Anoop Korattikara for helping with some of the experiments, and Nancy Chang for feedback on the paper.

References
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101 ­ mining discriminative components with random forests. In Proc. European Conf. on Computer Vision. A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hruschka Jr., and T. Mitchell. 2010. Toward an architecture for never-ending language learning. In Procs. AAAI. David L Chen and William B Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proc. ACL, HLT '11, pages 190­200, Stroudsburg, PA, USA. Association for Computational Linguistics. X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann, S. Sun, and W. Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In Proc. of the Int'l Conf. on Knowledge Discovery and Data Mining. Gregory Druck and Bo Pang. 2012. Spice it up?: Mining

6

Discussion and future work

In this paper, we have presented a novel method for aligning instructional text to videos, leveraging both speech recognition and visual object detection. We 151

