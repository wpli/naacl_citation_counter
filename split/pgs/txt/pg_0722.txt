Finally, the arcs with at least one endpoint in R are added, using Register-Stack arcs for those with the other endpoint in {a, b} and Left-Arc/Right-Arc for those with both endpoints in R. Before any vertex incident to a or b is shifted onto the stack, all tokens on the stack to the right of b are reduced. After all these arcs are added, the crossing interval is complete. The boundary points of the interval that can still participate in uncrossed arcs with the exterior are left on the stack and buffer after the clear operation, so the rest of the tree is still parsable.

7

Experiments

6

Worst-case Runtime

The two-registers system runs in O(n) time: it completes after at most O(n) transitions and each transition takes constant time. The total number of arc-adding actions (Left-Arc, Right-Arc, Register-Stack, or a Store that includes an arc) is bounded by n, as there are at most n arcs in the final output. The net result of {Store, Store, Clear} triples of transitions decreases the number of tokens on the buffer by at least one, so these triples, plus the number of Shifts and Right-Arcs, are bounded by n. Finally, each token can be removed completely at most once, so the number of Left-Arcs and Reduces is bounded by n. Every transition fell into one of these categories, so the total number of transitions is bounded by 5n = O(n). Each operation can be performed in constant time, as all operations involve moving vertices and/or adding arcs, and at most three vertices are ever moved (Clear) and at most one arc is ever added. Most preconditions can be trivially checked in constant time, such as checking whether a vertex already has a parent or not. The non-trivial precondition to check is acyclicity, and this can also be checked by adding some book-keeping variables that can be updated in constant time (full proof omitted due to space constraints). For example, in the derivation in Table 3, prior to the RegisterStack(2, to-stack) transition, R1 A R2 (helped A paint). After the arc (R2, 1 ) (paint, house) is added, R2 A 1 and by transitivity, R1 A 1 . The top of the stack is then reduced, and since 2 does not have a parent to its right, it is not a descendant of 1 , and so after Hans becomes the new 1 , the system makes the update that R1, R2 A 1 . 668

The experiments compare the two-registers transition system for mildly non-projective trees proposed here with two other transition systems: the arceager system for projective trees (Nivre, 2003) and the swap-based system for all non-projective trees (Nivre, 2009). We choose the swap-based system as our non-projective baseline as it currently represents the state-of-the-art in transition-based parsing (Bohnet et al., 2013), with higher empirical performance than the Attardi system or pseudo-projective parsing (Kuhlmann and Nivre, 2010). The arc-eager system is a reimplementation of Zhang and Nivre (2011), using their rich feature set and beam search. The features for the two other transition systems are based on the same set, but with slight modifications to account for the different relevant domains of locality. In particular, for the swap transition system, we updated the features to account for the fact that this transition system is based on the arc-standard model and so the most relevant positions are the top two tokens on the stack. For the two-register system, we added features over properties of the tokens stored in each of the registers. All experiments use beam search with a beam of size 32 and are trained with ten iterations of averaged structured perceptron training. Training set trees that are outside of the reachable class (projective for arc-eager, 2-Crossing Intervals for two-registers) are transformed by lifting arcs (Nivre and Nilsson, 2005) until the tree is within the class. The test sets are left unchanged. We use the standard technique of parameterizing arc creating actions with dependency labels to produce labeled dependency trees. Experiments use the ten datasets in Table 1 from the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We report numbers using both gold and automatically predicted part-of-speech tags and morphological attribute-values as features. For the latter, the part of speech tagger is a first-order CRF model and the morphological tagger uses a greedy SVM perattribute classifier. Evaluation uses CoNLL-X scoring conventions (Buchholz and Marsi, 2006) and we report both labeled and unlabeled attachment scores.

