Passive-Aggressive Learning We train our model with a structured version of the Passive-Aggressive (PA) algorithm (Crammer et al., 2006). The benefits of using PA in place of a CRF are that we require only Viterbi inference, and memory requirements are minimized, as we update the model one training sentence at a time. PA is an online, large-margin learning algorithm that attempts to separate correct sequences from incorrect ones by a margin of 1. For each update to the weight vector w, we select a training sentence x and its gold-standard tag sequence y . We use dynamic programming to search for a response y ^ that 1 maximizes the structured hinge loss: y ^ = arg max = 1 + wT (x, y ) - (x, y ) (1) where () maps an (x, y ) pair to a feature vector. If the loss is greater than 0, we update our model: w = w +  (x, y ) - (x, y ^) (2)
y =y

Phrase: [yj ], [yj , xs . . . xt-1 ], [yj , lc(xs . . . xt-1 )], [yj , ss(xs . . . xt-1 )] Word, for each i s.t. s  i < t: 2 {[yj , xi+k , k ]}2 k=-2 , {[yj , ers,t (i), xi+k , k ]}k=-2 , 2 {[yj , lc(xi+k ), k ]}k=-2 , {[yj , ers,t (i), lc(xi+k ), k ]}2 k=-2 , {[yj , ss(xi+k ), k ]}2 , k=-2 {[yj , ers,t (i), ss(xi+k ), k ]}2 k=-2 , 3 {[yj , pf(n, xi )]}n=1 , {[yj , ers,t (i), pf(n, xi )]}3 n=1 , 3 {[yj , sf(n, xi )]}3 , { [ y , er ( i ) , sf ( n, x )] } j s,t i n=1 n=1 ,
Table 1: Baseline features (s, t, yj , x). [str] stands for an indicator feature with the name str; lc() maps a string onto its lowercased form; ss() maps a string onto its word shape ("Apple Inc." becomes "Aa Aa."); pf(n, xi ) and sf(n, xi ) are n-character prefixes and suffixes of xi ; and ers,t (i) maps an absolute sentence position i (s  i < t) to a relative entity position drawn from {B, I, L, U }.

where  is an adaptive learning rate that scales the update to the smallest step size that achieves 0 loss:  = min C, 1 + wT (x, y ^) - (x, y ) ||(x, y ) - (x, y ^)|| (3)

C is a hyper-parameter that truncates large steps to prevent over-fitting. It is related to the C -parameter of an SVM (Martins et al., 2010). To further guard against over-fitting, we use the average of all vectors w seen during training when tagging new text (Collins, 2002). Features The feature function (x, y ) must decompose into the semi-Markov dynamic program: (x, y ) =
(s,t,yj )D(x,y )

(s, t, yj , x)

(4)

where D is a derivation decomposing (x, y ) into J entity-tag assignments (s, t, yj ), each asserting that the phrase xs . . . xt-1 is assigned the tag yj . Tagged spans are non-overlapping, and to eliminate spurious
This can be done by running a 2-best tagger. If the 1-best answer is not correct (y = y ), then it maximizes the loss, otherwise, the 2-best answer maximizes the loss.
1

ambiguity, constrained so that Outside can tag only single-word spans (t = s + 1). Our baseline feature set, shown in Table 1, closely mimics the set proposed by Ratnaparkhi (1996), covering word identity, prefixes, suffixes and surrounding words. It has been augmented with phraseidentity indicators and hierarchical word-level tags. These conjoin the entity class yj with the word's entity-relative position, backing off to yj alone. Most features look only at a single word xi , which improves efficiency by allowing the tagger to re-use word-level scores across many phrasal tags. There are some standard NER features that we chose not to include. We follow Lin and Wu (2009) in omitting POS tags and gazetteers in order to reduce our dependence on linguistic resources. We expect similar information to be provided by unsupervised word representations, and we test this assumption in Section 5.2. We omit context aggregation, which accounts for the repetition of entities (Ratinov and Roth, 2009), because Twitter's short message length reduces the utility of document-level features. 3.1 Word Representations

Our primary tool for domain adaptation will be unsupervised word representations, which convey information about a word's distributional profile.

737

