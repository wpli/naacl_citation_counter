· The number of words already chosen to be part of the headline, which could be zero. Therefore, the local feature function f (x, i, si-1 , yi ) will not only know about the whole text and the current token xi , whose category yi is to be assigned, but it will also hold information about the headline constructed so far. In our model, the objective of the local feature function is to return a vector that describes in an abstract euclidean space the outcome of placing, or not placing, token xi in the headline, provided that the words previously chosen form the state si-1 . We decide to make this feature vector consist of 23 signals, which only fire if the token xi is placed in the headline (i.e., yi = 1). The signals can be grouped into the following five sets: Language-model features: they assess the grammaticality of the headline being built. The first feature is the bigram probability of the current token, given the last one placed on the headline. The second feature is the trigram probability of the PoS tag of the current token, given the tags of the two last tokens on the headline. Keyword features: binary features that help the model detect whether the token under analysis is a salient word. The document's keywords are calculated as a preprocessing step via TF-IDF weighting, and the features fire depending on how good or bad the current token xi is ranked with respect to the others on the text. Dependency features: in charge of informing the model about syntactical dependencies among the tokens placed on the headline. For this end the dependency tree of all the sentences in the news article are computed as a pre-processing step2 . Named-entity features: help the system identify named entities3 in the text, including those that are composed of contiguous tokens. Headline-length features: responsible for enabling the model to decide whether a headline is too short or too long. As many previous studies report, an ideal headline must have from 8 to 10 tokens (Banko et al., 2000). Thus, we include three binary features that correspond to the following conditions: (1) if the headline length so far is less than or equal to seven; (2) if the headline length so far is greater
2 3

than or equal to 11; (3) if the token under analysis is assigned to the headline, which is a bias feature.

5

Decoding the model

We use the Stanford toolkit for computing parse trees. We only use PER, LOC, and ORG as entity annotations.

Decoding the model involves solving equation (1) being given a weight vector w, whose value stays unchanged during the process. A naive way of solving the optimization problem would be to try all possible |Y|n sequence combinations, which would lead to an intractable procedure. In order to design a polynomial algorithm that finds the global optimum of the aforementioned formula, the following four observations must be made: (1) Our model is designed so that the local feature function only fires when a token is selected for being part of a headline. Then, when evaluating an arbitrary solution y , only the tokens placed on the headline must be taken into account. (2) When applying the local feature function to a particular token xi (assuming yi = 1), the result of the function will vary only depending on the provided previous state si-1 ; all the other parameters are fixed. Moreover, a new state si will be generated, which in turn will include token xi . This implies that the entire evaluation of a solution can be completely modeled as a sequence of state transitions; i.e., it becomes possible to recover a solution's bitmap from a sequence of state transitions and vice-versa. (3) When analyzing the local feature function at any token xi , the amount of different states si-1 that can be fed to the function depend solely on the tokens taken before, for which there are 2i-1 different combinations. Nevertheless, because a state only holds three pieces of information, a better upperbound to the number of possible reachable states is equal to i2 × |P oS |, which accounts to: all possible candidates for the last token chosen before xi , times all possible combinations of total number of tokens taken before xi , times all possible PoS tags of the one-before-last token taken before xi . (4) The total amount of producible states in n 2 the whole text is equal to i=1 i × |P oS | = 3 O(n × |P oS |). If the model is also constrained to produce headlines containing no more than H tokens, the asymptotic bound drops to O(H × n2 × |P oS |).

136

