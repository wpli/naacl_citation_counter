Rank 1 2 3 4 5 1 2 3 4 5

Summary KPLM without sentence re-ranking (Pyramid score: 0.23) 3.1 decoding mcdonald et al (2005b) use the chu-liuedmonds (cle) algorithm to solve the maximum spanning tree problem. thus far, the formulation follows mcdonald et al (2005b) and corresponds to the maximum spanning tree (mst) problem. while we have presented signi cant improvements using additional constraints, one may won5even when caching feature extraction during training mcdonald et al (2005a) still takes approximately 10 minutes to train. we have successfully replicated the state-of-the-art results for dependency parsing (mcdonald et al, 2005a) for both czech and english, using bayes point machines. the search for the best parse can then be formalized as the search for the maximum spanning tree (mst) (mcdonald et al, 2005b). KPLM with sentence re-ranking (Pyramid score: 0.73) 3.1 decoding mcdonald et al (2005b) use the chu-liuedmonds (cle) algorithm to solve the maximum spanning tree problem. to learn these structures we used online large-margin learning (mcdonald et al, 2005) that empirically provides state-of-the-art performance for czech. while we have presented signi cant improvements using additional constraints, one may won5even when caching feature extraction during training mcdonald et al (2005a) still takes approximately 10 minutes to train. mcdonald et al (2005a) introduce a dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores. we take as our starting point a re-implementation of mcdonald's state-of-the-art dependency parser (mcdonald et al, 2005a).

Table 4: Summaries of paper P05-1012 produced using KPLM. Key facts in citing sentences are highlighted and OCR and sentence segmentation errors have been retained as they originally appeared in the corpus.

and Radev, 2008; Qazvinian et al., 2010) for crossreferencing results from a broader set of systems. 4.2 Results and Discussion Table 5 shows the pyramid score evaluation results for the 25 papers. To facilitate comparison and cross-referencing, the table has been formatted as close as possible to Table 7 in (Qazvinian and Radev, 2008) with figures in the Gold and CLexRank columns directly copied over. Note that a Gold pyramid score less than 1 suggests that there are more facts than can be covered using k sentences for that paper's citation summary. It can be seen that KPLM based summarisation achieves quite comparable results (especially in terms of the median score) with C-LexRank, even without top sentence re-ranking. When the re-ranking is introduced, our system outperforms the current state-ofthe-art C-LexRank by a measurable margin. Albeit the perceived differences in the results, a onetailed Wilcoxon signed-rank test indicated that our results are not statistically superior at significance level 0.05 (Z=-1.22, P=0.11). A power analysis reveals that in order to achieve a statistically significant result on this small sample of 25 papers, a system would need to score a medium to large effect size (Cohen's d > 0.53), which is a challenging task considering C-LexRank's strong baseline performance. We hope this analysis can inform future studies using Qazvinian's 25 papers corpus. Nevertheless, it should be pointed out that our approach is not only substantially simpler than C-LexRank, it also yields more interpretable results. We know of a more recent set of results reported in (Qazvinian et al., 2013), which again confirmed 130

C-LexRank's state-of-the-art status with a mean pyramid score of 0.799 (cf. Table 6 in (Qazvinian et al., 2013)). However those results are not comparable with ours for the following reasons. First, Qazvinian et al. (2013) used a slightly different corpus with 30 papers (5 extra papers from the Conditional Random Field domain). Second, results were based on a summary length limit of 200 words, so roughly equivalent to 6.3 sentences per paper, giving evaluations an extra edge. Both changes boosted system performance in those evaluations, as evidenced by comparing Table 7 in (Qazvinian and Radev, 2008) and Table 6 in (Qazvinian et al., 2013). Qazvinian et al. (2010) used the same corpus and evaluation method as our work; however the results have been presented as box plots (cf. Figure 1 in (Qazvinian et al., 2010)) from which only the fivenumber summary (i.e., minimum, lower quartile, median, upper quartile and maximum) of the pyramid scores can be reconstructed and consequently no significance test can be performed. Compared with the best performing variants of the system devised in (Qazvinian et al., 2010) based on unigrams, bigrams and trigrams, our system (KPLM+TSR) achieves a higher median score (0.86 vs. 0.80), as well as a lower score variation across the 25 papers. An arbitrarily imposed constraint in the evaluations is the summary length limit, which may be changed to suit a specific application context. The summarisation task becomes increasingly more challenging when summary length limit is further tightened as this would require a summariser to pinpoint the best sentences from a potentially large cita-

