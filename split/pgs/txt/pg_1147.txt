of the filtered subset (36 documents plus permutations). The training and test sets do not overlap. In this task, each training and test instance is composed of a pair of a source document and one of its permutations, and the source document is always considered more coherent than its permutation. 5.2 Summary coherence rating The second task is summary coherence rating, in which, given a pair of summaries about the same set of source documents, we determine the ranking of these two summaries based on their degrees of coherence. The performance of the model is assessed by comparing model-induced rankings against the rankings given by human judges. We use the same dataset (DUC 2003) as B&L and G&S did, which consists of summaries generated either by human writers or by automatic summarization systems. Each summary was given a coherence score by averaging among seven judges. Often, machinegenerated summaries receive low coherence scores because they contain sentences taken out of context and thus display problems with respect to coherence. This dataset consists of 16 input document clusters, each of which is associated with five machinegenerated summaries along with a human-written summary. In total, we have 96 summaries (for more details, see B&L). We form pairwise rankings by taking any two summaries originating from the same document cluster, given that the two summaries receive different coherence scores: 144 of the resulting rankings are used for training and 80 are for testing. 5.3 Experiment results In this section, we demonstrate the performance of our models with world knowledge encoded in one of the two ways: paths in a sentence graph or features in an entity grid. We compare our models against the original graph-based model (G&S) and entitybased model (B&L). The evaluation is conducted on the two tasks, sentence ordering and summary coherence rating, and the accuracy is the fraction of correct pairwise rankings. Table 1 shows the performance of various models on both tasks. The first section shows the results of G&S's graph-based local coherence model, including the performance reported in their original paper and that achieved by our re-implementation, repre1093

Model Graph model (G&S) Graph model (Implemented) Graph model + K Graph model + K + Avg R Entity model (B&L) Entity model (Implemented) Entity model + K

SO 88.9 89.6 91.3** 93.4** 88.9 93.7 95.1**

SCR 80.0 48.8 50.0 55.0* 83.8 90.0 91.3

Table 1: Accuracies (%) of various models on the two tasks, sentence ordering (SO) and summary coherence rating (SCR). Models that perform significantly better than their corresponding reimplemented basic models are denoted by ** ( p < .01) or * ( p < .05), verified using paired t-test. senting the effect with no world knowledge encoded. The second section shows the performance of our two graph-based models with world knowledge encoded. Graph model + K is the basic model with world knowledge encoded, but coherence is simply measured as the average out-degree as in G&S's approach. Graph model + K + Avg R replaces the out-degree measurement by our average reachability score (described in Section 4.1.3), which measures coherence in a more sophisticated way. The third section shows the results of B&L's entity-based local coherence model, including the originally reported performance and that obtained by our reimplementation, in which no world knowledge features are included. The last section, Entity model + K, shows the result of entity-based model with our world knowledge features encoded. Note that the random baseline of both tasks is 50%. Firstly, for graph-based models, our Graph model + K outperforms the original models, suggesting that world knowledge is truly helpful for capturing more coherence information3 . Moreover, by introThe large discrepancy between the performance reported by G&S and that of our re-implementation in Task 2 is due to the fact that G&S experimented with a set of specially formed summary pairs (see their paper for detail), which we have no access to. They also did not give sufficient details about how they constructed those summary pairs, which has a great impact on the final result. This made it difficult for us to fully reimplement their experiment. So we use B&L's set of summary pairs, which are generated randomly and are more difficult to distinguish, which explains our differing results from theirs.
3

