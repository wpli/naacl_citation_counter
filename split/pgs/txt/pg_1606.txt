In ACT, the emotion triggered by an event is a function of the fundamental identity for actor or object, i  {Se , Sp , Sa } or i  {Oe , Op , Oa }, and the transient identity for actor or object, i  {Se , Sp , Sa } or i  {Oe , Op , Oa }. ACT uses the following equation to predict emotions, with empirically measured coefficients as follows:   E (i - I i -  ) (4)

where E is a 3 × 3 matrix coefficient of the emotion profile, I is a 3×3 matrix coefficient for identity, and  is a vector of equation constants. 3.3 Emotion Elicitation Using ACT

We implement ACT to predict the triggered sentiment of a single sentence as follows: first we extract the subject, verb, object, setting, and modifiers (adjectives of subject and object) and look them up in the augmented ACT lexicon (see section 3.4) to get EPA values (fundamental sentiments, f ) for each word (MacKinnon, 2006). We next compute the transient impression  using f and Equations 1 and 2. After that, we compute the deflection using Equation 3, and the emotion towards the subject and object using Equation 4. We then map the resulting EPA scores for emotion to the nearest emotions label in ACT. The ACT dataset (MacKinnon, 2006) has 135 emotion labels, each with an EPA score (e.g., delighted= [2.04, 0.96, 1.48]), and we find the closest label using a Euclidean distance measure. We then compare these emotions  towards subject and object to corresponding ground truth in the news headline dataset (see Section 4) using root mean squared error (RMSE) and mean absolute error (MAE). We also discretize the predicted  and the ground truth into negative or positive EPA values, and compare accuracy of the discretized values. To extract the sentence's quintet (i.e., subject, verb, object, modifiers, and settings), we implement a search algorithm that takes a treebank parse tree (Socher et al., 2013) and returns a (subject, predicate , and object) triplet (Rusu et al., 2007). As Rusu et al.'s algorithm searches a parse tree of grammatically correct English sentences, we make some alterations to consider news headlines' grammar. News headlines are often written to be short and precise (i.e., often not grammatically correct), 1552

and usually consist of several noun phrases without articles or the verb "to be". They are written in the present tense for current or past events (e.g, terror strikes police base). The past tense verbs are rarely used in news headlines, whereas passive voice sentences are common. The passive voice sentences are often written without an auxiliary verb, which make it hard for standard parsers to distinguish their verbs from past tense verbs and to extract the triplet accurately (e.g, Six killed in accident). Our algorithm takes a parse tree and performs a breadth-first search and identifies the last noun descendent of the first noun phrase (NP) in the sentence as the subject and the previous descendent as the attributes (in Rusu et al.'s algorithm the subject is the first noun in (NP)). For example, in the sentence Super Bowl-winning quarterback Russell Wilson divorces wife, the actor/subject "Russell" is the last noun in the first noun phase and the previous adjectives and nouns are attributes (modifiers) of the subject. To locate the verbs, (similar to Rusu et al.'s algorithm) the algorithm searches the deepest verb phrase (VP) and returns the first verb (VB) descendent. If the verb is in the past tense, we transform it to passive voice. The algorithm (similar to Rusu et al.'s algorithm) returns the object that is co-located with the verb in the deepest verb phrase (VP). To extract the settings, we look for a noun phrase (NP) sub-tree that has a preposition (at, in, on) and return the last noun. This algorithm yields accuracies of 43%, 53% , and 26% with the ground truth (users' annotations of the subject, verb, and object). We also consider whether the verb type is transitive, which directly indicates positive or negative sentiment toward something (e.g., x killed y ), or intransitive, which transfers sentiments into nouns (e.g., x provides help to y ). For intransitive verbs, we choose the second verb as the behavior (verb) in the sentence (e.g., "x provides help to y " will be "x helps y "). We also used part-of-speech tagging to determine the elements of an event and to identify the places and names. The gender of the names is considered by training a na¨ ive Bayes classifier on names-gender corpus of 5001 female and 2943 male names1 which yielded an accuracy of 86% on classifying names according to their gender.
1

www.nltk.corpus

