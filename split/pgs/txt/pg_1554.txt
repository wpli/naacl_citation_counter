Model HVC (Thomason et al., 2014) FGM (Thomason et al., 2014) LSTMf lickr LSTMcoco LSTM-YT LSTM-YTf lickr LSTM-YTcoco LSTM-YTcoco+f lickr

S% 86.87 88.27 79.95 56.30 79.40 84.92 86.58 87.27

V% 38.66 37.16 15.47 06.90 35.52 38.66 42.23 42.79

O% 22.09 24.63 13.94 14.86 20.59 21.64 26.69 24.23

Model FGM (Thomason et al., 2014) LSTM-YT LSTM-YTf lickr LSTM-YTcoco LSTM-YTcoco+f lickr

BLEU 13.68 31.19 32.03 33.29 33.29

METEOR 23.90 26.87 27.87 29.07 28.88

Table 1: SVO accuracy: Binary SVO accuracy compared against any valid S,V,O triples in the ground truth descriptions. We extract S,V,O values from sentences output by our model using a dependency parser. The model is correct if it identifies S,V, or O mentioned in any one of the multiple human descriptions.

Table 3: Scores for BLEU at 4 (combined n-gram 1-4), and METEOR scores from automated evaluation metrics comparing the quality of the generation. All values are reported as percentage (%).

Model FGM (Thomason et al., 2014) LSTM-YT LSTM-YTcoco LSTM-YTcoco+f lickr GroundTruth

Relevance 2.26 2.74 2.93 2.83 4.65

Grammar 3.99 3.84 3.46 3.64 4.61

Model HVC (Thomason et al., 2014) FGM (Thomason et al., 2014) JointEmbed1 (Xu et al., 2015) LSTMf lickr LSTMcoco LSTM-YT LSTM-YTf lickr LSTM-YTcoco LSTM-YTcoco+f lickr

S% 76.57 76.42 78.25 70.80 47.44 71.19 75.37 76.01 75.61

V% 22.24 21.34 24.45 10.02 02.85 19.40 21.94 23.38 25.31

O% 11.94 12.39 11.95 07.84 07.05 09.70 10.74 14.03 12.42

Table 4: Human evaluation mean scores. Sentences were uniquely ranked between 1 to 5 based on their relevance to a given video. Sentences were rated between 1 to 5 for grammatical correctness. Higher values are better.

Table 2: SVO accuracy: Binary SVO accuracy compared against most frequent S,V,O triple in the ground truth descriptions. We extract S,V,O values from parses of sentences output by our model using a dependency parser. The model is correct only if it outputs the most frequently mentioned S, V, O among the human descriptions.

displayed only the sentences and did not show any video. Here, workers had to choose a rating between 1-5 for each sentence. Multiple sentences could have the same rating. We discard responses from workers who fail gold-standard items and report the mean ranking/rating for each of the evaluated models in Table 4. Individual Frames. In order to evaluate the effectiveness of mean pooling, we performed experiments to train and test the model on individual frames from the video. Our first set of experiments involved testing how well the image description models performed on a randomly sampled frame in the video. Similar to Tables 1 and 2, the model trained on Flickr30k when tested on random frames from the video scored better on subjects and verbs with any valid accuracy of 75.16% and 11.65% respectively; and 9.01% on objects. The one trained on COCO did better on objects (12.54%, any valid accuracy) but very poorly on subjects and verbs. In our next experiment, we used image description models (trained on Flickr30k, COCO or a combination of both) and fine-tuned them on individual frames in the video by picking a different frame

uations, we compare the generated sentences using both and present our results in Table 3. Human Evaluation. We used Amazon Mechanical Turk to also collect human judgements. We created a task which employed three Turk workers to watch each video, and rank sentences generated by the different models from "Most Relevant" (5) to "Least Relevant" (1). No two sentences could have the same rank unless they were identical. We also evaluate sentences on grammatical correctness. We created a different task which required workers to rate sentences based on grammar. This task
They evaluate against a filtered set of groundtruth SVO words which provides a tiny boost to their scores.
1

1500

