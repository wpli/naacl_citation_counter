point may be because news documents tend to have crucial sentences (as well as the headline) at the beginning. On this task, while both seq and bow-CNN outperform the baseline methods, bow-CNN outperforms seq-CNN, which indicates that in this setting the merit of having fewer parameters is larger than the benefit of keeping word order in each region. Now we turn to parallel CNN. On IMDB, seq2CNN, which has two seq-convolution layers (region size 2 and 3; 1000 neurons each; followed by one unit of max-pooling each), outperforms seq-CNN. With more neurons (3000 neurons each; Table 3) it further exceeds the best-performing baseline, which is also the best previous supervised result. We presume the effectiveness of seq2-CNN indicates that the length of predictive text regions is variable. The best performance 7.67 on IMDB was obtained by `seq2-bown-CNN', equipped with three layers in parallel: two seq-convolution layers (1000 neurons each) as in seq2-CNN above and one layer (20 neurons) that regards the entire document as one region and represents the region (document) by a bag-of-n-gram vector (bow3) as input to the computation unit; in particular, we generated bow3 vectors by multiplying the NB-weights with binary vectors, motivated by the good performance of NB-LM. This third layer is a bow-convolution layer13 with one region of variable size that takes one-hot vectors with n-gram vocabulary as input to learn document embedding. The seq2-bown-CNN for Elec in the table is the same except that the regions sizes of seqconvolution layers are 3 and 4. On both datasets, performance is improved over seq2-CNN. The results suggest that what can be learned through these three layers are distinct enough to complement each other. The effectiveness of the third layer indicates that not only short word sequences but also global context in a large window may be useful on this task; thus, inclusion of a bow-convolution layer with ngram vocabulary with a large fixed region size might be even more effective, providing more focused context, but we did not pursue it in this work. Baseline methods Comparing the baseline methods with each other, on sentiment classification, reducing the vocabulary to the most frequent n-grams
13 It can also be regarded as a fully-connected layer that takes bow3 vectors as input.

methods SVM bow3 (30K) SVM bow1 (all) SVM bow2 (all) SVM bow3 (all) NN bow3 (all) NB-LM bow3 (all) bow-CNN seq-CNN seq2-CNN seq2-bown-CNN

IMDB 10.14 11.36 9.74 9.42 9.17 8.13 8.66 8.39 8.04 7.67

Elec 9.16 11.71 9.05 8.71 8.48 8.11 8.39 7.64 7.48 7.14

RCV1 10.68 10.76 10.59 10.69 10.67 13.97 9.33 9.96 ­ ­

Table 2: Error rate (%) comparison with bag-of-n-gram-

based methods. Sentiment classification on IMDB and Elec (25K training documents) and 55-way topic categorization on RCV1 (16K training documents). `(30K)' indicates that the 30K most frequent n-grams were used, and `(all)' indicates that all the n-grams (up to 5M) were used. CNN used the 30K most frequent words.

SVM bow2 [WM12] 10.84 ­ WRRBM+bow [DAL12] 10.77 ­ NB+SVM bow2 [WM12] 8.78 ensemble NB-LM bow3 [MMRB14] 8.13 ­ Paragraph vectors [LM14] 7.46 unlabeled data seq2-CNN (3K×2) [Ours] 7.94 ­ seq2-bown-CNN [Ours] 7.67 ­ Table 3: Error rate (%) comparison with previous best methods on IMDB.

notably hurt performance (also observed on NB-LM and NN) even though some reduction is a common practice. Error rates were clearly improved by addition of bi- and tri-grams. By contrast, on topic categorization, bi-grams only slightly improved accuracy, and reduction of vocabulary did not hurt performance. NB-LM is very strong on IMDB and poor on RCV1; its effectiveness appears to be datadependent, as also observed by WM12. Comparison with state-of-the-art results As shown in Table 3, the previous best supervised result on IMDB is 8.13 by NB-LM with bow3 (MMRB14), and our best error rate 7.67 is better by nearly 0.5%. (Le and Mikolov, 2014) reports 7.46 with the semisupervised method that learns low-dimensional vector representations of documents from unlabeled data. Their result is not directly comparable with our supervised results due to use of additional resource. Nevertheless, our best result rivals their result. We tested bow-CNN on the multi-label topic categorization task on RCV1 to compare with

109

