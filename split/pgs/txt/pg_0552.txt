speaker may, particularly in tandem with other features, be informative about topic. Object (downstream): the direct object of the VP sister to the subject (if any). This feature exploits the fact that pronouns occurring as direct objects within a clause cannot be the same as the (zero) pronoun in subject position of that clause. Has question particle: a binary value indicating whether the clause contains a) a question particle or wh-word, or b) a question mark. This feature is likely to be a strong indicator of that the subject pronoun is not first person (also used by (Chen and Ng, 2013)). For example, in our training data we found that only 16% of the clauses with question particle were marked with first person label i.e. 1v or 1h. Bag of words: all words occurring in the clause. Apart from the verb, other words can also be highly informative about the nature of the subject. Bag of parts of speech: all parts of speech occurring in the clause. The structural make-up of clause may be informative about focus, for instance in the case of passive or possessive constructions. Hidden subject particles: a feature indicating whether the clause consists of a list of phrases consistently tagged with empty categories on the Chinese side, but consistently translated without subject pronouns on the English side (thus likely to correspond to labels 1h-3h). This feature is intended to help the model in recognizing clauses consistently corresponding to "hidden" labels. In addition, for the features that consist of sequence (bag of words, bag of part of speech, object, etc.) we additionally compute bigrams and trigrams. 3.3 Structured prediction We cast the above model as a sequence labeling problem over visible and hidden labels. We consider each conversation segment in the SMS as an input data sequence x = x1 , x2 , . . . , xn where each xi corresponds to a clause in Chinese. Each clause in Chinese is assigned a label from the label space Y = {1v, 2v, 3v, 1h, 2h, 3h, none}. The task then is to assign labels y = y1 , y2 , . . . , yn to the input data sequence from the label space Y based on the features described in Section 3.2. At training time we assign labels to the input sequence using the "bronze standard" method described in Section 3.1. 498

To train the sequence labeling model, we use an online variant of the DAgger imitation learning algorithm (Ross et al., 2011) as implemented in the Vowpal Wabbit machine learning library (Langford et al., 2007; Daum´ e III et al., 2014). DAgger, like its predecessor S EARN (Daum´ e III et al., 2009), solves structured prediction problems by transforming them into sequential decision making problems. In the case of sequence labeling, the natural order for sequence decision making is left-to-right. At test time, inference is performed greedily. At training time, the learning algorithm attempts to balance between training on "oracle" states (prefixes of decisions made optimally according to the true labels) and training on "system" states (prefixes of decisions made sub-optimally according to the learned model). The online variant of DAgger balances this trade-off by slowly transitioning from making past decisions optimally to making them using the currently learned predictor.

4

Experiments

Our goal in our experiments is to answer the following questions: 1. How well does the bronze-standard annotation capture the underlying truth? (Section 4.2). 2. Is our model able to leverage both dialogue structure and semantic content to accurately resolve pronouns? (Section 4.3) 3. How important are the different components in our model in making effective predictions? (Section 4.4) In the following sections, we describe the experiments we perform aimed at answering these questions. First, we describe the data we use for experimentation. 4.1 Experimental setup

For training our focus-tracking model, we use Chinese-English parallel data from the SMS/chat domain available as part of training data used in the Machine Translation task under the DARPA BOLT project. The training data consisted of 117k sentences. We test our model on heldout SMS/Chat data consisting of 1152 sentences (hand-annotated,

