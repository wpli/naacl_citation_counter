it could be added in principle.) Assigning a semantic class such as N : LOCATION to END would, in our judgment, be overly literal. To paint a coherent picture of the meaning of this sentence, it is better to treat HIGH END as a single unit, and because it serves as an adjective rather than a noun or verb, leave it semantically unclassified.4 STEAK HOUSE is arguably an entrenched enough compound that it should receive a single supersense-- in fact, WordNet spells it without a space. The phrases white pages, high school, (get) in touch (with), track. . . down, and one by one all are listed as MWEs in WordNet. As detailed in §4.1 below, the conventional BIO scheme used in existing supersense taggers is capable of representing most of these. However, it does not allow for gappy (discontinuous) uses of an expression, such as track people down. The corpus and analyzer presented in this work address these shortcomings by integrating a richer, more comprehensive representation of MWEs in the supersense tagging task.

The same annotators had already done the MWE annotation; whenever they encountered an apparent mistake from an earlier stage (usually an oversight), they were encouraged to correct it. Our annotation interface supports modification of MWEs as well as supersense labels in one view. To lessen the cognitive burden when reasoning about tagsets, supersense annotation was broken into separate phases: first we annotated nearly the entire R EVIEWS corpus for noun supersenses; then we made another pass to annotate for verbs. Roughly a tenth of the sentences were saved for a combined noun+verb phase at the end; annotators reported that constantly switching their attention between the two tagsets made this mode of annotation more difficult. 3.1 Nouns

3

Supersense Annotation for English

As suggested above, supersense tags offer a practical semantic label space for an integrated analysis of lexical semantics in context. For English, we have created the STREUSLE dataset, which fully annotates the R EVIEWS corpus (55k words) for noun and verb supersenses in a manner consistent with Schneider et al.'s (2014b) multiword expression annotations. Schneider et al. (2012) offered a methodology for noun supersense annotation in Arabic Wikipedia, and predicted that it would port well to other languages and domains. Our experience with English web reviews has borne this out. We generally adhered to the same supersense annotation process (for nouns); the most important difference was that the data had already been annotated for MWEs, and supersense labels apply to any strong5 MWEs as a whole.
Future supersense annotation schemes for additional parts of speech could be assimilated into our framework. Tsvetkov et al. (2014) take a step in this direction for adjectives. 5 The CMWE corpus distinguishes strong and weak MWEs-- essentially, the former are strongly entrenched and likely noncompositional, whereas weak MWEs are merely statistically collocated. See Schneider et al. (2014b) for details. Because they are deemed semantically compositional, weak MWEs do not receive a supersense as a whole.
4

Targets. Per the annotation standard, all noun singletons and noun-like MWEs should receive a noun supersense label. Annotation targets were determined heuristically from the gold (PTB-style) POS tags in the corpus: all lexical expressions containing a noun6 were selected. This heuristic overpredicts noun-like MWEs occasionally because it does not check the syntactic status of the MWE as a whole. During this phase, the backtick symbol (`) was therefore reserved for MWEs (such as light verb constructions) that contain a noun but should not receive a noun supersense.7 The annotation interface prevented submission of blank annotation targets to avoid oversights. Tagset conventions. Several brief annotation rounds were devoted to practice with Schneider et al.'s (2012) noun annotation guidelines,8 since the annotators were new to the scheme. Metonymy posed the chief difficulty in this domain: institutions with a premises (such as restaurants, hotels, and schools) are frequently ambiguous between N : GROUP (institution as a whole), N : ARTIFACT (the building), and N : LOCATION (site as a whole). Our convention was to use the reading that seemed most salient in context: for example, restaurant in a comment about the qualSpecifically, any POS tag starting with N or ADD (web addresses); pronouns were excluded. 7 Pronouns like anything also fall into this category because they are POS-tagged as nouns.
8 6

http://www.ark.cs.cmu.edu/ArabicSST/corpus/

guidelines.html

1539

