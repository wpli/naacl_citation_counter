Then, we remove non-alphabet characters, numbers, pronoun, punctuation and stop words from the text. Finally, the WordNet stemmer3 is applied to reduce the vocabulary size and settle the issue of data sparseness. For Chinese dataset, we first perform Chinese word segmentation with a popular Chinese auto-segmentation system ICTCLAS4 . Then, the words about time, numeral words, pronoun and punctuation are removed as they are unrelated to the sentiment analysis task. 4.2 Implementation Details We specify the hyper-parameters we use for the experiments. For all datasets, we choose  = 0.5, (p) = (0.95, 0.25, 0.4), (n) = (0.25, 0.95, 0.4), (u) = (0.6, 0.6, 0.4) and (0 , 1 ) = (0.25, 0.75). We use cross-validation to set the number of topics on datasets MR, SemEval and COAE as 20, 10 and 20, respectively. The seed words used to construct English and Chinese lexicons are the same as in previous literatures (Xie and Li, 2012) and (Yang et al., 2014). For the corpus-based method, each document is transformed into binary vectors which encodes the presence/absence of the terms. The autoencoder is constructed with 500 input neurons and 200 hidden neurons. Each autoencoder is trained by back propagation with 400 iterations. For all datasets, we set the iteration number of co-training to be k = 50. Other parameters of cotraining are chosen by cross-validation: u is set to be 10% of all unlabeled data, the sum of p and n are 0.8% of all unlabeled data, while their ratio are determined by the ratio of positive and negative samples in labeled training data. 4.3 Baseline Methods In this paper, we evaluate and compare our approach with an unsupervised method, two supervised methods and a variety of semi-supervised methods: SVM: 5000 words with greatest information gain are chosen as features. In our experiment, we use the LibLinear5 implementation of SVM. Lexical Classifier (LC): This method calculates the number of positive words and negative words contained in the Opinion Lexicon (Hu and Liu,
3 4

2004) for English texts or the HowNet6 lexicon for Chinese texts. If the positive sentiment words are more than negative words, then the document is classified as positive, and vice versa. Self-learning: Following the idea of (Zhu, 2006), this method uses the unlabeled data in a bootstrapping way. The SVM classifier is used to select most confident unlabeled samples in each iteration. Transductive SVM (TSVM) : Following the idea of (Joachims, 1999), this method seeks the largest separation between labeled and unlabeled data through regularization. We implement it with the SVM-light toolkit 7 . Dasgupta's method: This is a popular semisupervised approach to automatic sentiment classification proposed by Dasgupta and Ng (Dasgupta and Ng, 2009). The unambiguous reviews are first mined using spectral techniques, then classified by a combination of active learning, transductive learning, and ensemble learning. Li's method: This method is proposed in (Li et al., 2010). An unsupervised bootstrapping method is adopted to automatically split documents into personal and impersonal views. Then, two views are combined by an ensemble of individual classifier generated by each view. The co-training algorithm is utilized to incorporate unlabeled data. Nguyen's method: This method is proposed in (Nguyen et al., 2014), which achieves the stateof-the-art results in supervised sentiment classification. We follow all the settings in (Nguyen et al., 2014). For the document with no associated score, we predict a score for the document as the values of the rating-based features using a regression model learned from SRA148 dataset. 4.4 Experiment Results For each dataset, we use 80% instances as the training data and the remaining are used for testing. To test the performance of semi-supervised learning, we randomly select 10% of the training instances as labeled data and treat the remaining as unlabeled. For fair comparison, the fully supervised SVM and Nguyen's method use the 10% labeled data for training.
6 7

http://wordnet.princeton.edu/ http://www.ictclas.org 5 http://www.csie.ntu.edu.tw/~cjlin/liblinear/

http://www.keenage.com/download/sentiment.rar http://svmlight.joachims.org/ 8 https://sites.google.com/site/nquocdai/resources

552

