didate sentences. This is expected to better evaluate the salience and the novelty of the sentences. We use a regression model (SVR) for this reranking purpose. When training the model, a sentence's ROUGE2 score compared with the human generated summary is used as the regression target. After reranking, we just select the top sentences that satisfy the length constraint to form the final summary. In this work we do not use any redundancy removal (e.g., MMR method). This is because the ILP decoding process tries to find a global optimal set maximizing the concept coverage, subject to the length constraint, and thus already considers redundancy among sentences. Typically when the initial set (i.e., the output from the first ILP step) is not too big, redundancy is not a big problem.

there is limited research on update summarization and we cannot find better published results for these data sets than the TAC best systems.  Supervised ILP. This is our supervised ILP method where bigram weights are learned discriminately. It is the one-step system that generates the summary with the target length. We use the same bigram set as the ILP baseline system. For this method, we show results using different features: only using the importance features; and using all the features. This is used to evaluate the impact of the novelty features on the update summarization task.  Two-step method: supervised ILP followed by sentence reranking. We generate 200 (value of N ) words summary in the ILP system. Two different configurations are also used: with and without the sentence novelty features in the sentence ranking module. All the features (including the novelty features) are used in the ILP pre-selection step.  Sentence ranking without ILP. In this experiment, we do not use the ILP summarization module to generate candidate sentences first, but just apply sentence ranking to the entire data set. Then MMR is leveraged to select the final summary sentences. Again, we present results using different feature sets. We can see from the tables that the supervised ILP model outperforms the unsupervised one. After including the novelty related features, the model can assign higher weights for the bigrams with novel information, resulting in improved summarization performance. There is further improvement when using our 2-step approach with the sentence reranking model. Our proposed method (ILP followed by sentence reranking, and using all the features) outperforms the TAC best result in 2010 and 2011, and also yields competitive results in the other data sets. The gain of ROUGE-2 of our proposed system compared with the ILP baseline is statistically significant based on ROUGE's 95% confidence. When using sentence ranking on the entire document set, without the ILP pre-selection step, its performance is worse than our proposed method. This shows the benefit of doing pre-selection using the ILP module. Finally,

3

Experiments and Results

3.1 Data and Experiment Setup We evaluate our methods using several recent TAC data sets, from 2008 to 2011. Every topic has two sets of 10 documents (Set A and B). The update task aims to create a 100-word summary from Set B given a topic query and Set A. When evaluating on one year's data, we use the data from the other three years as the training set. This applies to both the supervised ILP method and the sentence reranking regression model. All the summaries are evaluated using ROUGE (Lin, 2004). An academic free solver2 does all the ILP decoding and libsvm3 is used for SVR implementation. 3.2 Results Table 2 and Table 3 show the R2 and R-SU4 values on different TAC data sets for the following systems.  ILP baseline. This is the unsupervised ILPbased summarization system (Gillick et al., 2009), in which only bigrams with document frequency greater than 2 are used in the ILP summarization process, and weight wi is the document frequency of that bigram.  TAC best. This is the best result in the TAC update summarization evaluation.4 Note that
http://www.gurobi.com http://www.csie.ntu.edu.tw/~ cjlin/libsvm/ 4 The ID of the TAC best system from 2008 to 2011 is 14,40,16 and 43.
3 2

1320

