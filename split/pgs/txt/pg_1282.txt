Ellison and Kirby (2006) use scaled edit distance (normalized by average length) to measure the intralexical divergence in a language. The inter-language distance matrix is supplied to a clustering algorithm to infer a tree for the Indo-European language family. The authors only perform a qualitative evaluation of the inferred tree. The authors mention string kernels but do not pursue this line of research further. Bouchard-C^ ot´ e et al. (2013) employ a graphical model to reconstruct the proto-word forms from the synchronic word-forms for the Austronesian language family. They compare their automated reconstructions with the ones reconstructed by historical linguists and find that their model beats an edit-distance baseline. However, their model has a requirement that the tree structure between the languages under study has to be known beforehand. Hauer and Kondrak (2011) ­ referred to as HK ­ supply different string similarity scores as features to a SVM classifier for determining if a given word pair is a cognate or not. The authors also employ an additional binary language-pair feature ­ that is used to weigh the language distance ­ and find that the additional feature assists the task of semantic clustering of cognates. In this task, the cognacy judgments given by a linear classifier is used to flat cluster the lexical items belonging to a single concept. The clustering quality is evaluated against the gold standard cognacy judgments. Unfortunately, the experiments of these scholars cannot be replicated since the partitioning details of their training and test datasets is not available. In our experiments, we compare our system's performance with the performance of the classifiers trained from HK-based features. In the next section, we will describe string similarity measures, subsequences features, dataset, and results.

Swedish i and Russian v `in', which are cognates, is 1. The edit distance treats both of the cognates at the same level and does not reflect the amount of change which has occurred in the Swedish and Russian words from the PIE word. Dice is another string similarity measure that defines similarity between two strings as the ratio between the number of common bigrams to the total number of bigrams. The similarity between Lusatian dolhi and Czech dluhe `long' is 0 since they do not share any common bigrams and the edit distance between the two strings is 3. Although the two words share all the consonants, the Dice score is 0 due to the intervening vowels. Another string similarity measure, Longest Common Subsequence (LCS) measures the length of the longest common subsequence between the two words. The LCS is 4 (hund), 0 (i and v), and 3 (dlh) for the above examples. One can put forth a number of examples which are problematical for the commonly-used string similarity measures. Alternatively, string kernels in machine learning research offer a way to exploit the similarities between two words without any restrictions on the length and character similarity. 3.2 Subsequence features Subsequences as formulated below weigh the similarity between two words based on the number of dropped characters and combine phoneme classes seamlessly. Having motivated why subsequences seems to be a good idea, we formulate subsequences below. We follow the notation given in Shawe-Taylor and Cristianini (2004) to formulate our representation of a string (word). Let  denote the set of phonetic alphabet. Given a string s over , the subsequence vector (s) is defined as follows. The string s can be decomposed as s1 , . . . , s|s| where |s| denotes  - the length of the string. Let I denote a sequence of indices (i1 , . . . , i|u| ) where, 1  i1 < . . . < i|u|  |s|. Then, a subsequence u is a sequence  - of characters s[ I ]. Note that a subsequence can occur multiple times in a string. Then, the weight -   -  l( I ) where, of u, u (s) is defined as - I :u=s[ I ]  - l( I ) = i|u| - i1 + 1 and   (0, 1) is a decay factor. The subsequence vector (s) is composed of

3
3.1

Cognate identification
String similarity features and issues

Edit distance counts the minimum number of insertions, deletions, and substitutions required to transform one word into another word. Identical words have 0 edit distance. For example, the edit distance between two cognates English hound and German hund is 1. Similarly, the edit distance between 1228

