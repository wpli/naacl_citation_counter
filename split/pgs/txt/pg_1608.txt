lexicon, and because the Wordnet-similarity measure that compares only words of the same part-ofspeech, only several adverbs were generated using label-propagation algorithm.
RMSE E P A Adjectives 378 1.2 1.2 0.9 Adverbs 5 1.0 1.1 1.3 2,787 1.2 1.1 0.8 Verbs Noun 7,079 1.3 1.1 0.9 Total 10,249 1.3 1.1 0.9 POS W E 0.9 0.7 1.0 1.0 1.0 MSA P A 1.0 0.8 0.8 0.9 0.9 0.6 0.9 0.7 0.9 0.7

Words Testing-EPA-lexicon LP-lexicon Incapable (adj.) [-1.83, -1.40, -0.54] [-1.56, -1.18, -2.59] Wrongly (adv.) [-1.96, -0.22, 0.17] [-2.02, -0.23, 0.18] Gauge (v.) [0.12, -0.55, 0.13] [0.18, -1.61, 0.25] Loser (n.) [-1.30, -1.75, 0.30] [-1.14, -1.52, 0.28] Table 2: Words and their EPA ratings from Testing-EPAlexicon and LP-lexicon=label propagation lexicon

5

Results

Table 1: The results of comparing the induced lexicon using label propagation and ground truth EPA values (POS= part-of-speech, W= the number of the induced words, MAS=mean absolute error, and RMSE= root mean squared error

4

Datasets

We evaluated the proposed method on a newly collected news-headlines dataset. We collected 2080 news-headlines from a group of news websites and archives (BBC, CNN, Reuters, The Telegraph, Times, etc). The news headlines were selected randomly from the period from 1999 to 2014. Through Mechanical Turk, we recruited participants located in North America with more than 500 approved hits and an approved rate above 90%. We asked the participants to locate the subject (actor), behavior (verb), and object of each sentence and to indicate their emotions towards them and towards the event (as a whole) in the EPA format  [-4.3, +4.3] (where -4.3 indicates strongly negative EPA value and +4.3 indicates strongly positive EPA value). The dataset was annotated by at least three judges per headline. We excluded any ratings that were filled with blanks, zeros, or similar values in all the fields. We also excluded the answers that did not have the appropriate subject, verb, or object form (e.g., behavior=Obama, subject=as). We also excluded all the answers rated by less than three participants. This screening resulted in 1658 headlines that had a mean EPA rating of 0.80, 1.04, and 1.02. Of these, 995 headlines had a positive evaluation score and 663 headlines had a negative evaluation score. Some examples from this dataset can be seen in Table 5. 1554

Our model (the augmented lexicon, and ACT equations) was evaluated in predicting the evoked sentiment towards the headlines as a whole by comparing the discretized evaluation (E) score  {0, 1} (where 0/1 indicates negative/positive emotions, resp.) of the generated EPA to the ground truth. This evaluation was performed using different configurations (Table 3): (1) Using users' annotated triplet (ACTUA) (i.e., subject, verb and object), the model yielded a precision of 75% compared to the ground truth. (2) Using the parse tree triplet (ACT-PTT) (see Section 3.3 for details), the precision dropped to 71%. (3) Adding the adjectives ( modifiers ) and settings to the subject, verb and object, which we will called parse tree quintet (ACT-PTQ) yielded a higher precision, with 82% precision in comparison with the corresponding ground truth. These results were also compared to the results obtained from a standard sentiment classifier (STD-calssifier) that uses occurrence frequencies of positive vs. negative words using SentiWordNet (Das and Bandyopadhyay, 2010). This classifier yielded a precision of 57% in comparison to the ground truth (Table 3). The parse tree quintet (ACT-PTQ) were also used to evaluate the ACT predicted emotions towards the actor (subject) and the object in the headline (Table 4). Three metrics were used in this evaluation: precision, MAS, and RMSE. We used precision to compare the disctized EPA scores  {0, 1} and MAS and RMSE to compare the real EPA scores  [-4.3, +4.3]. As shown in Table 4, the RMSE and MAS are almost all less than 1.5, and the precision varies from 67% to 79% across discretized E,P,A for subject and object. To put these results in context, a difference of 1.4 in the EPA space would equate to the difference between "accusing" someone ({-1.03, 0.26, 0.29}) and "pun-

