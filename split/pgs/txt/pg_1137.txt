Structured perceptron loss: Structured hinge loss: Structured ramp loss:
G

-score (G ) -score (G ) - max(score (G) - cost (G; G ))

+ + +

max score (G) max(score (G) + cost (G; G )) max(score (G) + cost (G; G ))
G G G

Table 4: Loss functions minimized in parameter estimation. G denotes the gold-standard summary graph. score (·) is as defined in Equation 1. cost (G; G ) penalizes each vertex or edge in G  G \ (G  G ). Since cost factors just like the scoring function, each max operation can be accomplished using a variant of ILP decoding (§4.2.1) in which the cost is incorporated into the linear objective while the constraints remain the same.
Subgraph Prediction Nodes R (%) F (%) 46.1 42.6 47.9 44.2 63.5 58.7 61.3 56.8 86.4 80.7 90.1 83.9 48.9 48.3 55.6 54.6 74.8 76.4 45.2 44.7 51.5 50.7 68.9 71.2 Summarization ROUGE-1 R (%) 27.1 28.3 39.0 37.4 52.8 35.0 33.6 40.0 40.0 43.7

goldstandard parses

Perceptron Hinge Ramp Ramp + Expand Oracle Oracle + Expand Perceptron Hinge Ramp Ramp + Expand Oracle Oracle + Expand

P (%) 39.6 41.2 54.7 53.0 75.8 78.9 42.2 41.7 48.1 47.5 64.1 66.9

Edges F (%) 24.7 26.4 39.0 36.1 52.2 64.0 14.5 15.8 20.0 19.0 31.1 46.7

P (%) 41.4 42.6 51.9 50.4 89.1 46.1 44.9 50.6 51.2 87.5

F (%) 32.3 33.5 44.3 42.8 65.8 39.5 38.2 44.4 44.7 57.8

JAMR parses

Table 5: Subgraph prediction and summarization (to bag of words) results on test set. Gold-standard AMR annotations are used for model training in all conditions. "+ Expand" means the result is obtained using source graph with expansion; edge performance is measured ignoring labels.

uating such less well-formed summaries, such as those generated from speech transcripts (Liu and Liu, 2013). Oracle summaries are produced by taking the gold-standard AMR parses of the reference summary, obtaining the most frequently aligned word span for each unique concept node using the JAMR aligner (§2), and then generating a bag of words summary. Evaluation of oracle summaries is performed in the same manner as for system summaries. The above process does not involve graph expansion, so summarization performance is the same for the two conditions "Oracle" and "Oracle + Expand." We find that JAMR parses are a large source of degradation of edge prediction performance, and a smaller but still significant source of degradation for concept prediction. Surprisingly, using JAMR parses leads to slightly improved ROUGE-1 scores. Keep in mind, though, that under our bag-of-words 1083

generator, ROUGE-1 scores only depend on concept prediction and are unaffected by edge prediction. The oracle summarization results, 65.8% and 57.8% F1 scores for gold-standard and JAMR parses, respectively, further suggest that improved graph summarization models (step 2) might benefit from future improvements in AMR parsing (step 1). Across all conditions and both evaluations, we find that incorporating a cost-aware loss function (hinge vs. perceptron) has little effect, but that using ramp loss leads to substantial gains. In Table 5, we show detailed results with and without graph expansion. "+ Expand" means the results are obtained using the expanded source graph. We find that graph expansion only marginally affects system performance. Graph expansion slightly hurts the system performance on edge prediction. For example, using ramp loss with JAMR parser as input, we obtained 50.7% and 19.0% for node and edge prediction with graph expansion; 51.5% and 20.0%

