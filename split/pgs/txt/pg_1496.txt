a word is used in each case. To make the measure interpretable, we take the log of the odds ratio producing the final weight for a word: log Odds(w, s) Odds(w, a) log P (w | class = s) P (w | class = a)

Summary-biased
spur hail allege avert slay exile claim intensify extradite oust overturn underscore cite devastate weigh defuse injure curb defy resign suspect warn quell kidnap stir plot widen charge thwart revive

Article-biased
chant talk sleep hate graduate realize dress understand quote sound add drink sing refer read think imagine remember shout sit happen cry wave like thank love smile accord reply misstate Table 1: Words with highest weights, drawn from verbs with frequency greater than the median verb frequency.

This metric gives an intuitive measure of the usage rate of words. For example, if a word occurs 3 times more often in the summaries it will be given a weight of log(3), and if it occurs three times more often in the articles it will be given a weight of - log(3).

Figure 2: Frequency vs. log odds ratio for each word. Positive odds ratios correspond to summary-biased words and negative correspond to article-biased. The lines indicate integer usage ratios (1:3, 1:2, 1:1, 2:1, 3:1, etc).

However, this metric is unreliable for verbs with low counts in either class of texts. If a verb occurs five times in summaries and only once in articles, it is difficult to say if there is true signal for importance. As the number of counts of a verb in the two classes increases, so does our certainty about the significance of any observed differences in usage rates. To obtain a measure of the statistical significance of the domain-level importance weight of a verb, we treat the set of observed tokens as a Bernoulli trial where each token occurs either in a summary (success) or in an article (failure). We apply a binomial test to compute the probability of the observed distribution of tokens in the two types of text under the null hypothesis that the word has equal frequency in summaries and articles. The p-value from the test gives a measure of the certainty that a word is important and not. We can filter out words with p-values 1442

above a certain threshold. Moving the threshold closer to 0 enforces more and more certainty about the classification, reducing vocabulary size but also decreasing noise. Discarding verbs with a p-value of less than 0.05 reduces the vocabulary size from 3,924 words to 1,210. In Figure 2, the log odds ratio is plotted against the overall frequency for each verb after discarding unreliable verbs. The most extreme weights occur mostly for the infrequent words, even after filtering out low p-value words. Although these words have extremely high bias weights, they tend to be uncommon and not particularly informative. Examples include verbs such as "hostage-take", "muckrake", and "blaspheme." To get a clearer picture of trends in the verbs in each category, we show in Table 1 the verbs with the 30 highest and 30 lowest weights among the verbs in the 50th percentile of total counts across all documents. In the following two sections, we turn to analyzing why certain verbs may be more important in the domain than others. First we examine the relationship between the summary- or article-bias of a word and categories in the General Inquirer lexicon. Then we develop a new characterization of verbs showing their association with person-centered perspective of the narrative.

3

General Inquirer

The General Inquirer lexicon provides a list of words manually annotated with a variety of tags (Stone et al., 1966). We considered eight of these tags that were relevant to our task and could explain why

