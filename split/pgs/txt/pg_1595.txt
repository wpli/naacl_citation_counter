are over different sentences, so they are not directly comparable, but they point to the robustness of the annotation scheme. Thanks to the double annotation plus reconciliation procedure, these numbers should underestimate the reliability of the final annotations. 3.5 Corpus Statistics A total of 9,000 noun mentions (1,300 of them MWEs) and 7,800 verb mentions (1,200 MWEs) incorporating 20,000 word tokens are annotated.11 Table 1 shows supersense mention counts and the most frequent example of each category in the corpus. 3.6 Copenhagen Supersense Data An independent English noun+verb supersense annotation effort targeting the Twitter domain was undertaken by the COASTAL lab at the University of Copenhagen (Johannsen et al., 2014). The overarching goal of annotating supersenses directly in running text was the same as in the present work, but there are three important differences. First, general-purpose MWE annotations were not considered in that work; second, sentences were pre-annotated by a heuristic system and then manually corrected, whereas here the annotations are supplied from scratch; and third, Johannsen et al. (2014) provided minimal instructions and training to their annotators, whereas here we have worked hard to encourage consistent interpretations of the supersense categories. Johannsen et al. have released their annotations on two samples of tweets (over 18,000 tokens in total). Johannsen et al.'s dataset illustrates why supersense annotation by itself is not the same as the full scheme for lexical semantic analysis proposed here. Many of the expressions that they have supersense-annotated as single-word nouns/verbs probably would have been part of larger units in MWE annotation: examining Johannsen et al.'s inhouse sample, multiword chunks arguably should have been used for verb phrases like gain entry, make sure, and make it (`succeed'), and for verb-particle constructions like take over, find out, and check out (`ogle'). Moreover, in the traditional supersense annotation scheme, there are no chunks not labeled
assigned a supersense, is very similar: .76, .93, and .90, respectively, reflecting strong agreement. 11 This excludes 1,200 auxiliary verb mentions, 100 of which are MWEs: have to, is going to, etc.

with a supersense; thus, e.g., PPs such as on tap, of ALL-Time, and up to [value limit] are not chunked. Many of the nominal expressions in Johannsen et al.'s (2014) data appear to have overly liberal boundaries, grouping perfectly compositional modifiers along with their heads as a multiword chunk: e.g., Panhandling Ban, Panda Cub, farm road crash, and Tomic's dad. Presumably, some of these were boundary errors made by the heuristic pre-annotation system that human annotators failed to notice.

4

Automatic Tagging

We now turn to automating the combined multiword expression and supersense prediction task in a single statistical model. 4.1 Background: Supersense Tagging with a Discriminative Sequence Model

Ciaramita and Altun's (2006) model represents the state of the art for full12 English supersense tagging on the standard SemCor test set, achieving an F1 score of 77%. It is a feature-based discriminative sequence model learned in a supervised fashion with the structured perceptron (Collins, 2002). For Ciaramita and Altun (2006) and hereafter, sequences correspond to sentences, with each sentence pre-segmented into words according to some tokenization. Figure 2 shows how token-level tags combine Ramshaw and Marcus (1995)≠style BIO flags with supersense class labels to represent the segmentation and supersense labeling of a sentence. These tags are observed during training, predicted at test time, and compared against the gold standard tags. Ciaramita and Altun's (2006) model uses a simple feature set capturing the lemmas, word shapes, and parts of speech of tokens in a small context window, as well as the supersense category of the first WordNet sense of the current word. (WordNet senses are ordered roughly by frequency.) On SemCor data, the model achieves a 10% absolute improvement in F1 over the first sense baseline.
Paaﬂ and Reichartz (2009) train a similar sequence model for classifying noun and verb supersenses, but treat multiword phrases as single words. Their model is trained as a CRF rather than a structured perceptron, and adds LDA word cluster features, but the effects of these two changes are not separated in the experiments. They also find benefit from constraining the label space according to WordNet for in-vocabulary words (with what they call "lumped labels").
12

1541

