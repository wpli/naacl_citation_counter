dependently extract initial phrases for each of the N languages using the standard bilingual phrase-extract algorithm, yielding initial phrase sets {BP1 , . . . , BPN }. Finally, we convert these bilingual sets of phrases into a single set of multilingual phrases. This can be done by noting that all source phrases fij will be associated with a set of 0 or more phrases in each target language. We define the set of multilingual phrases associated with fij as the cross product of these sets. In other words, if fij is associated with 2 phrases in T1, and 3 phrases in T2, then there will be a total of 2  3 = 6 phrase triples extracted as associated with fij .5 Once we have extracted multilingual phrases, the remaining creation of rules is essentially the same as the bilingual case, with sub-phrases being turned into non-terminals for the source and all targets. 4.3 Rule Scoring After we have extracted rules, we assign them feature functions. In traditional SCFGs, given a source and target  and 1 , it is standard to calculate the log forward and backward translation probabilities P ( |1 ) and P (1 | ), log forward and backward lexical translation probabilities Plex ( |1 ) and Plex (1 | ), a word penalty counting the nonterminals in 1 , and a constant phrase penalty of 1. In our MSCFG formalism, we also add new features regarding the additional targets. Specifically in the case where we have one additional target 2 , we add the log translation probabilities P ( |2 ) and P (2 | ), log lexical probabilities Plex ( |2 ) and Plex (2 | ), and word penalty for 2 . In addition, we add log translation probabilities that consider both targets at the same time P ( |1 , 2 ) and P (1 , 2 | ). As a result, compared to the 6-feature set in standard SCFGs, an MSCFG rule with two targets will have 13 features. 4.4 Rule Table Pruning In MT, it is standard practice to limit the number of rules used for any particular source  to ensure realistic search times and memory usage. This limit is generally imposed by ordering rules by the phrase
Taking the cross-product here has the potential for combinatorial explosion as more languages are added, but in our current experiments with two target languages this did not cause significant problems, and we took no preventative measures.
5

probability P (1 | ) and only using the top few (in our case, 10) for each source  . However, in the MSCFG case, this is not so simple. As the previous section mentioned, in the two-target MSCFG, we have a total of three probabilities conditioned on  : P (1 , 2 | ), P (1 | ), P (2 | ). As our main motivation for multi-target translation is to use T2 to help translation of T1, we can assume that the final of these three probabilities, which only concerns T2, is of less use. Thus, we propose two ways for pruning the rule table based on the former two. The first method, which we will call T1+T2, is based on P (1 , 2 | ). The use of this probability is straightforward, as it is possible to simply list the top rules based on this probability. However, this method also has a significant drawback. If we are mainly interested in accurate generation of the T1 sentence, there is a possibility that the addition of the T2 phrase 2 will fragment the probabilities for 1 . This is particularly true when the source and T1 are similar, while T2 is a very different language. For example, in the case of a source of English, T1 of French, and T2 of Chinese, translations of English to French will have much less variation than translastions of English to Chinese, due to less freedom of translation and higher alignment accuracy between English and French. In this situation, the pruned model will have a variety of translations in T2, but almost no variety in T1, which is not conducive to translating T1 accurately. As a potential solution to this problem, we also test a T1 method, which is designed to maintain variety of T1 translations for each rule. In order to do so, we first list the top 1 candidates based only on the P (1 | ) probability. Each 1 will be associated with one or more 2 rule, and thus we choose the 2 resulting in the highest joint probability of the two targets P (1 , 2 | ) as the representative rule for 1 . This pruning method has the potential advantage of increasing the variety in the T1 translations, but also has the potential disadvantage of artificially reducing genuine variety in T2. We examine which method is more effective in the experiments section.

5

Search with Multiple LMs

LMs computes the probability P (E ) of observing a particular target sentence, and are a fundamental

296

