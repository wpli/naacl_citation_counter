3

Joint Generation

In this section, we describe our approach of the joint transduction of a transliteration T from a source orthographic string S and a source phonemic string P (Figure 1). We implement our approach by modifying the D IREC TL+ system of Jiampojamarn et al. (2010), which we describe in Section 3.1. In the following sections, we discuss other components of our approach, namely alignment (3.2), scoring (3.3), and search (3.4). In Section 3.5 we generalize the joint model to accept multiple input strings. 3.1 DirecTL+ D IREC TL+ (Jiampojamarn et al., 2010) is a discriminative string transducer which learns to convert source strings into target strings from a set of parallel training data. It requires pairs of strings to be aligned at the character level prior to training. M2M-A LIGNER (Jiampojamarn et al., 2007), an unsupervised EM-based aligner, is often used to generate such alignments. The output is a ranked list of candidate target strings with their confidence scores. Below, we briefly describe the scoring model, the training process, and the search algorithm. The scoring model assigns a score to an aligned pair of source and target strings (S, T ). Assuming there are m aligned substrings, such that the ith source substring generates the ith target substring, the score is computed with the following formula:
m

Figure 1: Triple alignment between the source phonemes, source graphemes, and the target graphemes  (ARO-N).

tion. Given the training instance (S, T ) and the current feature weights k-1 , the update of the feature weights can be described as the following optimization problem: min k - k-1
k

^  Tn : s.t. T

^))  loss(T, T ^) k  ((S, T ) - (S, T ^ is a candidate target in the n-best list Tn where T found under the current model parameterized by k-1 . The loss function is the Levenshtein distance ^. between T and T Given an unsegmented source string, the search algorithm finds a target string that achieves the highest score according to the scoring model. It searches through all the possible segmentations of the source string and all possible target substrings using the following dynamic programming formulation: Q(0, $) = 0 Q(j, t) =
t ,t,j -N j <j t

  (i, S, T )
i

(1)

where  is the weight vector, and  is the feature vector. There are four sets of features. Context features are character n-grams within the source word. Transition features are character n-grams within the target word. Linear-chain features combine context features and transition features. Joint n-gram features further capture the joint information on both sides. The feature weights  are learned with the Maximum Infused Relaxed Algorithm (MIRA) of Crammer and Singer (2003). MIRA aims to find the smallest change in current weights so that the new weights separate the correct target strings from incorrect ones by a margin defined by a loss func945

max

j   (Sj +1 , t , t) + Q(j , t )

Q(J + 1, $) = max   ($, t , $) + Q(J, t ) Q(j, t) is defined as the maximum score of the target sequence ending with target substring t, generated by the letter sequence S1 ...Sj .  describes the features extracted from the current generator subj string Sj +1 of target substring t, with t to be the last generated target substring. N specifies the maximum length of the source substring. The $ symbols are used to represent both the start and the end of a string. Assuming that the source string contains J characters, Q(J + 1, $) gives the score of the highest scoring target string, which can be recovered through backtracking.

