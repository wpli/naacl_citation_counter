Ling tri/squ/pgram small/med/big left/mid/right prev referred top/cen/bottom referred 5 referred 10 referred 20

TaskSp most recent move mouse pointed Gaze most gazed at gazed at in utt longest gazed at recent fixation
Figure 2: Tangram later in the dialogue; the notion of right-ness and other spatial concepts changes throughout the dialogue (compare to Figure 1), the grids are added to show which objects receive which horizontal and which vertical properties.

Table 2: List of properties used for each source of information.

the shape and size properties which remained static through each dialogue. The position properties were derived from the corpus logs. For each object, the centroid of each object was computed. Then, the vertical and horizontal range for all of the objects was calculated and then split into three even sections in each dimension (see Figure 2). An object with a centroid in the left-most section of the horizontal range received a left property, similarly middle and right properties were calculated for corresponding objects. For vertical placement, top, center and bottom properties were given to objects in the respective vertical segments. Figure 2 shows an example segmentation. Each object had a vertical and a horizontal property at all times, however, moving an object could result in a change of one of these spatial properties as the dialogue progressed. As an example, compare Figure 1, which is a snapshot of the interaction towards the beginning, and Figure 2, which shows a later stage of the game board; spatial layout changes throughout the dialogue. These properties differ somewhat from the features for the Ling model presented in Iida et al. (2011). Three features that we did use as properties had to do with reference recency: the most recently referred object received the referred X properties, if an object was referred to in the past 5, 10, or 20 seconds. TaskSp Iida et al. (2011) used 14 task-specific features, three of which they found to be the most informative in their model. Here, we will only use the two most informative features as properties (the third one, whether or not an object was being manipulated at the beginning of the RE, did not improve 276

results in a held-out test): the object that was most recently moved received the most recent move property and objects that have the mouse cursor over them received the mouse pointed property (see Figure 2; object 4 would receive both of these properties, but only for the duration that the mouse was actually over it). Each of these properties can be extracted directly from the corpus annotations. Gaze Similar to Iida et al. (2011), we consider gaze during a window of 1500ms before the onset of the RE. The object that was gazed at the longest during that time received a longest gazed at property, the object which was fixated upon most recently during that interval before the RE onset received a recent fixation property, and the object which had the most fixations received the most gazed at property. During a RE, an object received the gazed at in utt property if it is gazed at during the RE up until that point. These properties can be extracted directly from the corpus annotations. Other gaze features are not really accessible to an incremental model such as this, as gaze features extracted from gaze activity over the RE can only be computed when it is complete. Our Gaze properties are made up of these 4 properties, as opposed to the 14 features in Iida et al. (2011). P(U|R) P (U |R) is the model that connects the property selected for verbalisation with a way of verbalising it (a value for U ). Instead of directly learning this model from data, which would suffer from data sparseness, we trained a naive Bayes model

