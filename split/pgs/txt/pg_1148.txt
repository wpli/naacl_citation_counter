Model Graph Model Graph Model + K Entity Model Entity Model + K

Accuracy (%) 27.4 44.2 65.8 71.1

Table 2: Performance of various models with and without world knowledge in the sentence ordering task, tested on short documents with 1­5 sentences. of document, we test our models with and without world knowledge encoded on each subset separately. Since the size of the available training data in each subset is relatively small, the supervised entitybased model suffers from sparsity. Therefore, we focus on the unsupervised graph-based models only. Figure 4 shows the performance on different subsets. We can see that the performance of both models generally improves as the number of sentences increases. This observation is quite intuitive, because the longer a document is, the higher the chance is that, after being shuffled, adjacent sentences in the resulting permutation would be completely irrelevant to each other. Therefore, for longer documents, it is much easier for the model to distinguish a permutation from its source document. Effect of document length on the model with world knowledge Moreover, we also observe that the document length has a non-universal effect, in terms of how the model could benefit from incorporating world knowledge. Specifically, we find that world knowledge has a greater effect on short documents, as demonstrated in Table 2. Evaluated on a set of documents composed of 30 extremely short documents only (1­5 sentences), we see that our enhanced graph-based model is able to improve the performance by 16.8% over the basic model, and our enhanced entity-based model achieves 5.3% improvement (both differences are significant at p < .01). We postulate that it is primarily because a document with fewer sentences tends to shift to another subtopic immediately without elaborating on the previous one, and strict entity matching would find it difficult to establish coherent transitions between them. Therefore, the contribution from semantic relatedness tends to dominate the overall performance. 5.4.2

Figure 4: Graph-based models, with and without world knowledge (labeled as With K and Without K ), tested on sets with different numbers of sentences. ducing the scoring scheme of average reachability score, our Graph model + K + Avg R achieves the best performance among all graph-based models. Secondly, for entity-based models, our enhanced model with knowledge features encoded also achieves superior performance than our reimplemented model, again confirming the usefulness of world knowledge. Interestingly, we observe that our re-implementation obtains higher accuracy compared to the performance reported by B&L. This is partly due to the fact that the documents in our dataset have an average length of 31.5 sentences, which are longer than those used in B&L's experiments. We will further discuss this problem in Section 5.4 and show that document length is an important factor to the overall performance. However, on the task of summary coherence rating, the difference between our extended models and the original ones is generally not significant, primarily due to the fact that the sample size for this task is too small, i.e., 80 pairwise rankings. 5.4 5.4.1 Effect of document length Effect of document length on the overall performance

We further analyze the impact of document length on the task of sentence ordering. We partition our original dataset, which consists of 214 documents and their permutations, into 8 non-overlapping subsets, according to the length of documents: 1-5, 610,. . . , and 36-40 sentences. To illustrate the correlation between the performance and the length 1094

