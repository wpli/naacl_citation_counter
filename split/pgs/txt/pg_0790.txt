tion of semi-Markov tagging for NER, and from de Bruijn et al. (2011), who apply a similar tagger to clinical information extraction. A number of previous studies have closely examined the use of word representations in NER, where one leverages unlabeled data to build features that help the tagger generalize across similar words. Miller et al. (2004) introduce this idea and provide the framework to build representation features from word clusters, while Lin and Wu (2009) extend this technique with phrases and sheer masses of unlabeled data. Turian et al. (2010) introduce continuous vectors as alternative word representations, and provide several experiments comparing these with clusters. Recently, Passos et al. (2014) have shown how continuous representations can be tailored to NER with a combination of context- and gazetteer-aware objectives. All of these studies employ representations only in newswire scenarios. Ratinov and Roth (2009) investigate cluster representations in a Web NER task, but the performance of their baseline indicates that it is not nearly so drastic a domain shift as our Twitter task. 2.1 Adapting to Social Media There has been much recent activity in adapting NLP tools for social media. Ritter et al. (2011) collect training data and adapt tools for a number of tasks, including part-of-speech (POS) tagging, shallow parsing and NER. Owoputi et al. (2013) extends a line of research on building robust POS taggers for Twitter, and share our focus on the utility of word representations in this domain. Liu et al. (2011) carry out the first study to specifically examine NER on Twitter. They use a nearestneighbour word classifier stacked with a CRF, along with a boot-strapping scheme for semi-supervised learning. Interestingly, they find no utility in using cluster-based word representations, perhaps because their model directly accounts for a type's global context with bag-of-word features. Ritter et al. (2011) also examine Twitter NER, developing a semi-supervised technique that uses labeled LDA to project information from Freebase gazetteers onto unlabeled tweets. Plank et al. (2014) suggest a distant-supervision scheme, creating artificial training data by projecting reliable NER tags from web pages onto the tweets that link to those pages. 736

Fromreide et al. (2014) and Plank et al. (2014) point out that NER performance can be overestimated when a system is tested on data extracted from the same pool as its training data. Temporal effects and annotation biases can result in gains that disappear when shifting to another test set. We follow their lead by testing on data that was annotated independently from our training data.

3

Methods

Our named entity recognizer is a discriminative, semi-Markov tagger, trained online using largemargin updates. It differs from word-based CRF systems in three ways: its inference algorithm, its tag structure, and its learning algorithm. This tagger allows us to develop new systems quickly, but it is important to emphasize that the adaptation strategies described later in this section can just as easily be applied to word-based CRFs. Semi-Markov Inference Sarawagi and Cohen (2004) describe a straightforward extension to the Viterbi algorithm that enables the tagging of contiguous phrases instead of words. Because each phrasal entity is tagged as a unit, we can recover entity boundaries without distinguishing between Begin and Inside tags, leaving the tagger to track only entity classes and Outside tags. This in turn allows us to run our tagger without Markov features. Since most entities are surrounded by Outside tags, conditioning on previous tag assignments has only limited utility. Finally, our phrasal tags enable useful features that consider entire entities, such as phrase-identity indicators. Phrasal and Word-level Tags In word-based models, it is beneficial to not only identify words that Begin entities, but also those that are in the middle (Inside) or at the end of entities (Last), as well as entities that consist of exactly one Unique word (Ratinov and Roth, 2009). Since we tag entire phrases at once, we can easily assign each word in the phrase to one of these four entity-relative positions. Therefore, even though our tagger tracks only entity class, its word-level features are annotated as if we maintained a full BILUO tag set.

