each entity-pair (ei , ej ) in the domain. Specifically, [F ] = (ei ,ej )P [Fij ], and therefore L([F ]) = (ei ,ej )P L([Fij ]). The gradients are derived as:
[Fij ] = [rs (ei , ej )] ([rt (ei , ej )] - 1) + 1 (6)  L([Fij ])  [rs (ei , ej )] = -[Fij ]-1 ([rt (ei , ej )] - 1)  vrs  vrs  L([Fij ])  [rt (ei , ej )] = -[Fij ]-1 [rs (ei , ej )] (7)  vrt  vrt  L([Fij ])  [rs (ei , ej )] = -[Fij ]-1 ([rt (ei , ej )] - 1)  vei ,ej  vei ,ej  [rt (ei , ej )] - [Fij ]-1 [rs (ei , ej )] . (8)  vei ,ej

Niepert, 2011). In this paper we focus the evaluation on the ability of various approaches to benefit from formulae that we directly extract from the training data using a simple method. Distant Supervision Evaluation We follow the procedure as used in Riedel et al. (2013) for evaluating knowledge base completion of Freebase (Bollacker et al., 2008) with textual data from the NYTimes corpus (Sandhaus, 2008). The training matrix consists of 4 111 columns, representing 151 Freebase relations and 3 960 textual patterns, 41 913 rows (entity-pairs) and 118 781 training facts of which 7 293 belong to Freebase relations. The entity-pairs are divided into train and test, and we hide all Freebase relations for the test pairs from training. Our primary evaluation measure is average and (weighted) mean average precision, MAP and wMAP respectively (see Riedel et al. (2013) for details). Formulae Extraction and Annotation We use a simple technique for extracting formulae from the matrix factorization model. We first run matrix factorization over the complete training data to learn accurate relation and entity-pair embeddings. After training, we iterate over all pairs of relations (rs , rt ) where rt is a Freebase relation. For every relationpair we iterate over all training atoms rs (ei , ej ), evaluate the score [rs (ei , ej )  rt (ei , ej )] as described in §3.2.1, and calculate the average to arrive at a score for the formula. Finally, we rank all formulae by their score and manually filter the top 100 formulae, which resulted in 36 annotated high-quality formulae (see Table 1 for examples). Note that our formula extraction approach does not observe the relations for test entity-pairs. All models used in our experiments have access to these formulae, except for the matrix factorization baseline. Methods Our proposed methods for injecting logic into relation embeddings are pre-factorization inference (Pre; §3.1) which performs regular matrix factorization after propagating the logic formulae in a deterministic manner, and joint optimization (Joint; §3.2) which maximizes an objective that combines terms from factual and first-order logic knowledge. Additionally, we use the following three baselines. The matrix factorization (MF; §2.2) model uses only ground atoms to learn relation and entity-pair embed-

Following such a derivation, one can obtain gradients for other first-order logic formulae as well. 3.2.2 Learning We learn the embeddings by minimizing Eq. 1 with 2 -regularization using AdaGrad (Duchi et al., 2011). Since we have no negative training facts, we follow Riedel et al. (2013) by sampling unobserved facts that we assume to be false. Specifically, in every epoch and for every true training fact rm (ei , ej ) we sample an (ep , eq ) such that rm (ep , eq ) is unobserved. Subsequently, we perform two kinds of updates: F = rm (ei , ej ) and F = ¬ rm (ep , eq ). For every non-atomic first-order formula in F we iterate over all entity-pairs for which at least one atom in the formula is observed (in addition to as many sampled entity-pairs for which none of the atoms have been observed) and add corresponding grounded propositional formulae to the training objective. At test time, predicting a score for any unobserved statement rm (ei , ej ) is done efficiently by calculating [rm (ei , ej )]. Note that this does not involve any explicit logical inference, instead we expect that the predictions from the learned embeddings already respect the provided formulae.

4

Experimental Setup

There are two orthogonal question when evaluating the effectiveness of low-rank logic embeddings: a) does injection of logic formulae into the embeddings of entity-pairs and relations provide any benefits, and b) where do the background formulae come from? The latter is a well-studied problem (Hipp et al., 2000; Schoenmackers et al., 2010; V¨ olker and

1123

