System A HUMAN DCGM-II DCGM-II+CMM DCGM-II+CMM

System B MT+CMM MT MT MT+CMM

Gain (%) 13.6* 1.9* 3.1* 1.5*

word-order information in context and message encodings. [12.4,14.8]
CI [0.8, 2.9] [2.0, 4.3] [0.5, 2.5]

6.3

Human Evaluation

Human evaluation was conducted using crowdsourced annotators. Annotators were asked to comDCGM-II IR 5.2* [4.0, 6.4] pare the quality of system output responses pairwise DCGM-II+CMM IR 5.3* [4.1, 6.6] ("Which is better?") in relation to the context and DCGM-II+CMM IR+CMM 2.3* [1.2, 3.4] message strings in the 2114 item test set. Identical Table 4: Pairwise human evaluation scores between Sys- strings were held out, so that the annotators only saw tem A and B. The first (second) set of results refer to the those outputs that differed. Paired responses were MT (IR) hypothesis list. The asterisk means agreement presented in random order to the annotators, and each between human preference and BLEU rankings. pair of responses was judged by 5 annotators. Table 4 summarizes the results of human evaluamodel provided by the RLM. In the case of IR modtion, giving the difference in mean scores (pairwise els, on the other hand, there is more headroom for preference margin) between systems and 95% confiimprovement and fluency is already guaranteed. Any dence intervals generated using Welch's t-test. Idengains must come from context and message matches. tical strings not shown to raters are incorporated with Hence, RLMT underperforms with respect to both an automatically assigned score of 0.5. The pattern in DCGM and IR+CMM. The DCGM models appear to these results is clear and consistent: context-sensitive have better capacity to retain contextual information systems (+CMM) outperform non-context-sensitive and thus achieve similar performance to IR+CMM systems, with preference gains as high as approxidespite their lack of exact n-gram match information. mately 5.3% in the case of DCGM-II+CMM versus In the present experimental setting, no striking IR, and about 3.1% in the case of DCGM-II+CMM performance difference can be observed between the versus MT. Similarly, context-sensitive DCGM systwo versions of the DCGM architecture. If multiple tems outperform non-DCGM context-sensitive syssequences were used as context, we expect that the tems by 1.5% (MT) and 2.3% (IR). These results are DCGM-II model would likely benefit more owing to consistent with the automated BLEU rankings and the separate encoding of message and context. confirm that our best performing DCGM models outDCGM+CMM We also investigated whether mix- perform both raw baseline and the context-sensitive ing exact CMM n-gram overlap with semantic in- baseline using CMM features. formation encoded by the DCGM models can bring additional gains. DCGM-{I-II}+CMM systems each 6.4 Discussion totaling 10 features show increases of up to 0.48 Table 5 provides examples of responses generated on BLEU points over MT+CMM and up to 0.88 BLEU the tuning corpus by the MT-based DCGM-II+CMM over the model based on Ritter et al. (2011). ME- system, our best system in terms of both BLEU and TEOR improvements similarly align with BLEU im- human evaluation. Responses from this system are on provements both for MT and IR lists. We take this average shorter (8.95 tokens) than the original human as evidence that CMM exact matches and DCGM responses in the tuning set (11.5 tokens). Overall, the semantic matches interact positively, a finding that outputs tend to be generic or commonplace, but are comports with Gao et al. (2014a), who show that often reasonably plausible in the context as in exsemantic relationships mined through phrase embed- amples 1-3, especially where context and message dings correlate positively with classic co-occurrence- contain common conversational elements. Example 2 based estimations. Analysis of CMM feature weights illustrates the impact of context-sensitivity: the word in Figure 4 (c) and (d) suggests that 1-gram matches "book" in the response is not found in the message. are explained away by the DCGM model, but that Nonetheless, longer generated responses are apt to higher order matches are important. It appears that degrade both syntactically and in terms of content. DCGM models might be improved by preserving We notice that longer responses are likely to present 203

