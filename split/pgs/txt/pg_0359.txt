transducers where the underlying to surface mapping consists of simple substitutions, so their model cannot handle the deletion phenomena studied here. B¨ orschinger et al. (2013) also generalise the Goldwater bigram model by including an underlyingto-surface mapping, but their mapping only allows word-final underlying /t/ to be deleted, which enables them to use a straight-forward generalisation of Goldwater's Gibbs sampling inference procedure. In phonology, Eisenstat (2009) and Pater et al. (2012) showed how to generalise a MaxEnt model so it also learns underlying forms as well as MaxEnt phonological constraint weights given surface forms in paradigm format. The vast sociolinguistic literature on /t/-/d/-deletion is surveyed in Coetzee and Pater (2011), together with prior OT and MaxEnt analyses of the phenomena. 2.1 The Berg-Kirkpatrick et al. model This section contains a more technical description of the Berg-Kirkpatrick et al. (2010) MaxEnt unigram model of word segmentation, which our model directly builds on. Our model integrates the MaxEnt unigram word segmentation model of BergKirkpatrick et al. with the MaxEnt phonology models developed by Goldwater and Johnson (2003) and Goldwater and Johnson (2004). Because both kinds of models are MaxEnt models, this integration is fairly easy, and the inference procedure requires optimisation of a fairly straight-forward objective function. We use a customised version of the OWLQN-LBFGS procedure (Andrew and Gao, 2007) that allows us to impose sign constraints on individual feature weights. As is standard in the word-segmentation literature, the model's input is a sequence of utterances D = (w1 , . . . , wn ), where each utterance wi = (wi,1 , . . . , wi,mi ) is a sequence of (surface) phones. The Berg-Kirkpatrick et al model is a unigram model, so it defines a probability distribution over possible words s, where s is also a sequence of phones. The probability of an utterance w is the sum of the probability of all word sequences that generate it: P(w | ) = P(sj | ) 305

Berg-Kirkpatrick et al's model of word probabilities P(s | ) is a MaxEnt model with parameters , where the features f (s) of surface form s are chosen to encourage the model to generalise appropriately over word shapes. While they don't describe their features in complete detail, they include features for each word s, features for the prefix and suffix of s and features for the CV skeleton of the prefix and suffix of s. In more detail, P(s | ) is a MaxEnt model as follows: P(s | ) = Z = 1 exp( · f (s)), where: Z exp( · f (s ))
s S

The set of possible surface word forms S is the set of substrings (i.e., sequences of phones) occuring in the training data D that are shorter than a userspecified length bound. We follow Berg-Kirkpatrick in imposing a length bound on possible words; for the Brent corpus the maximum word length is 10 phones, while for the Buckeye corpus the maximum word length is 15 phones (reflecting the fact that words are longer in this adult-directed corpus). While restricting the set of possible word forms S to the substrings appearing in D is reasonable for a simple multinomial model like the one in Liang and Klein (2009), it's interesting that this produces good results with a MaxEnt model like BergKirkpatrick et al's, since one might expect such a model would have to learn generalisations about impossible word shapes in order to perform well. Because S only contains a small fraction of the possible phone strings, one might worry that the model would not see enough "impossible words" to learn to distinguish possible words from impossible ones, but the model's good performance suggests this is not the case.1
1 The non-parametric Bayesian approach of Goldwater et al. (2009) and Johnson (2008) can be viewed as setting S to the set of all possible phone strings (i.e., a possible word can be any string of phones, whether or not it appears in D). The success of Berg-Kirkpatrick et al's approach suggests that these nonparametric methods might not be necessary here, i.e., the set of substrings actually occuring in D is "large enough" to enable the model to learn "implicit negative evidence" generalisations about impossible word shapes.

s1 ...s j =1 s.t.s1 ...s =w

