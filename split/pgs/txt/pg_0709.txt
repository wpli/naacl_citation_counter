phrase covered by the node, and ou - the outer representation, represents the context around that phrase. In addition, information in an IORNN is allowed to flow not only bottom-up as in RNNs, but also topdown. That makes IORNNs a natural tool for estimating top-down tree-generating models. Applying the IORNN architecture to dependency parsing is straightforward, along the generative story of the -order generative model. First of all, the "inside" part of this IORNN is simpler than what is depicted in Figure 1: the inner representation of a phrase is assumed to be the inner representation of its head. This approximation is plausible since the meaning of a phrase is often dominated by the meaning of its head. The inner representation at each node, in turn, is a function of a vector representation for the word (in our case, the word vectors are initially borrowed from Collobert et al. (2011)), the POS-tag and capitalisation feature. Without loss of generality and ignoring directions for simplicity, they assume that the model is generating dependent u for node h conditioning on context C  (u) which contains all of u's ancestors (including h) and theirs siblings, and all of previously generated u's sisters. Now there are two types of contexts: full contexts of heads (e.g., h) whose dependents are being generated, and contexts to generate nodes (e.g., C  (u)). Contexts of the first type are clearly represented by outer representations. Contexts of the other type are represented by partial ¯ u . Because the outer representations, denoted by o context to generate a node can be constructed recursively by combining the full context of its head and ¯u its previously generated sisters, they can compute o as a function of oh and the inner representations of ¯u, its previously generated sisters. On the top of o they put a softmax layer to estimate the probability P (x|C  (u)). Training this IORNN is to minimise the cross entropy over all dependents. This objective function is indeed the negative log likelihood P (D) of training treebank D. 4.3 The Reranker

where P (Equation 3) is computed by the -order generative model which is estimated by an IORNN; and k Dep(s) is a k -best list.

5

Complete System

Our system is based on the multi-phase IR. In general, any third-party parser for unsupervised dependency parsing can be used in phase 0, and any thirdparty parser that can generate k -best lists can be used in the other phases. In our experiments, for phase 0, we choose the parser using an extension of the DMV model with stop-probability estimates computed on a large corpus proposed by Marecek and Straka (2013). This system has a moderate performance3 on the WSJ corpus: 57.1% vs the SOTA 64.4% DDA of Spitkovsky et al. (2013). For the other phases, we use the MSTParser4 (with the second-order feature mode) (McDonald and Pereira, 2006). Our system uses Le and Zuidema (2014)'s reranker (Section 4.3). It is worth noting that, in this case, each phase with iterated reranking could be seen as an approximation of hard-EM (see Equation 2) where the first step is replaced by Di+1 = arg max Pi (D)
DN(Di )

(4)

In other words, instead of searching over the treebank space, the search is limited in a neighbour set N(Di ) generated by k -best parser Pi . 5.1 Tuning Parser P

Parser Pi trained on Di defines neighbour set N(Di ) which is the Cartesian product of the k -best lists in k Di . The position and shape of N(Di ) is thus determined by two factors: how well Pi can fit Di , and k . Intuitively, the lower the fitness is, the more N(Di ) goes far away from Di ; and the larger k is, the larger
Marecek and Straka (2013) did not report any experimental result on the WSJ corpus. We use their source code at http: //ufal.mff.cuni.cz/udp with the setting presented in Section 6.1. Because the parser does not provide the option to parse unseen sentences, we merge the training sentences (up to length 15) to all the test sentences to evaluate its performance. Note that this result is close to the DDA (55.4%) that the authors reported on CoNLL 2007 English dataset, which is a portion of the WSJ corpus. 4 http://sourceforge.net/projects/ mstparser/
3

Le and Zuidema's (generative) reranker is given by d = arg max P (d)
dk Dep(s)

655

