reranking feature gi g+t ti

I NSTANCE 42% 50% 46%

ASSOC

58% 50% 54%

Table 7: Human evaluation for image captioning: the
% of cases judged as visually more relevant, in pairwise comparisons. gi: GIST; ti: Tinyimage; g+t:= gi+ti.

of their associated images (about 280K). Evaluation. Automatic evaluation remains to be a challenge (Elliott and Keller, 2014). We report both BLEU (Papineni et al., 2002) at 1 without brevity penalty, and METEOR (Banerjee and Lavie, 2005) with balanced precision and recall. Table 6 shows the results: the ASSOC approach (w/  ) significantly outperforms the two baselines. The largest improvement over INSTANCE is 60% higher in BLEU, and 44% higher in METEOR, demonstrating the benefit of the innate association structure of our corpus. Using all visual neighborhood (ASSOC w/ all) does not yield as strong results as selective neighborhood (ASSOC w/  ), confirming our hypothesis that each visual concept can have diverse visual renderings. We also compute crowd-sourced evaluation on a subset (200 images) randomly sampled out of the test set. For each query image, we present two captions generated by two competing methods in a random order. Turkers choose the caption that is more relevant to the visual content of the given image. We aggregate the choices of three turkers by majority voting. As shown in Table 7, ASSOC shows overall improvement over baselines, where the difference is more pronounced when reranking is based on feature sets that differ from the one used during the candidate retrieval.

(1) Visual Paraphrasing using Crowd-sourcing: We use Amazon Mechanical Turk to annotate visual paraphrases for a subset of images in our corpus. Given each image with its original caption, we showed 10 randomly sampled candidate captions from our dataset that share at least one physicalobject noun7 with the original caption. Turkers choose all candidate captions that could also describe the given image. We collect 7,570 (i, c, p) paraphrase triples in total. (2) Visual Paraphrasing using Associative Structure: We also propose an algorithm for automatic visual paraphrasing by adapting the ASSOC algorithm for image captioning (§3) as follows: given an image-caption pair (i, c), it first prepares a set of candidate captions that share the largest number of physical-object nouns with c, which are likely to be semantically close to c; then we rerank the candidate captions using the same neighborhood-based affinity as described in §3. We apply this algorithm to generate a large set of visual paraphrases. For each caption in our corpus, we randomly sample two of its associated images, and generate one visual paraphrase for each imagecaption pair, which yields 353,560 (i, c, p) triples. See Figure 2 for example paraphrases. 5.1 Image Captioning using Visual Paraphrasing

5

Image Captioning using Visual Paraphrases

We present an exploration of visually situated paraphrase (or visual paraphrase in short hand), and demonstrate their utility for image captioning. Formally, given our corpus G = (T, V, E ), a visual paraphrase relation is a triple (i, c, p), where given an image i  V and its original caption c  T (i.e., e(c, i, original)  E ), p  T is a visual paraphrase for c situated in a visual context given by the image i (i.e, e(p, i, paraphrase)  E ). We collect visual paraphrases using both human annotation and an automatic algorithm. 509

We propose to utilize automatically-generated visual paraphrases to improve the ASSOC approach (§3) for image captioning. One potential limitation of the ASSOC approach is that for some captions, the number of associated images might be too small for reliable estimations of the neighborhood based affinity. We hypothesize that for a caption with a small visual neighborhood, merging its neighborhood with those associated with its visual paraphrases will give a more reliable estimation of the affinity between a query image and that caption. Thus we modify the ASSOC approach as follows. After preparing a pool of K candidate captions {c1 , c2 , . . . , cK }, automatically generate a visual paraphrase (ii , ci , pi ) for each (ii , ci ); then rerank the candidate captions by the following affinity function that merges the visual neighborhood from the
7

under the WordNet "physical entity.n.01" synset

