because it does not produce proper headlines, but it is used for naively trying to maximize the achievable value of the evaluation metrics. This is based on the assumption that keywords are the most likely tokens to occur in human-generated headlines. 7.3 Experiments and Results We trained our model with a corpus consisting of roughly 1.3 million financial news articles fetched from the web, written in English, and published on the second half of 2012. We decided to add three important constraints to the learning algorithm which proved to yield positive empirical results: (1) Large news articles are simplified by eliminating their most redundant or least informative sentences. For this end, the text ranking algorithm proposed by Mihalcea and Tarau (2004) is used for discriminating salient sentences in the article. Furthermore, a news article is considered large if it has more than 300 tokens, which corresponds to the average number of words per article in our training set. (2) Because we observed that less than 2% of the headlines in the training set contained more than 15 tokens, we constraint the decoding algorithm to only generate headlines consisting of 15 or fewer tokens. (3) We restrain the decoding algorithm from placing symbols such as commas, quotation marks, and question marks on headlines. Nonetheless, only headlines that end with a period are considered as solutions; otherwise the model tends to generate non-conclusive phrases as titles. For automated testing purposes we use a training set consisting of roughly 12,000 previously unseen articles, which were randomly extracted from the initial dataset before training. The evaluation consisted in producing seven candidate headlines per article: one for each of the four baselines, plus one for each of the three variations of our model (each differing solely on the scheme used to learn the weight vector). Then each candidate is compared against the article's reference headline by means of the five proposed metrics. Table 1 summarizes the obtained results of the models with respect to the ROUGE metrics. The results show that our model, when trained with our proposed forced-MIRA update, outperforms all the other baselines on all metrics, except for ROUGE139

2, where all differences are statistically significant when assessed via a paired t-test (p < 0.001). Also, as initially intended, the keywords baseline does produce better scores than all the other methods, therefore it is considered as a naive upper-bound. It must be highlighted that all the numbers on the table are rather low, this occurs because, as noted by Zajic et al. (2002), humans tend to use a very different vocabulary and writing-style on headlines than on articles. The effect of this is that our methods and baselines are not capable of producing headlines with wordings strongly similar to human-written ones, which as a consequence makes it almost impossible to obtain high ROUGE scores.
Perceptron MIRA f-MIRA 1st sent. HMM Word graphs Keywords R-1 0.157 0.172 0.187 0.076 0.090 0.174 0.313 R-2 0.056 0.042 0.054 0.021 0.009 0.060 0.021 R-SU 0.053 0.057 0.065 0.025 0.023 0.060 0.112 R-WSU 0.082 0.084 0.095 0.038 0.038 0.084 0.148

Table 1: Result of the evaluation of our models and baselines with respect to ROUGE metrics.

For having a more objective assessment of our proposal, we carried out a human evaluation of the headlines generated by our model when trained with the f-MIRA scheme and the word graphs approach by Filippova (2010). For this purpose, 100 articles were randomly extracted from the test set and their respective candidate headlines were generated. Then different human raters were asked to evaluate on a Likert scale, from 1 to 5, both the grammaticality and informativeness of the titles. Each article-headline pair was annotated by three different raters. The median of their ratings was chosen as a final mark. As a reference, the raters were also asked to annotate the actual human-generated headlines from the articles, although they were not informed about the provenance of the titles. We measured inter-judge agreement by means of their IntraClass Correlation (ICC) (Cicchetti, 1994). The ICC for grammaticality was 0.51 ± 0.07, which represents fair agreement, and the ICC for informativeness was 0.63 ± 0.05, which represents substantial agreement. Table 2 contains the results of the models with

