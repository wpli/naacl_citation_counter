4

Medication Detection

Detecting medication terms in discussion forums is challenging because of their informal nature. As we will show in Section 4.1, dictionary look-up from lexicons is not sufficient. Therefore the first part of our research aims to create an effective medication detector for these informal veterinary texts. We used the Stanford CoreNLP tools (Manning et al., 2014) for lemmatization, POS tagging and parsing, and created a SVM classifier with a linear kernel using SVMlin (Sindhwani and Keerthi, 2006). The classifier labels each token as a medication term or not a medication term. Adjacent medication tokens are then combined into a single medication mention. We designed three types of features: Word Features include the medication word's string, lemma, and part-of-speech tag. Since drugs often have common affixes (e.g., "-sone" is a common suffix for corticosteroids), we also defined features for character sequences of length 2-4 at the beginning and end of a word. Local Context Features represent the word preceding and the word following each medication term. We replace numbers with the symbol "CD". We also defined features to represent the syntactic dependency relations linked to the medication word using the Stanford Parser (De Marneffe et al., 2006). Web Context Features capture information from web sites that mention a term, which provides external context beyond the information available in the training texts. During training, we issued a Google query for each unique word in our training data and collected the title and text snippets of the top 10 retrieved documents. We then defined binary-valued features to represent all of the words in the retrieved texts.2 We store the results of each query so that additional queries are needed only for previously unseen words. 4.1 Medication Detection Results We conducted 10-fold cross-validation experiments on our data set to evaluate our medication detector. First, we created three baselines to assess the difficulty of medication detection for this data. The first row of Table 1 shows the performance of a veteriWe also tried different context windows but found that using the title and entire snippet achieved the best results.
2

nary thesaurus manually created by the VIN.3 We extracted all of the words in the entries categorized as Pharmacologic Substance and label all instances of those words as medication terms. The VIN thesaurus achieved high precision but only 51% recall. Some reasons for the low coverage include abbreviations, misspellings, general terms (e.g., "drug"), and pronouns that refer to medications (which are annotated in our data). The second row shows the results of MedEx (Xu et al., 2010), which uses the RxNorm drug lexicon and ranked in second place for the 2009 i2b2 Medication Extraction challenge. MedEx's low precision is primarily due to labeling chemical substances (e.g., "glucose") as medications, but in our data they are often test results (e.g., "the cat's glucose level..."). The third row shows the results of creating a Training Lexicon by collecting all nouns annotated as medications in the training data. We labeled all instances of these nouns as medication terms in the test data, which produced slightly higher recall and precision than MedEx.
Method Precision Recall VIN thesaurus 90.9 51.3 MedEX 52.5 73.8 Training Lexicon 59.4 76.9 SVM Classifier Word Features 88.2 79.9 + Local Context 89.7 81.2 + Web Context 89.2 86.1 F 65.6 61.4 67.0 83.9 85.3 87.6

Table 1: Medication Detection Results

The last three rows in Table 1 show the results for our classifier. With only Word Features, the classifier produced an 83.9% F score, outperforming the baselines. Adding the Local Context Features yielded small gains in recall and precision. The Web Context Features further increased recall from 81% to 86%, raising the F score to 87.6%. We tried adding features for the VIN thesaurus and MedEx system, but they did not improve upon the results obtained with the Web Context features. We observed that the Web Context Features can compensate for small amounts of training data. To demonstrate how powerful they are, we randomly selected 100 gold standard texts to use as a test
3

We used a version provided to us in 2013.

1454

