gives an example. In this formulation, bigram constraints are sufficient to ensure a globally consistent tagging of the sentence. There are N = 26 noun supersense classes and V = 16 verb classes (including the auxiliary verb class, abbreviated `a). In principle, then, there are {O o B b ~ I~ i} × (1 + N + V ) + {¯ I¯ i} = 260
6 43 2

New Capitalization Features 1. capitalized  i = 0  majority of tokens in the sentence are capitalized 2. capitalized  i > 0  w0 is lowercase Auxiliary Verb vs. Main Verb Feature 3. posi is a verb  posi+1 is a verb  (posi+1 is an adverb  posi+2 is a verb) WordNet Supersense Features (unlexicalized) Let cposi denote the coarse part-of-speech of token i: common noun, proper noun, pronoun, verb, adjective, adverb, etc. This feature aims primarily to inform the supersense label on the first token of nominal compounds and light verb constructions, where the "semantic head" is usually a common noun subsequent to the beginning of the expression: 4. subsequent noun's 1st supersense: where cposi is a common noun, verb, or adjective, cposi  for the smallest k > i such that posk is a common noun, the supersense of the first WordNet synset for lemma k --provided there is no intervening verb ( j such that cpos j is a verb and i < j < k) The following two feature templates depend on the tag yi . Let flag(yi ) denote the positional flag part of the tag (O, B, etc.) and sst(yi ) denote the supersense class label: 5. 1st supersense: · if flag(yi )  {O, o}: the supersense of the first WordNet synset for lemma i · else if cposi is a verb and there is a subsequent verb particle at position k > i with no intervening verb: the supersense of the first synset for the compound lemma i , k  (provided that the particle verb is found in WordNet) · otherwise: the supersense of the first WordNet synset for the longest contiguous lemma starting at position i that is present in WordNet: i , i+1 , .. . ,  j  ( j  i) 6. has supersense: same cases as the above, but instead of encoding the highest-ranking synset's supersense, encodes whether sst(yi ) is represented in any of the matched synsets for the given lemma. Note that for a given token, this feature can take on different values for different tags.

possible tags encoding position and class information, allowing for chunks with no class because they are neither nominal nor verbal expressions. In practice, though, many of these combinations are nonexistent in our data; for experiments we only consider tags occurring in train, yielding Y = 146. We also run a condition where the supersense refinements are collapsed, i.e. Y consists of the 8 MWE tags. This allows us to measure the impact of the supersenses on MWE identification performance. 4.5 Features We constrast three feature sets for full supersense tagging: (a) Schneider et al.'s (2014a) basic MWE features, which include lemmas, POS tags, word shapes, and whether the token potentially matches entries in any of several multiword lexicons; (b) the basic MWE features plus the Brown clusters (Brown et al., 1992) used by Schneider et al. (2014a); and (c) the basic MWE features and Brown clusters, plus several new features shown in figure 4. Chiefly, these new features consult the supersenses of WordNet synsets associated with words in the sentence: the first WordNet supersense feature is inspired by Ciaramita and Altun (2006) and subsequent work on supersense tagging, while the has-supersense feature is novel. There is also a feature aimed at distinguishing auxiliary verbs from main verbs, and new capitalization features take into account the capitalization of the first word in the sentence and the majority of words in the sentence. To keep the system as modular as possible, we refrain from including any features that depend on a syntactic parser.
class label should be interpreted as extending across the entire expression. This is for a technical reason: as our scheme allows for gaps, the classes of the tags flanking a gap in a strong MWE would be required to match for the analysis to be consistent. To enforce this in a bigram tagger, the within-gap tags would have to encode the gappy expression's class as well as their own, leading to an undesirable blowup in the size of the state space.

Figure 4: New features for MWE and supersense tagging. They augment the basic MWE feature set of Schneider et al. (2014a), and are conjoined with the current tag, yi .

The model's percepts (binary or real-valued functions of the input16 ) can be conjoined with any tag y  Y to form a feature that receives its own weight
16 We use the term percept rather than "feature" here to emphasize that we are talking about functions of the input only, rather than input­output combinations that each receive a parameter during learning.

1543

