discarded as noisy in the final model. Overall, H EADY is better at capturing paraphrases from the head of the pattern distribution, and is likely to ignore most of the long tail where useful paraphrases can still be found. · Simplifying assumptions. We already mentioned that the two systems share a common underlying assumption, i.e., that good paraphrase candidates can be found by looking at news published on the same day and mentioning the same entities. On top of this, N EWS S PIKE also assumes that better paraphrases are reported around spiky entities, verb tenses may not differ, there is one event mention per discourse, and others. These restrictions are not enforced by H EADY, where the common assumption is indeed even relaxed across days and entity sets. · Annotated data. N EWS S PIKE requires handannotated data to train the parameters of a supervised model that combines the different heuristics, whereas H EADY does not need annotated data. This paper describes I DEST, a new method for learning paraphrases of event patterns that is designed to combine the advantages of these two systems and compensate for their weaknesses. It is based on a new neural-network architecture that, like H EADY, only relies on the weak supervision signal that comes from the news published on the same day, requiring no additional heuristics or training data. Unlike N EWS S PIKE, it can generalize across different sets of extracted patterns, and each event pattern is mapped into a low-dimensional embedding space. This allows us to define a neighborhood around a pattern to find the ones that are closer in meaning. I DEST produces a robust global model that can also capture meaningful representations for rare patterns, thus overcoming one of H EADY's main limitations. Our evaluation of the potential trade-off between local and global paraphrase models shows that comparably good results to N EWS S PIKE can be attained without relying on supervised training. At the same time, the ability of I DEST to produce a global model allows it to benefit from a much larger news corpus. 1141

2

Related work

Relational Open-IE In an early attempt to move away from domain-specific, supervised IE systems, Riloff (1996) attempted to automatically find relational patterns on the web and other unstructured resources in an open domain setting. This idea has been further explored in more recent years by Brin (1999), Agichtein & Gravano (2000), Ravichandran & Hovy (2002) and Sekine (2006), among the others. Banko et al. (2007) introduced OpenIE and the T EXT RUNNER system, which extracted binary patterns using a few selection rules applied on the dependency tree. More recent systems such as R E V ERB (Fader et al., 2011) and O L LIE (Mausam et al., 2012) also define linguisticallymotivated heuristics to find text fragments or dependency structures that can be used as relational patterns. A natural extension to the previous work is to automatically identify which of the extracted patterns have the same meaning, by producing either a hard or a soft clustering. Lin & Pantel (2001) use the mutual information between the patterns and their observed slot fillers. Resolver (Yates & Etzioni, 2007) introduces a probabilistic model called the Extracted Shared Property (ESP) where the probability that two instances or patterns are paraphrases is based on how many properties or instances they share. USP (Poon & Domingos, 2009) produces a clustering by greedily merging the extracted relations. Yao et al. (2012) employ topic models to learn a probabilistic model that can capture also the ambiguity of polysemous patterns. More recent work also organizes patterns in clusters or taxonomies using distributional methods on the pattern contexts or entities extracted (Moro & Navigli, 2012; Nakashole et al., 2012), or implicitly clusters relational text patterns via the learning of latent feature vectors for entity tuples and relations, in a setting similar to knowledge-base completion (Riedel et al., 2013). A shared difficulty for systems that cluster patterns based on the arguments they select is that it is very hard for them to distinguish between identity and entailment. If one pattern entails another, both are likely to be observed in the corpus involving the same entity sets. A typical example illustrating this problem is the two patterns e1 married e2 and e1

