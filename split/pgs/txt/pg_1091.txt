ligature

diacritic

non-standard spelling

character elision

obsolete character

Input: Language model:

praesertim urgente causa

Figure 1: An example OCR input showing the original image and an example of an equivalent modernized text similar to data used to train the LM. and where indigenous-language orthographies were first being developed (Baddeley and Voeste, 2013). The 349 digital facsimiles in the Primeros Libros collection are characteristic of this trend. Produced during the first century of Spanish colonization, they represent the introduction of printing technology into the Americas, and reflect the (sometimes conflicted) priorities of the nascent colony, from religious orthodoxy to conversion and education. For our experiments, we focus on multilingual documents in three languages: Spanish, Latin, and Nahuatl. As Berg-Kirkpatrick et al. (2013) show, a language model built on contemporaneous data will perform better than modern data. For this reason, we collected 15­17th century texts from Project Gutenberg,1 producing Spanish and Latin corpora of more than one million characters each. Due to its relative scarcity, we augmented the Nahuatl corpus with a private collection of transcribed colonial documents.

cal offset of each character from a common baseline. Additionally, since documents exhibit variable inking levels (where individual characters are often faded or smeared with blotched ink) the system also models the amount of ink applied to each type piece. The generative process operates as follows. First, a sequence of character tokens is generated by a character n-gram language model. Then, bounding boxes for each character token are generated, conditioned on the character type, followed by vertical offsets and inking levels. Finally, the pixels in each bounding box are generated, conditioned on the character types, vertical offsets, and inking levels. In this work, we focus on improving the language model, and leave the rest of the generative process untouched.

4 Language Model
We present a new language model for Ocular that is designed to handle issues that are characteristic of older historical documents: code-switching and orthographic variability. We extend the conventional character n-gram language model and its training procedure to deal with each of these problems in turn. 4.1 Code-Switching

3

Baseline System

The starting point for our work is the Ocular system described by Berg-Kirkpatrick et al. (2013). The fonts used in historical documents are usually unknown and can vary drastically from document to document. Ocular deals with this problem by learning the font in an unsupervised fashion ­ directly from the input historical document. In order to accomplish this, the system uses a specialized generative model that reasons about the main sources of variation and noise in historical printing. These include the shapes of the character glyphs, the horizontal spacing between characters, and the verti1

http://www.gutenberg.org/

Because Ocular's character n-gram language model (LM) is fixed and monolithic, even when it is trained on corpora from multiple languages, it treats all text as a single "language"--a multilingual blur at best. As a result, the system cannot model the fact that different contiguous blocks of text correspond to specific languages and thus follow specific statistical patterns. In order to transcribe documents that feature intrasentential code-switching, we replace Ocular's simple n-gram LM with one that directly models code-switching by representing language segmentation as a latent variable. Our code-switching LM generates a sequence of pairs (ei , i ) where ei is the current character and i is the current language. The sequence of languages i specifies the segmentation of generated text into language regions. Our LM is built from several component models: First, for each language type , we incorporate a standard monolingual character n-gram model trained on data from language . The com-

1037

