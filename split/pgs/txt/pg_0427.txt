We follow Flanigan et al. (2014) in setting up the train/development/test splits1 for easy comparison: 4.0k sentences with document years 1995-2006 as the training set; 2.1k sentences with document year 2007 as the development set; 2.1k sentences with document year 2008 as the test set, and only using AMRs that are tagged ::preferred. Each sentence w is preprocessed with the Stanford CoreNLP toolkit (Manning et al., 2014) to get partof-speech tags, name entity information, and basic dependencies. We have verified that there is no overlap between the training data for the Stanford CoreNLP toolkit2 and the AMR Annotation Corpus. We evaluate our parser with the Smatch tool (Cai and Knight, 2013), which seeks to maximize the semantic overlap between two AMR annotations. 5.2 Action Set Validation

relation labels or/and gold concept labels. We can see in Table 3 that when provided with gold concept and relation labels as input, the parsing accuracy improves around 8% F-score (Row 6). Rows 4 and 5 present results when the parser is provided with just the gold relation labels (Row 4) or gold concept labels (Row 5), and the results are expectedly lower than if both gold concept and relation labels are provided as input. JAMR Our parser Our parser +lgr Our parser +lgc Our parser +lgrc Precision .52 .64 .68 .69 .72 Recall .66 .62 .65 .67 .70 F-score .58 .63 .67 .68 .71

One question about the transition system we presented above is whether the action set defined here can cover all the situations involving a dependencyto-AMR transformation. Although a formal theoretical proof is beyond the scope of this paper, we can empirically verify that the action set works well in practice. To validate the actions, we first run the oracle() function for each sentence w and its dependency tree Dw to get the "pseudo-gold" Gw . Then we compare Gw with the gold-standard AMR graph represented as span graph Gw to see how similar they are. On the training data we got an overall 99% F-score for all Gw , Gw pairs, which indicates that our action set is capable of transforming each sentence w and its dependency tree Dw into its goldstandard AMR graph through a sequence of actions. 5.3 Results

Table 3: Results on the test set. Here, lgc - gold concept label; lgr - gold relation label; lgrc - gold concept label and gold relation label. 5.4 Error Analysis

Table 3 gives the precision, recall and F-score of our parser given by Smatch on the test set. Our parser achieves an F-score of 63% (Row 3) and the result is 5% better than the first published result reported in (Flanigan et al., 2014) with the same training and test set (Row 2). We also conducted experiments on the test set by replacing the parsed graph with gold
1 A script to create the train/dev/test partitions is available at the following URL: http://goo.gl/vA32iI 2 Specifically we used CoreNLP toolkit v3.3.1 and parser model wsjPCFG.ser.gz trained on the WSJ treebank sections 02-21.

Figure 9: Confusion Matrix for actions tg , t . Vertical direction goes over the correct action type, and horizontal direction goes over the parsed action type. Wrong alignments between the word tokens in the sentence and the concepts in the AMR graph account for a significant proportion of our AMR parsing errors, but here we focus on errors in the transition from the dependency tree to the AMR graph. Since in our parsing model, the parsing process has been decomposed into a sequence of actions applied to the input dependency tree, we can use the oracle() function during parsing to give us the cor-

373

