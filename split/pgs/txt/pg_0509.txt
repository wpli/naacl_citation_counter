foreach sentence s do foreach sem. role ARGM - LOC(ypred , y)  s do foreach sem. role ARGi (xpred , x)  s do if is valid(x, y) then Is x located at y before ypred ? Is x located at y during ypred ? Is x located at y after ypred ?

Algorithm 1: Procedure to generate potential additional spatial knowledge of type (1) (Section 2.2).
Obviously, [the pilot]ARG0 , v1 did[n't]ARGM- NEG, v1 [think]v1 [too much]ARGM- EXT, v1 [about [what]ARG1 , v2 was [happening]v2 [on the ground]ARGM- LOC, v2 , or . . . ]ARG1 , v1

 probNO: It is probable that the answer is no, but it is not guaranteed.  UNK: There is not enough information to answer, I can't tell the location of x. The goal is to infer spatial knowledge as gathered by humans when reading text. Thus, annotators were encouraged to use commonsense and world knowledge. While simple and somewhat open to interpretation, these guidelines allowed as to gather annotations with "good reliability" (Section 3.3.1). 3.2 Annotation Examples

Figure 3: Sample sentence and semantic roles. Pair (x: about what was happening on the ground, y: on the ground) is invalid because x contains y. All potential additional spatial knowledge is generated with Algorithm 1, and a manual annotation effort determines whether spatial knowledge should be inferred. Algorithm 1 loops over all ARGM - LOC roles, and generates questions regarding whether spatial knowledge can be inferred for any numbered argument within the same sentence. is valid(x, y ) returns True if (1) x is not contained in y and (2) y is not contained in x. Considering invalid pairs would be trivial or nonsensical, e.g., pair (x: about what was happening on the ground, y: on the ground) is invalid in the sentence depicted in Figure 3. 3.1 Annotation Process and Guidelines

In a first batch of annotations, two annotators were asked questions generated by Algorithm 1 and required to answer YES or NO. The only information they had available was the source sentence without semantic role information. Feedback from this first attempt revealed that (1) because of the nature of x or y, sometimes questions are pointless, and (2) because of uncertainty, sometimes it is not correct to answer YES or NO, even tough there is some evidence that makes either answer likely. Based on this feedback, and inspired by previous annotation guidelines (Saur i and Pustejovsky, 2012), in a second batch we allowed five answers:  certYES: I am certain that the answer is yes.  probYES: It is probable that the answer is yes, but it is not guaranteed.  certNO: I am certain that the answer is no. 455

In this section, we present annotation examples after resolving conflicts (Figure 4). These examples show that ambiguity is common and sentences must be fully interpreted before annotating. Sentence 4(a) has four semantic roles for verb collecting (solid arrows), and annotators are asked to decide whether ARG0 and ARG1 of collecting are located at the ARGM - LOC before, during or after collecting (discontinuous arrows). Annotators interpreted that the FBI agents and divers (ARG0 ) and evidence (ARG1 ) were located at Lake Logan (ARGM LOC ) during collecting (certYES). They also annotated that the FBI agents and divers were likely to be located at Lake Logan before and after (probYES). Finally, they determined that the evidence was located at Lake Logan before the collecting (certYES), but probably not after (probNO). These annotations reflect the natural reading of sentence 4(a): (1) people and whatever they collect are located where the collecting takes place during the event, (2) people collecting are likely to be at that location before and after (i.e., presumably they do not arrive immediately before and leave immediately after), and (3) the objects being collected are located at that location before collecting, but probably not after. Sentence 4(b) is more complex. First, potential relation LOCATION(in sight, at the intersection ) is annotated UNK: it is nonsensical to ask for the location of sight. Second, the Disney symbols are never located at the intersection (certNO). Third, both the car and security guard were located at the intersection during the stop for sure (certYES). Fourth, annotators interpreted that the car was not at the intersection before (certNO), but they were not sure about after (probNO). Fifth, they considered that the security guard was probably located at the intersec-

