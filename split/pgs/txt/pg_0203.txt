2 1.9 Mean clip quality 1.8 1.7 1.6 1.5 1.4 1.3 1.2 0 0.5 1 Number of clips 1.5
Millions

Action Object

Figure 5: Average clip quality (precision) after filtering out
low confidence clips versus # clips retained (recall).

164 121 76

170

70

-2

-1

0 Rating

1

2

Figure 6: Histogram of human ratings comparing recipe steps

against ASR descriptions of a video clip. "2" indicate a strong preference for the recipe step; "-2" a strong preference for the transcript. See text for details.

the test set is not dominated by frequent actions or objects. We then performed a Mechanical Turk experiment on each test set. Each clip was shown to 3 raters, and each rater was asked the question "How well does this clip show the given action/object?". Raters then had to answer on a 3-point scale: 0 means "not at all", 1 means "somewhat", and 2 means "very well". The results are shown in Figure 4. We see that the quality of the hybrid method is significantly better than the baseline keyword spotting method, for both actions and objects.4 While a manually curated
4 Inter-rater agreement, measured via Fleiss's kappa by aggregating across all judgment tasks, is .41, which is statistically significant at a p < .05 level.

speech transcript indeed yields better results (see the bars labeled `manual'), we observe that automatically generated transcripts allow us to perform almost as well, especially using our alignment model with visual refinement. Comparing accuracy on actions against that on objects in the same figure, we see that keyword spotting is far more accurate for actions than it is for objects (by over 30%). This disparity is not surprising since keyword spotting searches only for action keywords and relies on a rough heuristic to recover objects. We also see that using alignment (which extracts the object from the "clean" recipe text) and visual refinement (which is trained explicitly to detect ingredients) both help to increase the relative accuracy of objects -- under the hybrid method, for example, the accuracy for actions is only 8% better than that of objects. Note that clips from the HMM and hybrid methods varied in length between 2 and 10 seconds (mean 4.2 seconds), while clips from the keyword spotting method were always exactly 8 seconds. Thus clip length is potentially a confounding factor in the evaluation when comparing the hybrid method to the keyword-spotting method; however, if there is a bias to assign higher ratings to longer clips (which are a priori more likely to contain a depiction of a given action than shorter clips), it would benefit the keyword spoting method. Segment confidence scores (from Section 3.5) can be used to filter out low confidence segments, thus improving the precision of clip retrieval at the cost of recall. Figure 5 visualizes this trade-off as we vary our confidence threshold, showing that indeed, segments with higher confidences tend to have the highest quality as judged by our human raters. Moreover, the top 167,000 segments as ranked by our confidence measure have an average rating exceeding 1.75. We additionally sought to evaluate how well recipe steps from the recipe body could serve as captions for video clips in comparison to the often noisy ASR transript, which serves as a rough proxy for evaluating the quality of the alignment model as well as demonstration a potential application of our method for "cleaning up" noisy ASR captions into complete grammatical sentences. To that end, we randomly selected 200 clips from our corpus that

# Raters

149

