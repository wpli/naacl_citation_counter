(a) e1 = hmix, , tarragon, vinegari (a) Butter a deep baking dish, e2 =(b) hblend , , mustard i e1 = hbutter, dishi put apples, water, flour, sugar e3 = hmix, , salt, pepperi (a) (b) Butter deep dish, put e2 = hput , apples, water, ... cinnamon in ait. Mix with e4 =and hblend , , mayonnaise ,baking sour cream i e1 = flour butter , dish apples, water, flour, sugar and cin, cinnamon, iti spoon and spread butter and salt e = hcover, i e2 = put, apples, water, ...5 in mix. it. MixBake with spoon and e3 = hmix , spoon, i over the at e6 = hchill ,namon iapple flour, cinnamon , it spread butter and salt over the ape4 = hspread , butter, salt, mixi 350 degrees F until the apples e3 = mix, with spoon, ple mix. Bake at 350 degrees F until e5 = hbake , Fspread i e4 = , butter, salt, ... are tender and the crust brown, the minutes. apples are Serve tender and the crust e6 = hserve, cream, cream i about 30 with over mix
e5 = bake, F e6 = serve, cream, cream
brown, about 30 minutes. cream or whipped cream. Serve with cream or whipped cream.

(b) you mix the tarragon and vinegar together and blend in the mustard. you mix in the salt and (c) pepper, blending well. you blendein the mayon1 naise and then the sour e5 and cream. you cover s chill.
e3 e6

(c)

e1
e2 f e4

s e3

Figure 1: (a) The sequence of events representing the recipe for the dish "Apple Crisp Ala [sic] Brigitte." (b) The actual recipe for this dish. (c) A complete Figure graph over the Example set of events with start and finisha states. Each in the graph is one of the 1: (a) of events describing recipe for internal the dishnode "." (b) The actual recipe for this dish. (c) A c events ei for i  {1, . . . , 6}. graph The path in blue denotes thewith correct Hamiltonian describing the set ofnode actions ordered inis one of the even over the bold set of events start and end path states. Each internal inas the graph the recipe. Red edges denote edges from start state and to the endCrisp state. Ala The [sic] in practice, are weighted. Figure 1: (a) Example of events describing the dish "Apple Brigitte." For brevity, i 2 {1, .a. recipe .the , 5} . for The path in bold denotes theedges, correct Hamiltonian path arguments describing the set of actions that ne are represented as headwords and taken their syntactic type is omitted. (b) The actual recipe for this dish. (c) A complete to follow the recipe. graph over the set of events with start and end states. Each internal node in the graph is one of the events ei for 4.3 Learning i 2 {1, .4.2 . . , 6}Inference . The path in blue bold denotes the correct Hamiltonian path describing the set of actions as ordered in the recipe. Red edges denote edges from the start state and to the end state. The edges, in practice, 3 Model, Inference and Learning The learning problem takesare as weighted. input a dataset con-

As mentioned above, inference with the edge- sisting of unordered sets of events, paired with a tarm +1 X Insolving this section we describe the main learning compofactored model requires the maximization get ordering. We consider two typesscore( of learning alh| S ) =  > ( hi ) ILP formulation yields superior performance to the nents that compose our approach to event ordering. problemsystems in Eq. (§8). 2. This corresponds to finding a gorithms for theX edge-factored model in the previousi=1 other evaluated ILP has been proven to Hamiltonian path in a complete graph, which is genZ (  , S, e ) = exp ( e1 , e ) section. The first learns in a> global training setting 1 be a practical and flexible tool in various structured Given a weight vector w and a set of even e : ( e ,e ) 2 E ( S ) erally an NP-hard problem. Reasonable approxima1 using the averaged structured perceptron (Collins, Event Ordering prediction tasks in NLP (Roth3.1 and Edge-Factored tau Yih, 2007; Model for is carried out by the computing the highe where e S [ the {s, decoding f }. Thisference is a locally normaltions for 2012; this problem are also NP-hard. Still tech1 , e2 2with 2002), algorithm being either Talukdar et al., Scaria et al., 2013). Our ILP We now turn to explain the linear model we use for ing Hamiltonian path in (S ): ized model that(henceforth, gives the probability ofRC), G niques are developed for specialized cases, due to log-linear one based on ILP G LOBAL -P or formulation is given in Appendix A. ordering events in time. Let S = { v , . . . , v }  U 1 to node me(G transitioning node e1). . Maximizing the problem's importance in discrete optimization. 2 from the greedy one REEDY -P RC Given a training inWe experiment with an additional greedy inferbe a set of events as mentioned in section 2. Let c the score in Eq. 1 hasits ancorrect interpretation of finding the perstance S and label h , the structured ence algorithm, similar to the one described LaDespite its theoretical this maxih = arg max score(h|S ) G(NP-hardness, S ) = (S [by {s, e }, E (S )) beceptron an almost-complete highest scoring path according to an edge-factored calls the inference procedure as a subroutine h 2 H (S ) pata (2003) for sentence The algorithm itmization problemordering. can be represented an Integer directed graph as with E (SMarkovian ) = (Sand [ {supdates })  (S such [ {eweight }that: )  vector  according to the difmodel, the eratively selects an outgoing (starting from the Linear Program (ILP),edge and solved using generic +1 (U then U ). Every Hamiltonian path5 in G(Sm ) thatvalue where Hfeature (S ) is the set of on Hamiltonian paths Y ference between the of the function node s)techniques that has thefor largest to a node that has ILP weight optimization. Due to the relap ( h |  , S ) = p ( e | e ,  , S ) , i i 1 starts in s and ends in e can bethe thought of aspath an orthat start with s and end with e. h is the b (h predicted ( h i )) and on the correct i=2 not been visited so length far, until all dering vertices are covered, tively short of recipes (13.8 events on average of the events in S . The path edge (v , v )h in)such ordering of the set of events S accordin ).+1 ) is poral im where h = ((hi ,c. j. ( .,h a Hamiltonian path h 1 at which the paththe terminates by travelling to fv . solved inpoint our corpus), problem can be effectively a path denotes that is the event that comes before structured model i learning algorithm tryin is Equation based 1 with weigh with hi The = (second e i 1 , ei ) being a directed we edge in in most cases. 4.3 Learning vj . w.algorithm maximizes the on factored training. This the path. Initial experimentation suggested that The learning problem takes as input a dataset conThe modeling problem, therefore, is to score The proposed algorithmic setting is appealing for likelihood of a conditional log-linear model greedy inference (henceforth, 3.2 G REEDY -L OG L IN)p: Inference sisting of of linear events, paired with a tarHamiltonian paths inallows a works given directed graph G ( S ) its unordered flexibility.sets The score formulation better in practice than the ILP formulation get ordering. We rich consider two such types of learning As mentioned above, inference with the us to use features, while using ILP alallows to as the above. Here, we use an edge-factored for the locally-normalized model. We do exp  therefore (e1 , e d gorithms for the edge-factored model in the previous factored model we 2 ) presented would have to s easily incorporate structural constraints. ILP model. Let Indeed, : (U not U )report ! results R be a global feature p(e = on inference with this log2 |e 1 , , S ) section.has The first learns in a global training setting maximization problem in Eq. 2. This corr Z (, S, e1 ) been proven valuable in various NLP tasks (Roth vector for pairs of events, represented as directed linear model. G REEDY -L OG L IN closely resembles d using the averaged structured perceptron (Collins, to finding an Hamiltonian path in a complet and Yih, 2007; Talukdar et al., In 2012; Scarialet etthe al., edges. addition,  2 R be a weight vec- (2003), except that it is learning model of Lapata 2002), with the decoding algorithm being either the which is generally an NP-hard problem6 . In 2013). See Appendix A tor. for our ILP we formulation. Then, define the score of an Hamiltonian a discriminative log-linear model, rather of a generone based on ILP (henceforth,path G LOBAL RC ),. .or Zh (, S, e = exp   e1 , reasonable e) there is(no approximatio 1) h = -P (h ,. , hm+1 ) (where E (S ) for eral case i 2 ative Markovian model. As a baseline, experiment with1an additional the greedy one (G REEDYwe -P RC ). e : ( e ,e )  E ( S ) rithm to solve the maximization algorithm, a 1 i 2 {1, . . . , m + 1}) as: greedy inference algorithm, similar to the one de5 The Feature Set The second learning algorithm we try is based 6 The NP complete problem of finding a Hamilton scribed by Lapata (2003) for sentence ordering. The 1where e1 ,all e2 the S  {s, f }set . This is a locally normalTable presents complete of features used 5 on factored training. This algorithm maximizes the An Hamiltonian path in a graph is a path that visits all in an undirected graph can be trivially reduced to fi algorithm iteratively selects anmodel outgoing ized log-linear model that gives the probability of for defining the feature function . We consider likelihood of a conditional log-linear p: edge (startnodes exactly once. maximal Hamiltonian cycle in a directed graph. ing from the node that to sets transitioning to node e2 from node the e1 . Maximizing three of features: Lexical encodes writexp > s () e1 , e2 )has the largest weight p( e 2 | e , , S )that = has not been visited so far, until all ten a1node ver-forms the of score Eq. 1 has predicates an interpretation of finding the thein event pair and objects; Z (, S, e1 ) tices are covered, at which point the path terminates highest scoring path according to an edge-factored by traveling to f . Markovian model, such that: 1164

