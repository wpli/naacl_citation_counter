We then find an edge set M, the best matching of G . X (S, C ) is defined as the sum over edge weights in M, i.e. X (S, C ) = eM weight(e). An example is illustrated in Figure 2.
Article Summary S0: The Crimean parliament officially 0.8 declared independence 0.1 0.1 and requested full accession to the Russian Federation. S1: Obama declared 0.3 0.8 sanctions on Russian 0.5 officials considered responsible for the crisis. Comment Summary C0: The "Crimean Parliament", headed by an ethnic Russian separatist, has voted for Crimea to be annexed into Russia... C1: The Obama administration has warned of "increasingly harsh sanctions", but it is unlikely that Europe will comply... C2: Sanctions are effective and if done in unison with the EU...

tite graph matching problem. This problem can be reduced to a maximum network flow problem, and then be solved by Ford-Fulkerson algorithm (details are discussed in (Kleinberg and Tardos, 2005)). Thereafter, for each iteration, we alternately find a better St with regard to Squal (S ; T ) +  X (S, Ct-1 ) using hill climbing, and an exact solution Ct to Cqual (C )+  X (St , C ) with Ford-Fulkerson algorithm. Iteration stops when the increase of Z (S, C ) is below threshold (set to 0.01). System performance is stable when we vary   [1.0, 10.0], so we set  = 1.0.
Input : sentences Vs , comments Vc , threads T ,  , threshold , functions Z (S, C ; T ), Squal (S ; T ), Cqual (C ), X (S, C ) Output: article summary S , comment summary C /* Initialize S and C by greedy algorithm
and Ford-Fulkerson algorithm

Figure 2: An example on computing the connectivity

between an article summary (left) and a comment summary (right) via best matching in bipartite graph. Number on each edge indicates the content similarity between a sentence and a comment. Solid lines are edges in the best matching graph. For this example, the connectivity X (S, C ) is 0.8 + 0.8 = 1.6.

We consider two options for conn(s, c). One is lexical similarity which is based on TF-IDF vectors. Another is semantic similarity. Let Rs = {(as , rs , bs )} and Rc = {(ac , rc , bc )} be the sets of dependency relations in s and c. conn(s, c) is calculated as:
(as ,rs ,bs )Rs

max(ac ,rc ,bc )Rc simi(as , ac ) × simi(bs , bc )
rs =rc

where simi(·, ·) is a word similarity function. We experiment with shortest path based similarity defined on WordNet (Miller, 1995) and Cosine similarity with word vectors trained on Google news (Mikolov et al., 2013). Systems using the three metrics that optimize Z (S, C ; T ) are henceforth called T HREAD +O PTTFIDF , T HREAD +O PTWordNet and T HREAD +O PTWordVec . 4.4 An Alternating Optimization Algorithm To maximize the full objective function Z (S, C ; T ), we design a novel alternating optimization algorithm (Alg. 1) where we alternately find better S and C . We initialize S0 by a greedy algorithm (Lin and Bilmes, 2011) with respect to Squal (S ; T ). Notice that Squal (S ; T ) is a submodular function, so that the greedy solution is a 1 - 1/e approximation to the optimal solution of Squal (S ; T ). Fixing S0 , we model the problem of finding C0 that maximizes Cqual (C ) +  X (S0 , C ) as a maximum-weight bipar1060

S0  maxS Squal (S ; T ); C0  maxC Cqual (C ) +  X (S0 , C ); t  1; Z   ; while Z > do /* Step 1: Hill climbing algorithm */ St  maxS Squal (S ; T ) +  X (S, Ct-1 ); /* Step 2: Ford-Fulkerson algorithm */ Ct  maxC Cqual (C ) +  X (St , C ); Z = Z (St , Ct ; T ) - Z (St-1 , Ct-1 ; T ); t  t + 1; end Algorithm 1: Generate article summary and comment summary for a given day via alternating optimization .

*/

Algorithm 1 is guaranteed to find a solution at least as good as S0 and C0 . It progresses only if Step 1 finds St that improves upon Z (St-1 , Ct-1 ; T ), and Step 2 finds Ct where Z (St , Ct ; T )  Z (St , Ct-1 ; T ).

5
5.1

Experimental Results
Evaluation of SENTENCE and COMMENT Importance Scorers

We test importance scorers (Section 3) on single document sentence ranking and comment ranking. For both tasks, we compare with two previous systems on joint ranking and summarization of news articles and tweets. Yang et al. (2011) employ supervised learning based on factor graphs to model content similarity between the two types of data. We use the same features for this model. Gao et al. (2012) summarize by including the complementary information between articles and

