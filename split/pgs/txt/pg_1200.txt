of noisy, unrelated patterns in the clusters. The cohesiveness of the clusters, which we will evaluate in Section 5.2, must also be considered to tell constructive and destructive lexical variability apart. 5.1.4 H EADY H EADY produces a soft-clustering from a generative model, and expects the maximum number of clusters to be provided beforehand. The model then tries to approximate this number. In our experiments, 5,496 clusters were finally generated. One weak point of H EADY, mentioned above, is that lowfrequency patterns do not have enough evidence and Noisy-OR Bayesian Networks tend to discard them; in our experiments, only 4.3% of the unique extracted patterns actually ended up in the final model. 5.2 Qualitative analysis

Figure 6: Cluster size (log-scale) and ratio of unique verb lemmas in the clusters generated from N EWS S PIKE and I DEST with compression-based pattern extraction.

The clusters obtained with different systems and dataset have been evaluated by five expert raters with respect to three metrics, according to the following rating workflow: 1. The rater is shown the cluster, and is asked to annotate which patterns are meaningless or unreadable2 . This provides us with a Readability score, which measures at the same time the quality of the extraction algorithm and the ability of the method to filter out noise. 2. The rater is asked whether there is a majority theme in the cluster, defined as having at least half of the readable patterns refer to the same real-world event happening. If the answer is No, the cluster is annotated as noise. We call this metric Cohesiveness. 3. If a cluster is cohesive, the rater is finally asked to indicate which patterns are expressing the main theme, and which ones are unrelated to it. The third metric, Relatedness, is defined as the percentage of patterns that are related to the main cluster theme. All the patterns in a non-cohesive cluster are automatically marked as unrelated.
In the data released by NewsSpike, R E V ERB patterns are lemmatized, but the original inflected sentences are also provided. We have restored the original inflection of all the words to make those patterns more readable for the raters.
2

Figure 7: Cluster size (log-scale) and ratio of unique verb lemmas in the clusters generated from I DEST with compression-based pattern extraction, using only the 500,000 N EWS S PIKE articles, or the large dataset.

our own crawl of news collected between 2008 and 2014. Using sentence compression, hundreds of millions of extractions have been produced. In order to keep the dataset at a reasonable size, and aiming at producing a model of comparable size to the other approaches, we applied a filtering step in which we removed all the event patterns that were not extracted at least five times from the dataset. After this filtering, 28,014,423 extractions remained, grouped in 8,340,162 non-singleton EECs. Figure 7 compares the resulting clusterings. In the all-data setting, clusters were generally smaller and showed less lexical variability. We believe that this is due to the removal of the long tail of lowfrequency and noisy patterns. Indeed, while high lexical variability is desirable it can also be a sign 1146

