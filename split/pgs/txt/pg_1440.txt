Simple task-specific bilingual word embeddings
Stephan Gouws Stellenbosch University stephan@ml.sun.ac.za Anders Søgaard University of Copenhagen soegaard@hum.ku.dk

Abstract
We introduce a simple wrapper method that uses off-the-shelf word embedding algorithms to learn task-specific bilingual word embeddings. We use a small dictionary of easily-obtainable task-specific word equivalence classes to produce mixed context-target pairs that we use to train off-the-shelf embedding models. Our model has the advantage that it (a) is independent of the choice of embedding algorithm, (b) does not require parallel data, and (c) can be adapted to specific tasks by re-defining the equivalence classes. We show how our method outperforms off-the-shelf bilingual embeddings on the task of unsupervised cross-language partof-speech (POS) tagging, as well as on the task of semi-supervised cross-language super sense (SuS) tagging.

1

Introduction

Using multi-layered neural networks to learn word embeddings has become standard in NLP (Turian et al., 2010; Guo et al., 2014). While there is still some controversy whether such methods are superior to older methods (Levy and Goldberg, 2014; Baroni et al., 2014), there is little doubt that continuous word representations can potentially solve some of the data sparsity problems inherent in NLP. Most research on word embeddings has focused on learning representations for the words in a single language, making syntactically or semantically similar words appear close in the embedding space. Embeddings have been applied to many tasks, from
The authors contributed equally to this work. The second author is funded by the ERC Starting Grant LOWLANDS No. 313695.


named entity recognition (Turian et al., 2010) to dependency parsing (Bansal et al., 2014). It has furthermore been shown that weakly supervised embedding algorithms can also lead to huge improvements for tasks like sentiment analysis (Tang et al., 2014). In this work, we also use weak or distant supervision, relying on small dictionary seeds. This paper, however, considers the problem of learning bilingual word embeddings, i.e., word embeddings such that similar words in two different languages end up close in the embedding space. Such bilingual word embeddings can potentially be used for better cross-language transfer of NLP models, as we show in this paper. Previous work on bilingual word embeddings have defined similar words as translation equivalents and evaluated embeddings in the context of document classification tasks (Klementiev et al., 2012; Kocisky et al., 2014). In this paper, we present a simple wrapper method to existing monolingual word embedding algorithms that can learn task-specific bilingual embeddings, e.g., for POS tagging, named entity recognition, or sentiment analysis. Our algorithm is simpler and performs better on the tasks where we could compare performance to existing algorithms. Also, we note that our approach, unlike existing algorithms (Klementiev et al., 2012), is as fast as learning monolingual embeddings. Our contributions In this paper we introduce a new approach for learning bilingual word embeddings and revisit the task of unsupervised crosslanguage POS tagging (Das and Petrov, 2011). Our bilingual embedding model, which we call Bilingual Adaptive Reshuffling with Individual Stochastic Alternatives (BARISTA), takes two (non-parallel) corpora and a small dictionary as input. The dictio-

1386
Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1386­1390, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

