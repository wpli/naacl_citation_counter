Semantic parsing of speech using grammars learned with weak supervision
Britta Wrede Judith Gaspers Philipp Cimiano Applied Informatics Semantic Computing Group Semantic Computing Group Faculty of Technology CITEC CITEC Bielefeld University Bielefeld University Bielefeld University {jgaspers|cimiano|bwrede}@cit-ec.uni-bielefeld.de

Abstract
Semantic grammars can be applied both as a language model for a speech recognizer and for semantic parsing, e.g. in order to map the output of a speech recognizer into formal meaning representations. Semantic speech recognition grammars are, however, typically created manually or learned in a supervised fashion, requiring extensive manual effort in both cases. Aiming to reduce this effort, in this paper we investigate the induction of semantic speech recognition grammars under weak supervision. We present empirical results, indicating that the induced grammars support semantic parsing of speech with a rather low loss in performance when compared to parsing of input without recognition errors. Further, we show improved parsing performance compared to applying n-gram models as language models and demonstrate how our semantic speech recognition grammars can be enhanced by weights based on occurrence frequencies, yielding an improvement in parsing performance over applying unweighted grammars.

1

Motivation

Semantic parsers map natural language utterances (NL) into formal meaning representations (MR), and are applied for both parsing of textual input and in Spoken Language Understanding (SLU). In datadriven SLU research, typically pipeline-based systems are applied in which first an automatic speech recognizer (ASR) is applied to transcribe speech input, and subsequently a semantic parser is applied

to map the transcriptions into some semantic form (Deoras et al., 2013). Such systems typically use different models for recognition and understanding. Since ASR yields recognition errors, parsing performance can degrade rapidly compared to parsing performance on written text. While the performance of ASR and parsing components are often optimized independently of each other, in particular in case of the ASR to minimize recognitions errors, research has shown that ASR transcriptions with a lower error rate can in fact yield worse understanding performance (Wang et al., 2003; Bayer and Riccardi, 2012) and that joint approaches to recognition and understanding can yield improved performance (Wang and Acero, 2006b; Deoras et al., 2013). In particular, Wang and Acero (2006b) have shown that applying the same grammar for speech recognition and understanding can yield improved understanding performance compared to applying a standard n-gram model with the ASR, since dependencies between acoustics and semantics can be captured. Their grammars are, however, learned in a supervised setting. In fact, while semantic grammars are often applied for speech recognition and/or understanding, they are often created manually or ­ as mentioned previously ­ learned from data containing semantic annotations, which are time-consuming to produce. In the field of Natural Language Processing (NLP), the development of semantic parsers has received considerable attention. While some researchers have considered fully supervised settings (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007), requiring accurate and complete semantic annota-

872
Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 872­881, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

