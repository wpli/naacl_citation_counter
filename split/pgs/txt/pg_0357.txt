Sign constraints on feature weights improve a joint model of word segmentation and phonology
Mark Johnson Macquarie University Sydney, Australia Mark Johnson@MQ.edu.au Robert Staubs University of Massachusetts, Amherst Amherst, MA, USA rstaubs@linguist.umass.edu Joe Pater University of Massachusetts, Amherst Amherst, MA, USA pater@linguist.umass.edu Emmanuel Dupoux ´ Ecole des Hautes Etudes en Sciences Sociales, ENS, CNRS, Paris, France emmanuel.dupoux@gmail.com
phones into sequences of words. This is an idealisation of the lexicon induction problem, since the resulting words are phonological forms for lexical entries. In its simplest form, the data for a word segmentation task is obtained by looking up the words of an orthographic transcript (of, say, child-directed speech) in a pronouncing dictionary and concatenating the results. However, this formulation significantly oversimplifies the problem because it assumes that each token of a word type is pronounced identically in the form specified by the pronouncing dictionary (usually its citation form). In reality there is usually a significant amount of pronunciation variation from token to token. The Buckeye corpus, on which we base our experiments here, contains manually-annotated surface phonetic representations of each word as well as the corresponding underlying form (Pitt et al., 2007). For example, a token of the word "lived" has the underlying form /l.ih.v.d/ and could have the surface form [l.ah.v] (we follow standard phonological convention by writing underlying forms with slashes and surface forms with square brackets, and use the Buckeye transcription format). There is a large body of work in the phonological literature on inferring phonological rules mapping underlying forms to their surface realisations. While most of this work assumes that the underlying forms are available to the inference procedure, there is work that induces underlying forms as well as the phonological processes that map them to sur-

Abstract
This paper describes a joint model of word segmentation and phonological alternations, which takes unsegmented utterances as input and infers word segmentations and underlying phonological representations. The model is a Maximum Entropy or log-linear model, which can express a probabilistic version of Optimality Theory (OT; Prince and Smolensky (2004)), a standard phonological framework. The features in our model are inspired by OT's Markedness and Faithfulness constraints. Following the OT principle that such features indicate "violations", we require their weights to be non-positive. We apply our model to a modified version of the Buckeye corpus (Pitt et al., 2007) in which the only phonological alternations are deletions of word-final /d/ and /t/ segments. The model sets a new state-ofthe-art for this corpus for word segmentation, identification of underlying forms, and identification of /d/ and /t/ deletions. We also show that the OT-inspired sign constraints on feature weights are crucial for accurate identification of deleted /d/s; without them our model posits approximately 10 times more deleted underlying /d/s than appear in the manually annotated data.

1

Introduction

This paper unifies two different strands of research on word segmentation and phonological rule induction. The word segmentation task is the task of segmenting utterances represented as sequences of 303

Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 303­313, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

