Brown clusters, for each i s.t. s  i < t: {[yj , brn(n, xi ), n]}n{2,4,8,12} , {[yj , ers,t (i), brn(n, xi ), n]}n{2,4,8,12} Word vectors, for each i s.t. s  i < t: {[yj , n] = w2v(n, xi )}300 n=1 , {[yj , ers,t (i), n] = w2v(n, xi )}300 n=1
Table 2: Word representation features in (s, t, yj , x). brn(n, xi ) maps a word xi to the first n bits of its Brown cluster bit sequence. w2v(n, xi ) maps xi to the nth component of its word vector, and [str] = v stands for a realvalued feature with name str and value v .

type-to-vector mapping can be learned with backpropagation. However, Mikolov et al. (2013) have shown that useful vector representations can be learned more efficiently by eschewing the languagemodeling objective. Their skip-gram model, which we adopt here, optimizes for each token, the likelihood of the tokens in a window surrounding it. This training process creates a linear classifier that predicts words conditioned on the central token's vector representation. The classifier and the word vectors are learned simultaneously, but once training is complete, the classifier is usually discarded, leaving only the vectors. These continuous representations project words into a low-dimensional space. Words that tend to have similar contexts, and therefore similar syntactic and semantic properties, will tend to be near one another in this space. We incorporate these representations into our NER system as real-valued features of each word xi , as shown in Table 2. 3.2 Data Weighting

Brown Clusters The Brown clustering algorithm assigns types to a deterministic, hierarchical clustering, which has been trained to optimize the likelihood of a firstorder, class-based language model (Brown et al., 1992). The clusters capture both syntactic and semantic regularities, and have been shown to perform well as unsupervised part-of-speech taggers (Blunsom and Cohn, 2011). The clusters are organized into a binary tree structure; therefore, each cluster can be represented as a bit string that encodes the branching decisions required to reach its leaf from the root. By truncating the bit string at different prefix lengths, one can access different granularities of clusters. Cluster membership can then be used to create indicators similar to the baseline's word identity features. This results in two feature templates, shown in Table 2.2 This technique has been previously applied to both newswire NER (Miller et al., 2004; Turian et al., 2010; Passos et al., 2014) and Twitter NER (Ritter et al., 2011; Plank et al., 2014). But previous work on Twitter NER has not directly tested the impact of Brown clusters; instead, they generally appear as part of an adapted baseline. Word Vectors An alternative word representation maps each word type deterministically to a low-dimensional continuous vector space. This technique was originally used as the bottom layer for continuous-space language models (Bengio et al., 2003), where the
2 We also experimented with templates over clusters and vectors for surrounding words, to no benefit.

Our next tool for domain adaptation is a small pool of in-domain, annotated data. The easiest way to make use of this data is to append it to our large pool of out-of-domain training data, which is what has been done in previous work on Twitter NER (Ritter et al., 2011; Plank et al., 2014). However, we have the strong intuition that greater weight should be placed on the in-domain data. Assume that for each training pair (x, y ) we also have an importance weight  . In our case, all outof-domain pairs will share one value for  , and all in-domain pairs will share another, higher  . We modify our PA learner to calculate  using a version of Equation 3 that replaces C with C . Unlike scaling  directly, scaling C has the desirable property of having even high- examples stop updating at precisely 0 loss, just as if we had duplicated that training example  times (Karampatziakis and Langford, 2010). If we view C as a regularization term, then this modification can also be interpreted as implementing example-specific regularization. Importance weights can also be incorporated into CRFs by modifying their training objective; however, this is not a standard feature of most CRF packages.

738

