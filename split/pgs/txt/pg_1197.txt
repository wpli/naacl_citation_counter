natural language. In particular the creation of socalled word embeddings has attracted a lot of attention in the past years, often by implementing neuralnetwork language models. Prominent examples include the works by Bengio et al. (2003) and Mikolov et al. (2013), with the skip-gram model of the latter providing a basis for the vector representations learned in our approach. Also closely related to I DEST are approaches which employ neural networks capable of handling word sequences of variable length. For example, Le & Mikolov (2014) extend the architectures of Mikolov et al. (2013) with artificial paragraph tokens, which accumulate the meaning of words appearing in the respective paragraphs. In contrast to these shallow methods, other approaches employ deep multi-layer networks for the processing of sentences. Examples include Kalchbrenner et al. (2014), who employ convolutional neural networks for analyzing the sentiment of sentences, and Socher et al. (2013), who present a special kind of recursive neural network utilizing tensors to model the semantics of a sentence in a compositional way, guided by the parse tree. A frequent issue with the deeper methods described above is the high computational complexity coming with the large numbers of parameters in a multi-layer neural network or in the value propagation in unfolded recursive neural networks. To circumvent this problem, our model is inspired by Mikolov's simpler skip-gram model, as described below.

ing that each word should be able to predict to some extent the other words in its context. A skip-gram architecture consists of: 1. An input layer, usually represented as a one-ofV or one-hot-spot layer. This layer type has as many input nodes as the vocabulary size. Each training example will activate exactly one input node corresponding to the current word wi , and all the other input nodes will be set to zero. 2. A first hidden layer, the embedding or projection layer, that will learn a distributed representation for each possible input word. 3. Zero or more additional hidden layers. 4. An output layer, expected to predict the words in the context of wi : wi-K , ..., wi-1 , wi+1 , ..., wi+K . In practice, when training based on this architecture, the network converges towards representing words that appear in similar contexts with vectors that are close to each other, as close vectors will produce a similar distribution of output labels in the network. 3.2 I DEST neural network Figure 3 shows the network architecture we use for training our paraphrase model in I DEST. In our case, the input vocabulary is the set of N unique event patterns extracted from text, and our supervision signal is the co-occurrence of event patterns in EECs. We set the input to be a one-hot-spot layer with a dimensionality of N , and for each pair of patterns that belong to the same EECs, we will have these patterns predict each other respectively, in two separate training examples. The output layer is also a oneof-V layer, because for each training example only one output node will be set to 1, corresponding to a co-occurring pattern. After training, if two patterns Pi and Pj have a large overlap in the set of entities they co-occur with, then they should be mapped onto similar internal representations. Note that the actual entities are only used for EEC construction, but they do not play a role in the training itself, thus allowing the network to generalize over specific entity instantiations. To exemplify, given the two EECs {"[Alex] married [Leslie]", "[Leslie] tied the knot with

3

Proposed model

Similarly to H EADY and N EWS S PIKE, our model is also based on the underlying assumption that if sentences from two news articles were published on the same day and mention the same entity set, then they are good paraphrase candidates. The main novelty is in the way we train the paraphrase model from the source data. We propose a new neural-network architecture which is able to learn meaningful distributed representations of full patterns. 3.1 Skip-gram neural network

The original Skip-gram architecture (Mikolov et al., 2013) is a feed-forward neural network that is trained on distributional input examples, by assum1143

