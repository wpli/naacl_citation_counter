VSM as a post-processing step that performs graph propagation on the structure of the ontology. The second is applicable to the wide range of current techniques that learn word embeddings from predictive models that maximize the likelihood of a corpus (Collobert and Weston, 2008; Mnih and Teh, 2012; Mikolov et al., 2013a). Our technique adds a latent variable representing the word sense to each token in the corpus, and uses EM to find parameters. Using a structured regularizer based on the ontological graph, we learn grounded sense-specific vectors. There are several reasons to prefer ontologies as distant sources of supervision for learning senseaware VSMs over previously proposed unsupervised context clustering techniques. Clustering approaches must often parametrize the number of clusters (senses), which is neither known a priori nor constant across words (Kilgarriff, 1997). Also the resulting vectors remain abstract and uninterpretable. With ontologies, interpretable sense vectors can be used in downstream applications such as WSD, or for better human error analysis. Moreover, clustering techniques operate on distributional similarity only whereas ontologies support other kinds of relationships between senses. Finally, the existence of cross-lingual ontologies would permit learning multi-lingual vectors, without compounded errors from word alignment and context clustering. We evaluate our methods on 3 lexical semantic tasks across 7 datasets and show that our sensespecific VSMs effectively integrate knowledge from the ontology with distributional statistics. Empirically, this results in consistently and significantly better performance over baselines in most cases. In the more marginal cases, analysis reveals that our performance is a result of the deficient structure of the ontology. We discuss and compare the two different approaches from the perspectives of performance, generalizability, flexibility and computational efficiency. Finally, we qualitatively analyze the vectors and show that they indeed capture sensespecific semantics.

ogy. We begin with notation. Let W = {w1 , ..., wn } be a set of word types, and Ws = {sij | wi  W , 1  j  ki } a set of senses, with ki the number of senses of wi . Moreover, let  = (T , E ) be an ontology represented by an undirected graph. The vertices T = {tij |  sij  Ws } correspond to the word senses in the set Ws , while the edges E = {er ij -i j } connect some subset of word sense pairs (sij , si j ) by semantic relation r1 . 2.1 Retrofitting Vectors to an Ontology

Our first technique assumes that we already have ^ = a vector space embedding of a vocabulary U {u ^i |  wi  W }. We wish to infer vectors V = {vij |  sij  Ws } for word senses that are maxi^ and , by some notion mally consistent with both U of consistency. We formalize this notion as MAP inference in a Markov network (MN). The MN we propose contains variables for every ^ and V . These variables are connected to vector in U one another by dependencies as follows. Variables for vectors vij and vi j are connected iff there exists an edge er ij -i j  E connecting their respective word senses in the ontology. Furthermore, vectors u ^i for the word types wi are each connected to all the vectors vij of the different senses sij of wi . If wi is not contained in the ontology, we assume it has a single unconnected sense and set it's only sense vector vi1 to it's empirical estimate u ^i . The structure of this MN is illustrated in Figure 1, where the neighborhood of the ambiguous word "bank" is presented as a factor graph. We set each pairwise clique potential to be of the form exp(a u - v 2 ) between neighboring nodes. Here u and v are the vectors corresponding to these nodes, and a is a weight controlling the strength of the relation between them. We use the Euclidean norm instead of a distance based on cosine similarity because it is more convenient from an optimization perspective. Our inference problem is to find the MAP esti^ , which may be stated mate of the vectors V , given U
For example there might be a "synonym" edge between the word senses "cat(1)" and "feline(1)".
1

2

Unified Symbolic and Distributional Semantics

In this section, we present our two techniques for inferring sense-specific vectors grounded in an ontol684

