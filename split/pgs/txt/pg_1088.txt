tags for each word in the 20-best list is 1.1. The joint tagging and parsing approach of Wang and Xue (2014) improves the F1 score from 80.1% to 83.6% (see lines 4 and 5). We instead use sausage lattices, a much cheaper way. The non-DP (1-best POS) and non-DP (20-best POS) lines show the effectiveness of using sausage lattices (+1.1 for tagging and +2.6 for parsing). As Wang and Xue (2014) is a non-DP model, it is comparable to our non-DP results. With the help of 20-best tagging lattices, we achieve the same tagging accuracy at 95.5%, but still 0.4 worse on the F1 score than the joint model. It suggests that we need a larger k to catch up the gap. But our DP model boosts the performance further to the best score at 83.9% with a similar set of features. The last two lines (non-DP and DP) in Table 2 show our English lattice parsing results. So we run another baseline with the non-DP English parser on 1-best POS tags, and the baseline achieves a tagging accuracy at 97.11 and an F1 score at 90.1. Comparing to the tagging accuracy (97.15) and F1 score (90.3) of our non-DP lattice parser, sausage lattice parsing doesn't help the tagging accuracy, but helps parsing a little by 0.2 points. The statistics show that 2 percent of POS tags in the lattice parsing result are different from the baseline, and those differences lead to a slight improvement on parsing.

References
Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of CoNLL 2008. Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL. Shay B. Cohen, Carlos G´ omez-Rodr´ iguez, and Giorgio Satta. 2011. Exact inference for generative probabilistic non-projective dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of ACL. Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania. Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML, pages 175­182. Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun'ichi Tsujii. 2011. Incremental joint pos tagging and dependency parsing in chinese. In IJCNLP. Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL 2010. Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of NAACL. Marco Kuhlmann, Carlos Gmez-Rodrguez, and Giorgio Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In Proceedings of ACL. Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for chinese pos tagging and dependency parsing. In Proceedings of EMNLP, pages 1180­1191. Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Incremental Parsing: Bringing Engineering and Cognition Together. Workshop at ACL-2004, Barcelona. Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL. Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of EMNLP, pages 1­10. Kenji Sagae and Alon Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proceedings of ACL (poster).

6

Conclusions

In this paper, we present a dynamic programming algorithm based on graph-structured stack (GSS) for shift-reduce constituency parsing, and extend the algorithm to take tagging sausage lattices as input. Experiments on both English and Chinese treebanks show that our DP parser outperforms almost all other parsers except of Carreras et al. (2008), which runs in a much higher time complexity.

Acknowledgment
We thank the anonymous reviewers for comments. Haitao Mi is supported by DARPA HR0011-12C-0015 (BOLT), and Liang Huang is supported by DARPA FA8750-13-2-0041 (DEFT), NSF IIS1449278, and a Google Faculty Research Award. The views and findings in this paper are those of the authors and are not endorsed by the DARPA.

1034

