G(x) is a matrix formed by appending the training source vectors in the neighborhood of x. More specifically, G(x) is a matrix that has the vectors xi s.t. i  g (x) as its rows. Similarly, H (x) is a matrix that has the vectors yi s.t. i  g (x) as its rows (in the corresponding order). W (x) is a matrix that has the weights w(x, xi ) s.t. i  g (x) along its diagonal (also in the corresponding order). This approach is much faster than the exhaustive method, but at typical audio sampling frequencies, inferring the transformation of the signal for an entire sentence can take over 30 seconds on a modern CPU, which is too slow for real-time conversion. In order to transform the signal for a new sentence, ^(x) must be computed for each frame. This means A that for each frame x, the distance to all training source frames must be computed, neighborhood matrices G(x) and H (x) must be formed, followed by several matrix multiplies, an LU decomposition, and a triangular solve operation. 3.2 Inference on the GPU ^(x) for a block of multiple inThe computation of A put frames can be done in parallel if a fixed amount of lag is tolerated in the conversion process. The parallel computation of large number of small dense matrix operations (multiply, LU decomposition, triangular solve) is a perfect fit for implementation a GPU, which can achieve vastly more throughput than modern CPUs can. However, using the CPU's neighborhood structure on the GPU has a crippling bottleneck. The extraction of the neighborhood matrices G(x) and H (x) from the training data requires a large number of memory accesses that are effectively random. The indices of the closest K training source vectors to an input x are generally noncontiguous. As a result, the K vectors in each of the neighborhood matrices must be copied with separate memory accesses, and since random access time on modern GPUs is very slow, extraction becomes a bottleneck. In initial experiments, we found that when this approach is implemented on a GPU it is even slower than the CPU-based implementation. In contrast, memory bandwidth is extremely high on modern GPUs. Thus, if it were possible to order the training vectors xi and yi in GPU memory such that neighborhood matrices G(x) and H (x) were composed of contiguous blocks of training vectors, 1336

GPU surrogate neighborhood g ~(x)

first principal direction

x2
CPU neighborhood g ( x)

input frame x

x1
Figure 1: Depiction of standard neighborhood function g (x) used for local regression on the CPU and surrogate neighborhood function g ~(x) used for inference on the GPU, plotted for two-dimensional input data.

extraction on the GPU could be made very efficient. Unfortunately, with the current definition of the neighborhood function, g , such an ordering does not exist. Therefore, we define a new GPU-friendly neighborhood function, g ~, for which an ordering that permits contiguous extraction does exist. Let u(x) be the projection of source vector x onto the first principal component resulting from running PCA on the source side of the training data. Now, we define g ~ as follows: g ~(x) is the set of K indices for which |u(x) - u(xi )| is the smallest, or, in other words, the set of training indices with source projections closest to the projection of the input frame. By ordering the training data by their projection onto the first principal component, we can ensure that G(x) and H (x) are contiguous in memory. The hope is that this approach yields substantial speedups on the GPU without negatively impacting the learned transformation (see Section 4). Figure 1 depicts the difference between the CPU neighborhood function g and the GPU function g ~. Intuitively, when most of the variance in the training data occurs along the first principal direction, the CPU and GPU neighborhood functions may be similar since the distance between projections is a good proxy for distance in the original space. The weighting function, w, is still computed in the original space, so distant training vectors that are inadvertently included in the neighborhood will be severely down-weighted. The potential pitfall is that for a fixed neighborhood size, g ~ may be less efficient at collecting training points that are relevant to the input.

