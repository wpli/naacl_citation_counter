Table 2: A randomly chosen report and its predicted sources, per LDA-Bayes, illustrating that a report and predicted source may be contextually similar but that their titles may have few words in common.
Position Report: Japanese Dependency Structure Analysis Based On Support Vector Machines (2000) Cited Year Source Name Source? 1996 A Maximum Entropy Approach To Natural Language Processing 1 Natural Language Processing 1993 Building A Large Annotated Corpus 2 Of English: The Penn Treebank 3 1996 A Maximum Entropy Model For Part-Of-Speech Tagging 1994 A Syntactic Analysis Method Of Long Japanese 4 Sentences Based On The Detection Of Conjunctive Structures 5 1992 Class-Based N-Gram Models Of Natural Language ... ... ... 1996 Three New Probabilistic Models For 11 Dependency Parsing: An Exploration 2000 Introduction To The CoNLL-2000 12 Shared Task: Chunking 13 1995 A Model-Theoretic Coreference Scoring Scheme 1988 A Stochastic Parts Program And Noun 14 Phrase Parser For Unrestricted Text 1999 Japanese Dependency Structure Analysis 15 Based On Maximum Entropy Models

similar across most topics and (2) the relative importance of each topic. For all of our experiments (including LDA-Bayes) we used 125 topics to model the corpus; thus, this feature becomes expanded to 125 individual indices within our vector, which is why we name this system Logit-Expanded. Namely, i  K, let feature fi = |ri - si |. 3.2.2 Meta-data Features

 Report Author Previously Cited Source?: We believe authors have a tendency to cite documents they have cited in the past  Report Author Previously Cited a Source Author?: Authors also have a tendency to "subscribe" to certain authors and are more familiar with particular people's works, and thus cite those papers more often.  Prior Citation Probability: A distinguishing feature of our LDA-Bayes model is that it factors in the prior probability of a source being cited, based on the maximum likelihood estimate from the training data. So, we explicitly include this as a feature.  Number of Overlapping Authors: Authors have a tendency to cite their co-authors, in part because their co-authors' past work has an increased chance of being relevant.  Number of Years between Report and Source: Authors tend to cite more recent papers.  Title Similarity between Report and Source: As shown in Table 2, some sources erroneously returned by our baseline system could have been discarded had we judged them by how dissimilar their titles are from the report's title. In Table 2's example, the one correct source to find (within 12,000) was returned at position 15 and has many words in common with the report (namely, "Japanese Dependency Structure Analysis Based On" appears in the titles of both the report and correctly predicted source).

criminative classifiers to learn important features for classification, we use logistic regression with a linear kernel. Specifically, we train using L2regularization, which during test time allows us to get a probability estimate for each queried vector (i.e., a report-source pair). The details of the training and testing data are provided in Section 4.2. However, it is important to understand that each training and testing instance corresponds to a distinct report-source document pair and is represented as a single fixed-length vector. The vector is comprised of the following features, which our experiments illustrate are useful for determining if there exists a link between the associated report and source: 3.2.1 Topic/Content-Based Features  LDA-Bayes: Our baseline system showed strong results by itself, so we include its predictions as a feature (that is, P (s|r)).  Topics: LDA-Bayes ranks report-source pairs by marginalizing over all topics (see Equation 2); however, we assert that not all topics are equally important. Allowing each topic to be represented as its own feature, while keeping the value based on the report-source's relationship for that topic (i.e., the absolute value of the difference), can potentially allow the logistic regression to learn both (1) the importance for report-source pairs to be generally 78

