product with the gate value, and the weight matrices denoted by Wij are the trained parameters. 3.2 CNN-LSTMs for video description We use a two layer LSTM model for generating descriptions for videos based on experiments by Donahue et al. (2014) which suggest two LSTM layers are better than four and a single layer for image to text tasks. We employ the LSTM to "decode" a visual feature vector representing the video to generate textual output. The first step in this process is to generate a fixed-length visual input that effectively summarizes a short video. For this we use a CNN, specifically the publicly available Caffe (Jia et al., 2014) reference model, a minor variant of AlexNet (Krizhevsky et al., 2012). The net is pre-trained on the 1.2M image ILSVRC-2012 object classification subset of the ImageNet dataset (Russakovsky et al., 2014) and hence provides a robust initialization for recognizing objects and thereby expedites training. We sample frames in the video (1 in every 10 frames) and extract the output of the fc7 layer and perform a mean pooling over the frames to generate a single 4,096 dimension vector for each video. The resulting visual feature vector forms the input to the first LSTM layer. We stack another LSTM layer on top as in Figure 2, and the hidden state of the LSTM in the first layer is the input to the LSTM unit in the second layer. A word from the sentence forms the target of the output LSTM unit. In this work, we represent words using "one-hot" vectors (i.e 1-of-N coding, where is N is the vocabulary size). Training and Inference: The two-layer LSTM model is trained to predict the next word Swt in the sentence given the visual features and the previous t - 1 words, p(Swt |V, Sw1 , . . . , Swt-1 ). During training the visual feature, sentence pair (V, S ) is provided to the model, which then optimizes the log-likelihood (Equation 1) over the entire training dataset using stochastic gradient descent. At each time step, the input xt is fed to the LSTM along with the previous time step's hidden state ht-1 and the LSTM emits the next hidden state vector ht (and a word). For the first layer of the LSTM xt is the concatenation of the visual feature vector and the previous encoded word (Swt-1 , the ground truth word during training and the predicted word during test

time). For the second layer of the LSTM xt is zt of the first layer. Accordingly, inference must also be performed sequentially in the order h1 = fW (x1 , 0), h2 = fW (x2 , h1 ), until the model emits the endof-sentence (EOS) token at the final step T . In our model the output (ht = zt ) of the second layer LSTM unit is used to obtain the emitted word. We apply the Softmax function, to get a probability distribution over the words w in the vocabulary D. p(w|zt ) = exp(Ww zt ) w D exp(Ww zt ) (10)

where Ww is a learnt embedding vector for word w. At test time, we choose the word w ^ with the maximum probability for each time step t until we obtain the EOS token. 3.3 Transfer Learning from Captioned Images Since the training data available for video description is quite limited (described in Section 4.1), we also leverage much larger datasets available for image captioning to train our LSTM model and then fine tune it on the video dataset. Our LSTM model for images is the same as the one described above for single video frames (in Section 3.1, and 3.2). As with videos, we extract fc7 layer features (4096 dimensional vector) from the network (Section 3.2) for the images. This forms the visual feature that is input to the 2-layer LSTM description model. The vocabulary is the combined set of words in the video and image datasets. After the model is trained on the image dataset, we use the weights of the trained model to initialize the LSTM model for the video description task. Additionally, we reduce the learning rate on our LSTM model to allow it to tune to the video dataset. This speeds up training and allows exploiting knowledge previously learned for image description.

4
4.1

Experiments
Datasets

Video dataset. We perform all our experiments on the Microsoft Research Video Description Corpus (Chen and Dolan, 2011). This video corpus is a collection of 1970 YouTube snippets. The duration of each clip is between 10 seconds to 25 seconds, typically depicting a single activity or a short

1498

