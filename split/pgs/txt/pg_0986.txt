Penalized Expectation Propagation for Graphical Models over Strings
Ryan Cotterell and Jason Eisner Department of Computer Science, Johns Hopkins University {ryan.cotterell,jason}@cs.jhu.edu Abstract
We present penalized expectation propagation (PEP), a novel algorithm for approximate inference in graphical models. Expectation propagation is a variant of loopy belief propagation that keeps messages tractable by projecting them back into a given family of functions. Our extension, PEP, uses a structuredsparsity penalty to encourage simple messages, thus balancing speed and accuracy. We specifically show how to instantiate PEP in the case of string-valued random variables, where we adaptively approximate finite-state distributions by variable-order n-gram models. On phonological inference problems, we obtain substantial speedup over previous related algorithms with no significant loss in accuracy.

1

Introduction

Graphical models are well-suited to reasoning about linguistic structure in the presence of uncertainty. Such models typically use discrete random variables, where each variable ranges over a finite set of values such as words or tags. But a variable can also be allowed to range over an infinite space of discrete structures--in particular, the set of all strings, a case first explored by Bouchard-Côté et al. (2007). This setting arises because human languages make use of many word forms. These strings are systematically related in their spellings due to linguistic processes such as morphology, phonology, abbreviation, copying error and historical change. To analyze or predict novel strings, we can model the joint distribution of many related strings at once. Under a graphical model, the joint probability of an assignment tuple is modeled as a product of potentials on sub-tuples, each of which is usually modeled in turn by a weighted finite-state machine. In general, we wish to infer the values of unknown strings in the graphical model. Deterministic
This material is based upon work supported by the National Science Foundation under Grant No. 1423276, and by a Fulbright Research Scholarship to the first author.


approaches to this problem have focused on belief propagation (BP), a message-passing algorithm that is exact on acyclic graphical models and approximate on cyclic ("loopy") ones (Murphy et al., 1999). But in both cases, further heuristic approximations of the BP messages are generally used for speed. In this paper, we develop a more principled and flexible way to approximate the messages, using variable-order n-gram models. We first develop a version of expectation propagation (EP) for string-valued variables. EP offers a principled way to approximate BP messages by distributions from a fixed family--e.g., by trigram models. Each message update is found by minimizing a certain KL-divergence (Minka, 2001a). Second, we generalize to variable-order models. To do this, we augment EP's minimization problem with a novel penalty term that keeps the number of n-grams finite. In general, we advocate penalizing more "complex" messages (in our setting, large finite-state acceptors). Complex messages are slower to construct, and slower to use in later steps. Our penalty term is formally similar to regularizers that encourage structured sparsity (Bach et al., 2011; Martins et al., 2011). Like a regularizer, it lets us use a more expressive family of distributions, secure in the knowledge that we will use only as many of the parameters as we really need for a "pretty good" fit. But why avoid using more parameters? Regularization seeks better generalization by not overfitting the model to the data. By contrast, we already have a model and are merely doing inference. We seek better runtime by not over-fussing about capturing the model's marginal distributions. Our "penalized EP" (PEP) inference strategy is applicable to any graphical model with complex messages. In this paper, we focus on strings, and show how PEP speeds up inference on the computational phonology model of Cotterell et al. (2015). We provide further details, tutorial material, and results in the appendices (supplementary material).

932
Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 932­942, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

