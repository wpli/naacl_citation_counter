rithms (Yarowsky, 1995) have been shown to be successful at bootstrapping (Collins and Singer, 1999). Co-training (Blum and Mitchell, 1998) and its boostrapped adaptation (Collins and Singer, 1999) require disjoint views of the features of the data. Whitney and Sarkar (2012) proposed a modified Yarowsky algorithm that used label propagation on graphs, inspired by Subramanya et al. (2010) algorithm that used a large labeled data for domain adaptation. In this paper, we use the setting of bootstrapped pattern-based entity extraction (Riloff, 1996; Thelen and Riloff, 2002). This can be viewed as a form of the Yarowsky algorithm, with pattern learning as an additional step. Pattern based approaches have been widely used for IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005). Patterns are useful in two ways: they are good features, and they identify promising candidate entities. Recently, Gupta and Manning (2014) improved pattern scoring (Step 2 in Algorithm 1) using predicted labels of unlabeled entities. For entity scoring (Step 3), they used an average of feature values to predict the scores. We use the same framework but focus on improving the entity classifiers. In most IE systems, including ours, word classes or word vectors are used as features in a classifier (Haghighi and Klein, 2006; Ratinov and Roth, 2009). To the best of our knowledge, our work is the first to use distributed representations of words to improve a bootstrapped system by expanding the training set.

sifier (Step 3) by expanding its training data using distributed vector representations of words. Algorithm 1 Bootstrapped Pattern-based Entity Extraction Given: Text D, labels L, seed entities El l  L while not-terminating-condition (e.g. precision is high) do for l  L do 1. Label D with El 2. Create patterns around labeled entities. Learn good patterns and use them to extract candidate entities Cl . 3. Learn an entity classifier and classify Cl . Add new classified entities to El . Labeling known entities: The text is labeled using the label dictionaries, starting with the seed dictionaries in the first iteration. Creating and Learning Patterns: Patterns are then created using the context around the labeled entities to create candidate patterns for label l. Candidate patterns are scored using a pattern scoring measure and the top ones are added to the list of learned patterns for label l. In our experiments, we use a widely used pattern scoring measure, RlogF (Riloff, 1996; Thelen and Riloff, 2002). Top ranked patterns with scores above a certain threshold are used to extract candidate entities Cl from text. Learning entities: An entity classifier predicts the labels of Cl and adds the newly classified entities to label l's dictionary, El . We discard common words, negative entities, and those containing nonalphanumeric characters from the set. Entity Classifier We build a one-vs-all entity classifier using logistic regression. In each iteration, for label l, the entity classifier is trained by treating l's dictionary entities (seed and learned in previous iterations) as positive and entities belonging to all other labels as negative. To improve generalization, we also sample the unlabeled entities that are not function words as negative. To train with a balanced dataset, we randomly sub-sample the negatives such that the number of negative instances is equal to the number of positive instances. The features for the entities are similar to Gupta and Manning (2014): edit distances from positive and negative entities, relative frequency of the entity words

3

Background

In a bootstrapped pattern-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific label from unlabeled text (Riloff, 1996; Collins and Singer, 1999) using patterns, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). We use lexicosyntactic surface word patterns to extract entities from unlabeled text starting with seed dictionaries for multiple classes. Algorithm 1 gives an overview. In this paper, we focus on improving the entity clas-

1216

