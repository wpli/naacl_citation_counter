2008 ILP Baseline TAC Best w/o novelty features w all features w/o novelty features w all features w/o novelty features w all features

2009

2010 2011 7.04 8.00 7.39 7.76 7.97 8.11 7.41 7.70 8.63 9.58 9.20 9.46 9.70 9.99 9.18 9.43

8.55 8.84 10.10 10.41 Supervised ILP 9.18 9.4 9.65 9.99 9.25 9.42 9.06 9.28 9.47 9.61 9.10 9.32

lems is our future work.
10.5 TAC 2008 TAC 2009 TAC 2010 TAC 2011

10

9.5 ROUGE-2

2-step: Supervised ILP + Sentence Ranking

9

8.5

8

Sentence Ranking w/o ILP

7.5 150

200

250

300

350

400

Summary Length of ILP Output

Table 2: ROUGE-2 results on TAC 2008-2011 data.
2008 ILP Baseline NIST Best 2009 2010 2011

Figure 1: ROUGE-2 results when varying the output length for the first ILP selection step.

12.17 12.54 10.57 12.01 13.66 13.95 11.97 13.08 Supervised ILP

4

Conclusions

w/o novelty features 12.57 12.94 11.01 12.76 w all features 12.78 13.21 11.61 12.95 2-step: Supervised ILP + Sentence Ranking w/o novelty features 13.10 13.65 11.98 13.24 w all features 13.61 13.77 12.20 13.42 Sentence Ranking w/o ILP w/o novelty features 12.60 12.99 11.25 12.73 w all features 12.85 13.31 11.50 12.90

Table 3: ROUGE-SU4 results on TAC 2008-2011 data.

In this paper, we adopt the supervised ILP framework for the update summarization task. A set of rich features are used to measure the importance and novelty of the bigram concepts used in the ILP model. In addition, we proposed a re-selection component to rank candidate sentences generated by the ILP model based on sentence level features. Our experiment results show that our features and the reranking procedure both help improve the summarization performance. This pilot research points out new directions for generic or update summarization based on the ILP framework.

for all the methods, adding the novelty related features always performs better than that without them, proving the effect of our novelty features for update summarization. Lastly we evaluate the effect of the summary length from the ILP module on the two-step summarization systems. Figure 1 shows the performance when N changes from 150 to 400. We can see that there is some difference in the patterns for different data sets, and the best results are obtained when N is around 150 to 250. When the first ILP module produces many sentence candidates, it is likely that there is redundancy among them. In this case, redundancy removal approaches such as MMR need to be used to generate the final summary. In addition, for a large candidate set, our current regression model also faces some challenges due to its limited features used in sentence reranking. Addressing these prob-

Acknowledgments
We thank the anonymous reviewers for their detailed and insightful comments on earlier drafts of this paper. The work is partially supported by NSF award IIS-0845484 and DARPA Contract No. FA8750-132-0041. Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views of the funding agencies.

References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of ACL. Florian Boudin, Marc El-B` eze, and Juan-Manuel TorresMoreno. 2008. The LIA update summarization systems at tac-2008. In Proceedings of TAC.

1321

