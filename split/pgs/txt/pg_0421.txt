come concepts while function words either become relations or get omitted if they do not contribute to the meaning of a sentence. This is illustrated in Figure 1, where `the' and `to' in the dependency tree are omitted from the AMR and the preposition `in' becomes a relation of type location. In AMR, reentrancy is also used to represent co-reference, but this only happens in some limited contexts. In Figure 1, `police' is both an argument of `arrest' and `want' as the result of a control structure. This suggests that it is possible to transform a dependency tree into an AMR with a limited number of actions and learn a model to determine which action to take given pairs of aligned dependency trees and AMRs as training data. This is the approach we adopt in the present work, and we present a transition-based framework in which we parse a sentence into an AMR by taking the dependency tree of that sentence as input and transforming it to an AMR representation via a series of actions. This means that a sentence is parsed into an AMR in two steps. In the first step the sentence is parsed into a dependency tree with a dependency parser, and in the second step the dependency tree is transformed into an AMR graph. One advantage of this approach is that the dependency parser does not have to be trained on the same data set as the dependency to AMR transducer. This allows us to use more accurate dependency parsers trained on data sets much larger than the AMR Annotation Corpus and have a more advantageous starting point. Our experiments show that this approach is very effective and yields an improvement of 5% absolute over the previously reported best result (Flanigan et al., 2014) in F-score, as measure by the Smatch metric (Cai and Knight, 2013). The rest of the paper is as follows. In §2, we describe how we align the word tokens in a sentence with its AMR to create a span graph based on which we extract contextual information as features and perform actions. In §3, we present our transition-based parsing algorithm and describe the actions used to transform the dependency tree of a sentence into an AMR. In §4, we present the learning algorithm and the features we extract to train the transition model. In §5, we present experimental results. §6 describes related work, and we conclude in §7.

2

Graph Representation

Unlike the dependency structure of a sentence where each word token is a node in the dependency tree and there is an inherent alignment between the word tokens in the sentence and the nodes in the dependency tree, AMR is an abstract representation where the word order of the corresponding sentence is not maintained. In addition, some words become abstract concepts or relations while other words are simply deleted because they do not contribute to meaning. The alignment between the word tokens and the concepts is non-trivial, but in order to learn the transition from a dependency tree to an AMR graph, we have to first establish the alignment between the word tokens in the sentence and the concepts in the AMR. We use the aligner that comes with JAMR (Flanigan et al., 2014) to produce this alignment. The JAMR aligner attempts to greedily align every concept or graph fragment in the AMR graph with a contiguous word token sequence in the sentence.
want-01 ARG0 police ARG0 ARG1 arrest-01 ARG1 person name name op1 "Micheal" op2 "Karras" s6,8 : person+name s2,3 :police ARG1 s3,4 :want-01 ARG1 ARG0 ARG0 s5,6 :arrest-01 s0,1 :ROOT

(a) AMR graph

(b) Span graph

Figure 2: AMR graph and its span graph for the sentence, "The police want to arrest Micheal Karras." We use a data structure called span graph to represent an AMR graph that is aligned with the word tokens in a sentence. For each sentence w = w0 , w1 , . . . , wn , where token w0 is a special root symbol, a span graph is a directed, labeled graph G = (V, A), where V = {si,j |i, j  (0, n) and j > i} is a set of nodes, and A  V × V is a set of arcs. Each node si,j of G corresponds to a continuous span (wi , . . . , wj -1 ) in sentence w and is indexed by the starting position i. Each node is assigned a concept label from a set LV of concept labels and each arc is assigned a relation label from a set LA

367

