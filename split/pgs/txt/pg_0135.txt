citation-probability that plays a role in LDA-Bayes. Supporting this idea, we see the biggest performance increase when we pair LDA-Bayes with the PrevCitedSource feature ­ a non-topic-based feature, which provides the system with a different type of data to leverage.
Table 5: Analysis of each feature used in LogitExpanded. Results based on the first 50 sources returned, averaged over all reports. Our Starting Point* system listed within the "Addage" columns used LDA-Bayes as the only feature. Our Starting Point* system within the "Removal" columns used every feature. Figure 4: Recall vs Precision Performance across all Reports from the full ACL Anthology. Logit-Expanded's slight blips at recall = 0.25, 0.33, and 0.5 is due to the truth set having many reports with only 4, 3, or 2 golden sources, respectively.
Starting Point* LDA-Bayes Topics Expanded PrevCitedSource PrevCitedAuthor # Shared Authors Prior Prob. Cited Title Similarity # Years Between recall .496 .564 .581 .484 .543 .501 .513 .498 Addage precision .012 .014 .014 .012 .013 .012 .012 .012 fscore .024 .027 .028 .023 .026 .023 .023 .023 recall .647 .583 .606 .599 .641 .636 .639 .623 .645 Removal precision .016 .014 .015 .014 .016 .015 .015 .015 .016 fscore .031 .028 .028 .028 .030 .029 .030 .029 .030

is in column addage. Table 5 reveals insightful results: it is clear that LDA-Bayes is a strong baseline and useful feature to include in our system, for removing it from our feature list causes performance to decrease more than removing any other feature. PrevCitedSource and Topics Expanded are the second and third strongest features, respectively. We suspect that PrevCitedSource was a good feature because our corpus was sufficiently large; had our corpus been much smaller, there might not have been enough data for this feature to provide any benefit. Next, Title Similarity and # Shared Authors were comparably good features. PrevCitedAuthor and # Years Between were the worst features, as we see negligible performance difference when we (1) pair either with LDABayes, or (2) remove either from our full feature list. An explanation for the former feature's poor performance could be that authors vary in (1) how often they repeatedly cite authors, and most likely (2) many authors have small publication histories within training, so it might be unwise to base prediction on this limited information. Last, it is worth noting that when we pair Topics Expanded with LDABayes, that alone is not enough to give the best performance from a pair. An explanation is that it dominates the system with too much content-based (i.e., topic) information, overshadowing the prior81

Additionally, when using only the metadata features (i.e., not LDA-Bayes or Topics-Expanded), performance for returning 50 sources averaged 0.403, 0.010, and 0.019 for recall, precision, and fscore, respectively ­ demonstrating that the metadata features alone do not yield strong results but that they complement the LDA-Bayes and TopicsExpanded features. 4.3.2 Topic Importance Although Report-to-Source citation prediction was our primary objective, our feature representation of topics allows logistic regression to appropriately learn which topics are most useful for predicting citations. In turn, these topics are arguably the most cohesive; thus, our system, as a byproduct, provides a metric for measuring the "quality" of each topic. Namely, the weight associated with each topic feature indicates the topic's importance ­ the lower the weight the better. Table 6 shows our system's ranking of the most important topics, signified by "Logit-weight." We did not prompt humans to evaluate the quality of the topics, so in attempt to offer a comparison, we also rank each topic according to two popular metrics: Pointwise Mutual Information (PMI) and Topic Coherence (TC) (Mimno et al., 2011). For a topic k ,

