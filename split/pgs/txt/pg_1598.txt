(parameter). To avoid having to learn a model with tens of millions of parameters, we impose a percept cutoff during learning: only zero-order percepts that are active at least 5 times in the training data (with any tag) are retained in the model (with features for all tags). There is no minimum threshold for first-order percepts.17 The resulting models are of a manageable size: about 4 million parameters with the full tagset. 4.6 Experimental Setup Our setup mostly echoes that of Schneider et al. (2014a). We adopt their train (3312 sentences/ 48k words) vs. test (500 sentences/7k words) split, and tune hyperparameters by 8-fold cross-validation on train. By this procedure we chose a percept cutoff of 5 to use throughout, and tuned the number of training iterations for each experimental condition (early stopping within each cross-validation fold so as to greedily maximize tagging accuracy on the held-out portion, and averaging the best number of iterations across folds). For simplicity, we use oracle POS tags in our experiments and do not use Schneider et al.'s (2014a) recall-oriented cost function. Experiments were managed with Jonathan Clark's ducttape tool.18 4.7 Results Table 2 shows full supersense tagging results, separating the MWE identification performance (measured by link-based precision, recall, and F1 ; see Schneider et al., 2014a) from the precision, recall, and F1 of class labels on the first token of each expression (segments with no class label are ignored).19 Exact tagging accuracy (last column) is higher because it rewards true negatives, i.e. single-word segments with no nominal or verbal class label (the O and o tags). Tag space. The sequence tagging framework makes it simple to model MWE identification jointly with supersense tagging: this is accomplished by packing information about both kinds of output into
17 Zero-order percepts are percepts which are to be conjoined with only the present tag to form zero-order features. First-order percepts are to be conjoined with the present and previous tags. 18 19

the tags. But there is always a risk that a larger tag space will impair the model's ability to generalize. By comparing the first two rows of the results, we can see that jointly modeling supersenses along with multiword expressions results in only a minor decrease (<2 F1 points) in MWE identification performance under the most basic feature set. Further, we see that most of that decrease is recovered with richer features. Thus, we conclude that it is empirically reasonable to model these phenomena together. Runtime. Our final system (146 tags; last row of table 2) tags 140 words (10 sentences) per second. Features. Comparing the bottom three rows in the table indicates that features that generalize beyond lexical items lead to better supersense labeling. The best model has access to supersense information in the WordNet lexicon; it is 4 F1 points better at choosing the correct class label than its nearest competitor, which relies on word clusters to abstract away from individual lexical items. Nouns, verbs, and auxiliaries all see improvements. We also inspect the learned parameters. The highest-weighted parameters suggest that the best model relies heavily on the supersense lookup features, whereas the second-best model--lacking those--in large part relies on Brown clusters (cf. Grave et al., 2013). The auxiliary verb vs. main verb feature in the best model is highly weighted as well, helping to distinguish between `a and V: STATIVE. Polysemy. We have motivated the task of supersense tagging in part as a coarse form of word sense disambiguation. Therefore, it is worth investigating how well the learned model manages to choose the correct supersense for nouns and verbs that are ambiguous in the data. A handful of lemmas in test have at least two different supersenses predicted several times; an examination of four such lemmas in table 3 shows that for three of them the tagging accuracy exceeds the majority baseline. In the case of look, the model is usually able to distinguish between V: COGNITION (as in looking for a company with decent rates) and V: PERCEPTION (as in sometimes the broccoli looks browned around the edges). Out-of-domain baseline. To assess the importance of in-domain data for learning, we used a SemCor-trained supersense tagger--a reimplemen-

https://github.com/jhclark/ducttape/

We count the class label only once for MWEs--otherwise this measure would be strongly dependent on segmentation performance. However, the MWE predictions do have an effect when the prediction and gold standard disagree on which token begins a strong nominal or verbal expression.

1544

