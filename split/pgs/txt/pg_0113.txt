stack and the next four words in the input buffer. Complete details of the feature set can be found in their paper. For our own model, RevInc, in addition to these features used for NonInc, we also provide features based on the right periphery of top node in the stack. For nodes in the right periphery, we provide uni-gram and bi-gram features based on the node's CCG category. For example, if S0 is the node on the top of the stack, B1 is the bottom most node in the right periphery, and c represent the node's CCG category, then B1c, and B1cS0c are the uni-gram and bi-gram features respectively. Unlike Z&C, we do not use a beam for our experiments, although we use a beam of 16 for comparison of our results with their parser. The latter gives competitive results with the state-of-theart CCG parsers. Z&C and Xu et al. (2014), use C&C's generate script and unification mechanism respectively to extract dependencies for evaluation. C&C's grammar doesn't cover all the lexical categories and binary rules in the CCGbank. To avoid this, we adapted Hockenmaier's scripts used for extracting dependencies from the CCGbank derivations. These are the two major divergences in our re-implementation from Z&C. 4.1 Data and Settings We use standard CCGbank training (sections 02 - 21), development (section 00) and testing (section 23) splits for our experiments. All sentences in the training set are used to train NonInc. But for RevInc, we used 98% of the training set (the coverage of our algorithm). We use automatic POS-tags and lexical CCG categories assigned using the C&C POS tagger and supertagger respectively for development and test data. For training data, these tags are assigned using ten-way jackknifing. Also, for lexical CCG categories, we use a multitagger which assigns k-best supertags to a word rather than 1-best supertagging (Clark and Curran, 2004). The number of supertags assigned to a word depends on a  parameter. Unlike Z&C, the default value of  gave us better results rather than decreasing the value. Not using a beam could be the reason for this. Following Z&C and Xu et al. (2014), during training, we also provide the gold CCG lexical category to the list of CCG lexical categories for a word if it is not assigned by the supertagger.

4.2 Connectedness and Waiting Time Before evaluating the performance of our algorithm, we introduce two measures of incrementality: connectedness and waiting time. In a shift-reduce parser, a derivation is fully connected when all the nodes in the stack are connected leading to only one node in the stack at any point of time. We measure the average number of nodes in the stack before shifting a new token from input buffer to the stack, which we call as connectedness. For a fully connected incremental parser like Hassan et al. (2009), connectedness would be one. As our RevInc algorithm is not fully connected, this number will be greater than one. For example, in a noun phrase `the big book', when `the' and `big' are in the stack, as there is no dependency between these two words, our algorithm doesn't combine these two nodes resulting in having two nodes in the stack3 . Second column in Table 1 gives this number for both NonInc and RevInc algorithms. Though our algorithm is not fully connected, connectedness of our algorithm is significantly lower than the NonInc algorithm as our algorithm is more incremental.
Algorithm NonInc RevInc Connectedness 4.62 2.15 Waiting Time 2.98 0.69

Table 1: Connectedness and waiting time. We define waiting time as the number of nodes that need to be shifted to the stack before a dependency between any two nodes in the stack is resolved. In our example sentence, there is a dependency between `John' and `likes'. For NonInc, this dependency is resolved only after all the four remaining words in the sentence are shifted. In other words, it has to wait for four more words before this dependency is resolved and hence the waiting time is four. Whereas, in our RevInc algorithm, this dependency is resolved immediately, without waiting for more words to be shifted, and hence the waiting time is zero. The third column in Table 1 gives the waiting time for both the algorithms. Since we compromised incrementality in cases like coordination, waiting time for our RevInc algorithm is not zero but it is significantly lower than the
This is a case where the dependencies are not true to the CCG grammar, and make our algorithm less incremental than SCH would allow.
3

59

