Combination Type 1. Direct phrase table (baseline) 2. Best result using single pivot 3. Combine All Pivots using MDP 4. A - Linear Interpolate All Pivot tables (BLEU score ratio) 5. B - Fill Interpolate All Pivot tables (Priority according to BLEU score) 6. Linear Interpolate (9:1 ratio) Direct with All Pivot tables 7. Linear Interpolate (BLEU score ratio) Direct with All Pivots 8. Fill Interpolate Direct with All Pivots (Priority according to BLEU score) 9. Combine Direct and A using MDP 10. Combine Direct and B using MDP 11. Combine Direct and All Pivots tables using MDP 12. Combine Direct and Top 3 (BLEU) pivot tables using MDP (Oracle)

Jap-Hin 33.86 35.74 (Esp.) 34.49 32.50 32.12 34.56 35.24 35.28 36.40 36.67 38.42 38.22

Hin-Jap 37.47 39.49 (Kor.) 37.02 35.65 34.44 38.60 39.08 38.70 39.85 40.07 40.19 41.09

Table 3: Results Using Multiple Pivots With Different Combination Methods

training instances (29000 tuples - see section 3.2). Typically the interpolation methods are shown to give substantial performance boosts when the direct source-target phrase table is obtained using relatively smaller corpora sizes compared to those used for the source-pivot and pivot-target tables. In case of linear interpolation with a 9:1 weight ratio, the scores improve slightly in some cases for JapaneseHindi but degrade in case of Hindi-Japanese. However, in the case of linear interpolation where the BLEU score ratio is used as the weight ratio, the improvements are much better10 . Fill based interpolation also gives improvements in some cases, mostly when Chinese and Korean are used as pivots. An overall comparison shows that there is no consistency when a single pivot language is used and no conclusive comment can be made on the efficacy of these interpolation methods. Multiple Pivots: However in Table 3, rows 6 to 8 show that using all the pivots together, result in a significant improvement over the direct phrase tables. Linear interpolation with BLEU score ratio gives 35.24 BLEU (33.86 for direct phrase table) for Japanese-Hindi and 39.08 BLEU (37.47 for direct phrase table). Rows 4 and 5 show the scores of the linear and fill interpolation of only the pivot based phrase tables. It is interesting to see that in case of Japanese-Hindi the BLEU scores rival that of the direct phrase table (32.50/32.12 v.s. 33.86). This is
10

Expected as we use test set evaluation information.

similar in the case of Hindi-Japanese: 35.65/34.44 v.s. 37.47. The following points must be noted: a. Since the setting is multilingual and improvements, however slight, are observed in some cases it must be the case that, through pivoting, additional (and possibly improved) phrase pairs are induced which are not extracted using the direct sourcetarget parallel corpus. This also gives reason to believe that every pivot induces a different set of phrase pairs thereby overcoming the limitations of poor alignment (and effectively phrase extraction) on small corpora. Even if there is no alignment error, pivoting still introduces new phrase pairs which improves MT performance. b. The pivot based phrase tables already have an incomplete probability space with respect to the phrase pair distribution. Linear interpolation tends to violate the overall probability mass since the phrase pair distribution gets changed. Fill interpolation just adds additional phrase pairs from the next phrase table when not available in the current one which leads to poor mixing of different probability models giving poorer performance in-spite of additional phrase pairs being available. c. Since some pivot languages are obviously bad, their probability scores would drastically affect the overall probability mass. They should be excluded or given low weights, which we do by considering the BLEU score ratio. However, this is not a good idea because the scores for Telugu, a bad pivot for Hindi-Japanese translation, degraded to a lesser ex-

1198

