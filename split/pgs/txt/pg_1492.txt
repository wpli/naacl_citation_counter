tary, yielding incremental improvements. In fact, the combination of all three improves over the best published results on SL999, and approaches the best results for WS353, as shown in the last row of the Table. The state of the art on SL999 corresponds to (Hill et al., 2014a), who training a Recurrent Neural Net model on bilingual text. The best results on WS353 correspond to (Radinsky et al., 2011), who combine a Wikipedia-based algorithm with a corpus-based method which uses date-related information from news to learn word representations. Note that we have only performed some simple combination to show the complementarity of each information source. More sophisticated combinations (e.g. learning a regression model) could further improve results. We have performed some qualitative analysis, which indicates that there is a slight tendency for corpus embeddings (with the window size used in the experiments) to group related words (e.g. physics - proton), and not so much similar words (e.g. vodka - gin), while our KB embeddings include both. This analysis agrees with the results in Table 1, where all KB results are better than corpusbased Skip-gram for the semantic similarity dataset (SL999). In passing, note that the best published results to date on similarity (Hill et al., 2014a) use embeddings learnt from bilingual text which suggests that bilingual corpora are better suited to learn embeddings capturing semantic similarity.

A promising direction of this research is to leverage multilingual Wordnets to produce cross-lingual embeddings. On another direction, one of the main limitations of KB approaches is that they produce a relatively small number of embeddings, limited by the size of the dictionary. In the future we want to overcome this sparsity problem by combining both textual and KB based embeddings into a unified model. In fact, we think that our technique opens up exciting opportunities to combine distributional and knowledgebased word representations. It would also be interesting to investigate the influence of the different semantic relations in WordNet, either by removing certain relations or by assigning different weights to them. This investigation could give us deeper insights about the way our knowledge-based approach codes meaning in vector spaces.

Acknowledgements
This work was partially funded by MINECO (CHIST-ERA READERS project ­ PCIN-2013002-C02-01, and SKaTeR project ­ TIN201238584-C06-02), and the European Commission (QTLEAP ­ FP7-ICT-2013.4.1-610516). The IXA group is funded by the Basque Government (A type Research Group).

References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas ¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19­27. Association for Computational Linguistics. Eneko Agirre, Montse Cuadros, German Rigau, and Aitor Soroa. 2010. Exploring Knowledge Bases for Similarity. In LREC. Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57­84. K. Avrachenkov, N. Litvak, D. Nemirovsky, and N. Osipova. 2007. Monte carlo methods in pagerank computation: When one iteration is sufficient. SIAM J. Numer. Anal., 45(2):890­904.

5

Conclusions

We have presented a novel algorithm which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models to produce new word representations. Our evaluation in word relatedness and similarity datasets has shown that these new word representations attain similar results to those of the original random walk algorithm, using 300 dimensions instead of tens of thousands. Furthermore, the word representations are complementary to those of the random walk algorithm and to corpus-based continuous representations, producing better results when combined, and improving the state-of-the-art in the similarity dataset. Hand inspection reinforces the observation that WordNet-based 1438

