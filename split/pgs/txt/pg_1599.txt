MWE ID Feature Set MWE
Y Model Size M 8 194k 3,555k 4,371k 4,388k 4 5 5 4

Class labeling F1 P -- R -- F1 --
NSST R VSST R Aux R

Tag Acc -- 80.73 81.20 82.49 --
71.64 71.34 74.17

P

R

72.97 55.55 63.01

--
59.14 61.49 66.95

--
93.71 92.45 94.97

MWE 146 MWE+clusters 146 MWE+clusters+SST 146

67.77 55.76 61.14 64.68 66.78 65.71 68.55 56.73 62.04 65.69 67.76 66.71 71.05 56.24 62.74 69.47 71.90 70.67

Table 2: Results on test for lexical semantic analysis of noun and verb supersenses and MWEs with increasingly complex models. Class labeling performance is given in aggregate, and class labeling recall is further broken down into noun supersense tagging (NSST), verb supersense tagging (VSST), and auxiliary verb tagging. All of these results use a percept cutoff of 5. The first result row uses a collapsed tagset (just the MWE status) rather than predicting full supersense labels, as described in §4.4. The number of training iterations M was tuned by cross-validation on train. The best result in each column and section is bolded. lemma get look take time(s) unique SSTs 7 gold, 8 pred. 2 gold, 3 pred. 5 gold, 5 pred. 3 gold, 2 pred. majority baseline accuracy 12 8 8 8 28 13 21 14 6 12 11 9 28 13 21 14

estimation, additional annotated resources that cover a fuller spectrum of written language.

5

Conclusion

Table 3: Four polysemous lemmas and counts of their gold vs. predicted supersenses in test (limited to cases where both the gold standard tag and the predicted tag included a supersense). The distribution of gold supersenses for take, for example, is V: SOCIAL: 8, V: MOTION: 7, V: POSSESSION : 1, V: STATIVE : 4, V: EMOTION : 1.

tation of Ciaramita and Altun (2006)20 --to tag our test data in the reviews domain. By our class labeling evaluation, the result is 51.05% precision, 48.93% recall, and 49.97% F1 .21 Even without word clusters or the supersense-tailored features of figure 4, our simplest in-domain model reaches 65.71% F1 . Though there are minor differences in features between the two models, both are first-order structured perceptron taggers. We believe that this wide gulf is primarily an artifact of the training data. The annotation methodology was very different (direct MWE and supersense annotation in our case, vs. relying on mappings from WordNet synsets in the case of SemCor), and the vocabulary and style are vastly different between casual online writing and edited prose. Building lexical semantic models that are robust to many domains at once will require further experimentation, and in our
By Michael Heilman (Heilman, 2011, pp. 47­48); downloaded from: http://www.ark.cs.cmu.edu/mheilman/
questions/SupersenseTagger-10-01-12.tar.gz 21
20

We have integrated the multiword expression identification task formulated in Schneider et al. (2014a) with the supersense tagging task of Ciaramita and Altun (2006). Supersenses offer coarse-grained and broadly applicable semantic labels for lexical expressions and naturally complement multiword expressions in lexical semantic analysis. We have annotated English online reviews for supersenses, including developing detailed annotation criteria for verbs. Experiments with discriminative joint tagging of MWEs and supersenses establish a strong baseline for future work, which may incorporate new features, richer models, and indirect forms of supervision (cf. Grave et al., 2013; Johannsen et al., 2014) for this task. We also expect future investigations will apply our tagger to a downstream task such as semantic parsing or machine translation (for further discussion of potential applications, see Schneider, 2014, pp. 179­189). Our data and open-source software is available at http://www.ark.cs.cmu.edu/LexSem/.

Acknowledgments
We thank our energetic annotators, Nora Kazour, Spencer Onuffer, Emily Danchik, and Michael T. Mordowanec, as well as Chris Dyer, Lori Levin, Ed Hovy, Tim Baldwin, Mark Steedman, and anonymous reviewers for useful feedback on the technical content. This research was supported in part by NSF CAREER grant IIS-1054319 and DARPA grant FA8750-12-2-0342 funded under the DEFT program.

Excluding auxiliaries (which are not part of the original supersense representation and thus not predicted by Heilman's tagger) from the evaluation, recall rises to 52.50% and F1 to 51.76%.

1545

