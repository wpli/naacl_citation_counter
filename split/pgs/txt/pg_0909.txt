prompt. A dialog system's dialog manager chooses the prompt to say next according to its dialog strategy, which maps each system state to an action. An effective dialog strategy guides users to informative explanations that provide novel relations which let K NOWBOT successfully answer the question. We compare two different strategies. A userinitiative strategy always asks open-ended questions to prompt the user for new explanations, e.g. line S2 in Figure 1. These prompts let users introduce salient concepts on their own. In contrast, a mixed-initiative strategy utilizes focused prompts (line S4 in Figure 1) to introduce potentially related concepts. K NOWBOT chooses what pair of concepts to ask about based on how discriminative they are. The most discriminative concepts are the pair of question and support concepts that (1) don't already have an edge between them, (2) satisfies the alignment constraint for the user's answer, and (3) satisfies the alignment constraint for the fewest alternative answers. By proposing relations that would lead to a swift completion of the dialog task, K NOWBOT shares the burden of knowledge acquisition with the user. Both dialog strategies are question-independent, but because we don't use a comprehensive dialog model to represent the state space, we rely on hand built rules instead of optimizing with respect to a reward function. For example, K NOWBOT always starts by asking the user for their answer, and if a new support sentence is found will always immediately present it to the user for confirmation.

question if they felt the dialog was not progressing. Individual dialog sessions were anonymous. The system starts each dialog with an empty knowledge graph, using only identity relations to select its answer. This default answer is correct on 44 of the 107 questions, and an additional 10 questions have no associated supporting sentence for the correct answer in S CITEXT. We run dialogs for the remaining 53 questions, for which each answer candidate has 80 supporting sentences in S CITEXT on average. A successful dialog terminates when the system extracts enough novel relations from the user that the correct answer has the highest alignment score with one of its supporting sentences. 4.1 Baseline: Interactive query expansion To evaluate whether task-driven relation extraction is an effective method for knowledge acquisition in the absence of an explicit dialog model, we also implement a baseline dialog strategy based on interactive query expansion (IQE). This baseline is similar to the recent knowledge acquisition dialog system of Rudnicky and Pappu (2014a; 2014b). In IQE, new knowledge is learned in the form of novel keywords that are appended to the questionanswer statement. For example, the dialog in Figure 1 shows the user teaching K NOWBOT how metal relates to electricity. K NOWBOT understands that the user intends that relation because it drives the dialog forward. IQE, in contrast, treats the user utterance as an unstructured bag of keywords. The unrecognized word "metal" is added to the bag of keywords representing each of the four alternative answers to form four augmented queries, and new overlap scores against sentences from S CITEXT are computed. The dialog progresses whenever a new vocabulary word increases the score for the augmented query for the user's chosen answer. The intuition behind query expansion is that users will explain their answers with salient keywords missing from the original question sentence. The expanded query will overlap with and uprank a support sentence that contains the acquired keywords. 4.2 Performance metrics Task completion is the proportion of dialogs that end in agreement. Higher task completion indicates that the dialog system is more successful in acquir-

4

Evaluation of dialog strategies

Our first experiment compares mixed-initiative and user-initiative strategies (section 3.2) to a baseline interactive query expansion (section 4.1). The purpose of this experiment is to investigate whether users can successfully complete our complex dialog task even though we don't use a trained semantic parser for natural language understanding. Dialogs were conducted through a web browser. Users were colleagues and interns at the Allen Institute for Artificial Intelligence, and so were familiar with the question-answering task but were not expert annotators. Users were invited to converse with the system of their choice, and to move on to a new 855

