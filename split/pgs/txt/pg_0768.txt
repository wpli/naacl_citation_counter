Multitask Learning for Adaptive Quality Estimation of Automatically Transcribed Utterances
Jos´ e G. C. de Souza  , Hamed Zamani , Matteo Negri , Marco Turchi , Daniele Falavigna University of Trento, Italy  Fondazione Bruno Kessler, Italy  School of ECE, College of Engineering, University of Tehran, Iran {desouza, negri, turchi, falavi}@fbk.eu h.zamani@ut.ac.ir

Abstract
We investigate the problem of predicting the quality of automatic speech recognition (ASR) output under the following rigid constraints: i) reference transcriptions are not available, ii) confidence information about the system that produced the transcriptions is not accessible, and iii) training and test data come from multiple domains. To cope with these constraints (typical of the constantly increasing amount of automatic transcriptions that can be found on the Web), we propose a domain-adaptive approach based on multitask learning. Different algorithms and strategies are evaluated with English data coming from four domains, showing that the proposed approach can cope with the limitations of previously proposed single task learning methods.

1

Introduction

The variety of applications for large vocabulary speech recognition technology (LVCSR) is rapidly growing. For instance, automatic transcriptions are now used, either as-is or as rough material to be checked and corrected by humans, for captioning and subtitling DVD movies, Youtube videos, TV programs and recordings in noisy environments such as meetings and teleconferences. To enable further integration in these and other scenarios, the improvement of the core automatic speech recognition (ASR) technology should go hand in hand with the development of evaluation methods adequate to address new needs and constraints. Indeed, the standard evaluation protocol, based on computing the 714

word error rate of transcription hypotheses against reference transcripts,1 is not always a viable solution. In terms of needs, the aforementioned applications call for efficient and replicable evaluation methods suitable for real-time processing. While the availability of manually-created reference transcripts is a core ingredient for system development, tuning and lab testing, their use for on-field evaluation (i.e. during the actual use) is impractical for obvious reasons (i.e. the need of a quick response). In terms of constraints, the problem is that ASR technology is often used as a black-box, that is, without any knowledge of how the transcriptions are generated.2 This calls for techniques capable to estimate ASR output quality under the rigid constraint of having, as a basic source of information, only the spoken utterance (the acoustic signal) and the transcription itself. Indeed, the invaluable information provided by current confidence estimation methods (e.g. word posterior probabilities (Evermann and Woodland, 2000; Wessel et al., 2001), consensus decoding (Mangu et al., 2000) and minimum Bayesrisk decoding (Xu et al., 2010)) is not accessible when evaluating the output of an unknown system.
The word error rate (WER) is the minimum edit distance between an hypothesis and the reference transcription. Edit distance is calculated as the number of edits (word insertions, deletions, substitutions) divided by the number of words in the reference. 2 For instance, as announced by Google, in 2012 about 157 million YouTube videos in 10 languages already featured captions generated by a black-box ASR system (source: http://techcrunch.com/2012/06/15/ youtube-launches-auto-captions-in-spanish/).
1

Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 714­724, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

