Lemma Local +Fertility +Predicate-centric +Argument-centric +Temporal +All Factors

F1 68.1 73.0 77.1 * 74.1 * 73.7 73.7 77.5 *

P 79.3 * 75.8 83.9 * 80.7 * 81.2 * 78.2 * 86.3 * RF Pred P 47.6 63.5 57.4 NA NA 57.4 56.9

R 59.6 70.5 71.3 68.6 67.5 69.7 70.3

Arg F1 61.7 67.7 66.6 67.4 66.8 67.9 65.8

EECB Arg P 79.1 * 76.3 80.9 * 81.6 * 83.0 * 80.6 * 83.1 *

Arg R 50.6 60.8 56.6 57.3 55.9 58.7 54.5

Pred F1 75.0 78.7 82.8 * 79.7 * 79.3 79.0 83.7 *

Pred P 87.3 * 81.4 87.4 * 85.0 * 85.1 * 82.1 89.7 *

Pred R 65.7 76.2 78.7 * 75.1 74.3 76.1 78.4 *

Lemma Local +Fertility +Predicate-centric +Argument-centric +Temporal +All factors

Pred F1 52.4 58.1 60.0 NA NA 59.0 59.4

Pred R 58.2 * 53.6 62.4 * NA NA 60.6 * 62.2 *

Figure 3: Cross validation results for EECB (above)

(Lee et al., 2012) and RF (left) (Roth and Frank, 2012). Statistically significant improvements from Local marked * (p < 0.05 using a one-sided pairedbootstrap test) and best results are bolded.

predicate- and argument-centric improve similarly across both predicates and arguments on EECB. While each of the joint factors all improve over the baselines on RF, the full model with all the joint factors does not perform as well as with some factors excluded. Specifically, the fertility model performs the best. We attribute this small gap to lack of training data (RF only contains 64 training document pairs in our experiments), as this is not a problem on the larger EECB dataset. Additionally, the joint models seem to trade precision for recall on the RF dataset compared to the Local baseline. Note that both models are tuned to maximize F1, so this tells you more about the shape of the ROC curve as opposed to either models' ability to achieve either high precision or recall. Since we don't see this behavior on the EECB corpus, it is more likely that this is a property of the data than the model.

8

Related Work

The task of predicate argument linking was introduced by Roth and Frank (2012), who used a graph parameterized by a small number of semantic features to express similarities between predicates and used min-cuts to produce an alignment. This was followed by Wolfe et al. (2013), who gave a locallyindependent, feature-rich log-linear model that utilized many lexical semantic resources, similar to the 18

sort employed in RTE challenges. Lee et al. (2012) considered a similar problem but sought to produce clusters of entities and events rather than an alignment between two documents with the goal of improving coreference resolution. They used features which consider previous event and entity coreference decisions to make future coreference decisions in a greedy manner. This differs from our model which is built on non-greedy joint inference, but much of the signal indicating when two mentions corefer or are aligned is similar. In the context of in-document coreference resolution, Recasens et al. (2013) sought to overcome the problem of opaque mentions7 by finding highprecision paraphrases of entities by pivoting off verbs mentioned in similar documents. We address the issue of opaque mentions not by building a paraphrase table, but by jointly reasoning about entities that participate in coreferent events (c.f. §4); the approaches are complementary. In this work we incorporate ordering information of events. Though we consider it an upstream task, there is a line of work trying to predict temporal relations between events (Pustejovsky et al., 2003; Mani et al., 2006; Chambers et al., 2014). Our results indicate this is a useful source of information, one of the first results to show an improvement from this
7

A lexically disparate description of an entity.

