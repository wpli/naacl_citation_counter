Turian et al., 2010). NNLM extract meaning from unlabeled corpora following the distributional hypothesis (Harris, 1954), where semantic features of a word are related to its co-occurrence patterns. NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature. The representations are obtained by optimizing the likelihood of existing unlabeled text. More recently, Mikolov et al. have developed simpler NNLM architectures (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), which drastically reduced computational complexity by deleting the hidden layer, enabling to compute accurate word representations from very large corpora. The representations obtained by these methods are compact, taking 1.5G for 3M words on 300-dimensional space, and have been shown to outperform other distributional corpus-based methods on several tasks, including the WS353 word similarity dataset (Baroni et al., 2014). In this work we propose to encode the meaning of words using the structural information in knowledge bases. That is, instead of modeling the meaning based on the co-occurrences of words in corpora, we model the meaning based on random walks over the knowledge base. Each random walk is seen as a context for words in the vocabulary, and fed into the NNLM architecture, which optimizes the likelihood of those contexts (cf. Fig. 1). The resulting word representations are more compact than those produced by regular random walk algorithms (300 vs. tens of thousands), and produce very good results on two well-known benchmarks on word relatedness and similarity: WS353 (Finkelstein et al., 2001) and SL999 (Hill et al., 2014b), respectively. We also show that the obtained representations are complementary to those of random walks alone and to distributional representations obtained by the same NNLM algorithm, improving the results. Some recent work has explored embedding KBs in low-dimensional continous vector spaces, representing each entity in a k-dimensional vector and characterizing typed relations between entities in the KB (e.g. born-in-city in Freebase or part-of in WordNet) as operations in the k-dimensional space (Wang et al., 2014). The model estimates the parameters which maximize the likelihood of the triples, which 1435

can then be used to infer new typed relations which are missing in the KB. In contrast, we use the relations to explicitly model the context of words, in two complementary approaches to embed information in KBs into continuous spaces.

2

NNLM

Neural Network Language Models have become a useful tool in NLP on the last years, specially in semantics. We have used the two models proposed in (Mikolov et al., 2013c) due to their simplicity and effectiveness in word similarity and relatedness tasks (Baroni et al., 2014): Continuous Bag of Words (CBOW) and Skip-gram. The first one is quite similar to the feedforward Neural Network Language Model, but instead of a hidden layer it has a projection layer, and thus all the words are projected in the same position. Word order has thus no influence in the projection. The training criterion is as follows: knowing previous and subsequent words in context, the model maximizes the probability of the predicting the word in the middle. The Skip-gram model uses each current word as an input to a log-linear classifier with a continuous projection layer, and predicts the previous and subsequent words in a context window. Although the Skip-gram model seems to be more accurate in most of the semantic tasks, we have used both variants in our experiments. We used a publicly available implementation1 .

3

Random Walks and NNLM

Our method performs random walks over KB graphs to create synthetic contexts which are fed into the NNLM architecture, creating novel word representations. The algorithm used for creating the contexts is a Monte Carlo method for computing the PageRank algorithm (Avrachenkov et al., 2007). We consider a KB as undirected graph G = (V, E ), where V is the set of concepts and E represents links among concepts. We also need a dictionary, an association from words to KB concepts. We construct an inverse dictionary that maps graph vertices with the words than can be linked to it. The inputs of the algorithm are: 1) the graph G = (V, E ), 2) the inverse dictionary and 3) the damp1

https://code.google.com/p/word2vec/

