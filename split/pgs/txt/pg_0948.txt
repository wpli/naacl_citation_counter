new sample D of size N :
 Rf := w



fw (x1 ), .., fw (xN ) , y1 , .., yN

dP r(D ) (3)

the training objective can be written as the following unconstrained optimization problem: 1 min ||w||2 2 + C max max w  (X, H, Y ) w 2 H Y - max w  (X, H, Y) + (Y , Y)
H

Generally, the loss function  cannot be decomposed into a linear combination of a loss function  over individual training samples. However, most discriminative large-margin learning algorithms assume for simplicity that the loss function is decomposable and the samples are i.i.d. (independent and identically distributed), which simplifies the sample  as: risk Rf w
 := Rf w

(9)

 (fw (x ), y )dP r(x , y )

(4)

which is similar to the training objective for the latent SVMs (Yu and Joachims, 2009), with the difference that instance-dependent loss function  is replaced by the sample loss function . Learning w by optimizing the above objective function is challenging, and is the subject of the next section.

Often learning algorithms make use of the empirical risk as an approximation of the sample risk: ^  := 1 R fw N
N

3

Optimizing the Training Objective

 (fw (xi ), yi )
i=1

(5)

In this section we present our method to learn latent SVMs with non-decomposable loss functions. Our training objective is Eq (9), which can be equivalently expressed as:
min
w

For non-decomposable loss functions, such as F ,  cannot be expressed in terms of instance-specific loss function  to construct the empirical risk of the kind in Eq. (5). Instead, we need to optimize the empirical risk constructed based on the sample loss: ^  :=  R fw or equivalently ^  := (fw (X), Y) R fw fw (x1 ), .., fw (xN ) , y1 , .., yN (6) (7)

1 ||w||2 2 + C max 2 y1 ,..,yN
N h

 (y1 , .., yN ), (y1 , .., yN )
N

+
i=1

max w  (xi , h, yi ) -

max w  (xi , h, yi )
i=1 h

(10)

The training objective is non-convex, since it is the difference of two convex functions. In this section we make use of the CCCP to populate the hidden variables (Yu and Joachims, 2009; Yuille and Rangarajan, 2001), and interleave it with dual decomposition (DD) to solve the resulting intermediate lossaugmented inference problems (Ranjbar et al., 2012; Rush and Collins, 2012; Komodakis et al., 2011). 3.1 Concave-Convex Procedure (CCCP)

where fw (X) := (fw (x1 ), .., fw (xN )). Having defined the empirical risk in Eq (7), we formulate the learning problem as a structured prediction problem. Instead of learning a mapping function fw : X  Y from an individual instance x  X to its label y  Y , let us learn a mapping function f : X  Y from all instances X  X to their labels Y  Y . We then define the best labeling using a linear discriminant function: f (X) := arg max max w  (X, H, Y )
Y Y HH

The CCCP (Yuille and Rangarajan, 2001) gives a general iterative method to optimize those nonconvex objective functions which can be written as the difference of two convex functions g1 (w) - g2 (w). The idea is to iteratively lowerbound g2 with a linear function g2 (w(t) ) + v  (w - w(t) ), and take the following step to update w: w(t+1) := arg min g1 (w) - w  v(t)
w

(8)

(11)

where (X, H, Y ) := N i=1 (xi , hi , yi ). Based on the margin re-scaling formulation of structured prediction problems (Tsochantaridis et al., 2004), 894

In our case, the training objective Eq (10) is the difference of two convex functions, where the second function g2 is C N i=1 maxh w(xi , h, yi ) . The

