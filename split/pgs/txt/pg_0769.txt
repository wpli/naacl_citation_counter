To cope with these issues, Negri et al. (2014) proposed a reference-free ASR quality estimation (QE) method capable to operate both in a glass-box (i.e. having access to confidence information) and in a black-box fashion (i.e. without any knowledge about the ASR system's inner workings). According to the authors, despite the promising evaluation results, the supervised learning approach adopted has a main limitation: the degradation in performance when models are trained on non-homogeneous data that comes from different domains, speakers, or systems. However, although empirical evidence of this limitation is provided, the robustness of ASR QE systems to the heterogeneity of training and test data is left as an open issue. Filling this gap, which is the goal of this paper, would be a significant step towards real-time ASR output evaluation, and its seamless integration in a number of application frameworks. Along this direction, we propose and evaluate a supervised domain adaptation technique based on multitask learning (Caruana, 1997). Our approach aims to exploit training data coming from different "domains" (in a broad sense, e.g. different genres, speakers, topics, styles, etc.) and to obtain ASR QE models that are robust to differences with respect to the test data. Experiments are carried out with English data coming from four domains, and by comparing different algorithms and strategies. Overall, our contributions can be summarized as follows: · Multitask learning (MTL) is investigated for the first time in the ASR QE scenario, as a way to cope with the dissimilarity between training and test data coming from multiple domains. · The QE problem is approached both as a regression (assignment of real-valued quality labels) and as a binary classification task (assignment of `good'/`bad' labels according to a given, arbitrary WER threshold). The latter task is introduced as a preliminary study. · Results are thoroughly analyzed, considering both the amount of training data coming from the different domains and the relative distance between their distributions. 715

2

Related Work

In the ASR field, most prior works that address the reference-free estimation of output quality fall into the confidence estimation (CE) framework. In this framework, the reliability of a transcription is estimated from the system's standpoint, that is, as a function of the process that generated the transcription (Sukkar and Lee, 1996; Evermann and Woodland, 2000; Wessel et al., 2001; Sanchis et al., 2012; Seigel, 2013, inter alia). In CE, the information available to the estimator covers all the aspects of the decoding process (e.g. word posterior probabilities, n-best lists, hypotheses density, language model scores). Although related to our problem, CE hence builds on a strong assumption (i.e. the ASR system is known), which does not hold in many situations. Quality estimation, instead, operates in the least favorable condition in which, besides the lack of references, the ASR system is regarded as a "blackbox". To our knowledge, the study proposed in (Negri et al., 2014) is the most relevant related work along this direction. In their investigation, the authors run a set of experiments aimed to predict the WER of automatically transcribed utterances in different testing conditions (by varying the distance between training and test data), with different stateof-the-art learning algorithms (all for regression), and with different groups of features (the so called "black-box" and "glass-box" feature groups). The major problem emphasized in their analysis is the strong dependency between QE models and the degree of homogeneity of training and test data. From the application perspective, this is a severe limitation since (as in any other supervised learning setting) the similarity of training and test sets is a strong requirement that should be bypassed (possibly with minimal loss in performance). This issue, which has not been addressed yet, is the starting point of our investigation. Another aspect that so far has been disregarded concerns the type of estimates that a model should return. Indeed, while ASR QE has been explored as a regression task (i.e. aiming to return real-valued quality estimates), nothing has been done to approach it as a classification problem (i.e. assigning quality estimates chosen from two or more classes). In classification mode, we return explicit good/bad

