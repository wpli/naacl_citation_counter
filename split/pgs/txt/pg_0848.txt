a new intermediate representation called "headordered dependency trees", which encode head ordering information in dependeny labels. Their algorithm is based on a reduction of the constituent parsing to dependency parsing of such trees. There has been successful work combining dependency and phrase-structure information to build accurate c-parsers. Klein and Manning (2002) construct a factored generative model that scores both context-free syntactic productions and semantic dependencies. Carreras et al. (2008) construct a stateof-the-art parser that uses a dependency parsing model both for pruning and within a richer lexicalized parser. Similarly, Rush et al. (2010) use dual decomposition to combine a powerful dependency parser with a lexicalized phrase-structure model. This work differs in that we treat the dependency parse as a hard constraint, hence largely reduce the runtime of a fully lexicalized phrase structure parsing model while maintaining the ability, at least in principle, to generate highly accurate phrasestructure parses. Finally there have also been several papers that use ideas from dependency parsing to simplify and speed up phrase-structure prediction. Zhu et al. (2013) build a high-accuracy phrase-structure parser using a transition-based system. Hall et al. (2014) use a stripped down parser based on a simple X-bar grammar and a small set of lexicalized features.

6

Methods

We ran a series of experiments to assess the accuracy, efficiency, and applicability of our parser, PAD, to several tasks. These experiments use the following setup. For English experiments we use the standard Penn Treebank (PTB) experimental setup (Marcus et al., 1993). Training is done on 221, development on 22, and testing on 23. We use the development set to tune the regularization parameter,  = 1e - 8, and the pruning threshold,  = 0.95. For Chinese experiments, we use version 5.1 of the Penn Chinese Treebank 5.1 (CTB) (Xue et al., 2005). We followed previous work and used articles 001270 and 4401151 for training, 301325 for development, and 271300 for test. We also use the development set to tune the regularization parame-

ter,  = 1e - 3. Part-of-speech tagging is performed for all models using TurboTagger (Martins et al., 2013). Prior to training the d-parser, the training sections are automatically processed using 10-fold jackknifing (Collins and Koo, 2005) for both dependency and phrase structure trees. Zhu et al. (2013) found this simple technique gives an improvement to dependency accuracy of 0.4% on English and 2.0% on Chinese in their system. During training, we use the d-parses induced by the head rules from the gold c-parses as constraints. There is a slight mismatch here with test, since these d-parses are guaranteed to be consistent with the target c-parse. We also experimented with using 10fold jacknifing of the d-parser during training to produce more realistic parses; however, we found that this hurt performance of the parser. Unless otherwise noted, in English the test dparsing is done using the RedShift implementation6 of the parser of Zhang and Nivre (2011), trained to follow the conventions of Collins head rules (Collins, 2003). This parser is a transition-based beam search parser, and the size of the beam k controls a speed/accuracy trade-off. By default we use a beam of k = 16. We found that dependency labels have a significant impact on the performance of the RedShift parser, but not on English dependency conversion. We therefore train a labeled parser, but discard the labels. For Chinese, we use the head rules compiled by Ding and Palmer (2005)7 . For this data-set we trained the d-parser using the YaraParser implementation8 of the parser of Zhang and Nivre (2011), because it has a better Chinese implementation. We use a beam of k = 64. In experiments, we found that Chinese labels were quite helpful, and added four additional features templates conjoining the label with the non-terminals of a rule. Evaluation for phrase-structure parses is performed using the evalb9 script with the standard setup. We report labeled F1 scores as well as recall and precision. For dependency parsing, we report
https://github.com/syllog1sm/redshift http://stp.lingfil.uu.se/~nivre/ research/chn_headrules.txt 8 https://github.com/yahoo/YaraParser 9 http://nlp.cs.nyu.edu/evalb
7 6

7

794

