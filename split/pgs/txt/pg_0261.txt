F1 R1 EH3 N3 D4 EH3 N3 AH5 M5 IY5

T1 OW1 F1 UW3 T2 ER3 K5 IY5

Figure 2: Derivations for friend + enemy  "frenemy" and tofu + turkey  "tofurkey." Subscripts indicate the step applied to each phoneme.

We take a statistical modeling approach to portmanteau generation, using training examples (Table 1) to learn weights for a cascade of finite state machines. To handle the 2-input, 1-output problem inherent in the task, we implement a multitape FST. This work's contributions can be summarized as:  a portmanteau generation model, trained in an unsupervised manner on unaligned portmanteaux and component words,  the novel use of a multitape FST for a 2-input, 1-output problem, and  the release of our training data.1

3. 1+ phonemes from W1 pron are aligned with an equal number of phonemes from W2 pron . For each aligned pair of phonemes (x, y ), either x or y is output. 4. 0+ phonemes from W1 pron are deleted, until the 1 end of Wpron . 5. 0+ phonemes from W2 pron are output, until the end of W2 . pron

3

Multitape FST model

2

Definition of a portmanteau

In this work, a portmanteau PM and its pronunciation PMpron have the following constraints:  PM has exactly 2 component words W1 and 2 W2 , with pronunciations W1 pron and Wpron .  All of PM's letters are in W1 and W2 , and all 2 phonemes in PMpron are in W1 pron and Wpron .  All pronunciations use the Arpabet symbol set.  Portmanteau building occurs at the phoneme level. PMpron is built through the following steps (further illustrated in Figure 2): 1. 0+ phonemes from W1 pron are output. 2. 0+ phonemes from W2 pron are deleted.
1

Finite state machines (FSMs) are powerful tools in NLP and are frequently used in tasks like machine transliteration and pronunciation. Toolkits like Carmel and OpenFST allow rapid implementations of complex FSM cascades, machine learning algorithms, and n-best lists. Both toolkits implement two types of FSMs: finite state acceptors (FSAs) and finite state transducers (FSTs), and their weighted counterparts (wFSAs and wFSTs). An FSA has one input tape; an FST has one input and one output tape. What if we want a one input and two output tapes for an FST? Three input tapes for an FSA? Although infrequently explored in NLP research, these "multitape" machines are valid FSMs. 2 In the case of converting {W1 pron , Wpron } to PMpron , an interleaved reading of two tapes would be impossible with a traditional FST. Instead, we model the problem with a 2-input, 1-output FST (Figure 3). Edges are labeled x : y : z to indicate input 2 tapes W1 pron and Wpron and output tape PMpron , respectively.

4

FSM Cascade

Available at both authors' websites.

We include the multitape model as part of an FSM cascade that converts W1 and W2 to PM (Figure 4). : : : y: : : q3 a q3 : : x: y: x/y : : q4 a q4 : : x: : : : q5 a q5 : y: y

q1 : : q1 a

: : x: : x : :

q2

q2 a

Figure 3: A 2- input, 1-output wFST for portmanteau pronunciation generation.

207

