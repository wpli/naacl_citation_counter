We demonstrate in Section 5 that our dataset is significantly different from newswire based on results from the effective, widely-used Berkeley system (Durrett and Klein, 2013). These results motivate us to develop a very simple end-to-end coreference resolution system consisting of a crfbased mention detector and a pairwise classifier. Our system outperforms the Berkeley system when both have been trained on our new dataset. This result motivates further exploration into complex coreference types absent in newswire data, which we discuss at length in Section 7.

2

Newswire's Limitations for Coreference

Newswire text is widely used as training data for coreference resolution systems. The standard datasets used in the muc (MUC-6, 1995; MUC-7, 1997), ace (Doddington et al., 2004), and CoNLL shared tasks (Pradhan et al., 2011) contain only such text. In this section we argue why this monoculture, despite its many past successes, offer diminishing results for advancing the coreference subfield. First, newswire text has sparse references, and those that it has are mainly identity coreferences and appositives. In the CoNLL 2011 shared task (Pradhan et al., 2007) based on OntoNotes 4.0 (Hovy et al., 2006),2 there are 2.1 mentions per sentence; in the next section we present a dataset with 3.7 mentions per sentence.3 In newswire text, most nominal entities (not including pronouns) are singletons; in other words, they do not corefer to anything. OntoNotes 4.0 development data contains 25.4K singleton nominal entities (Durrett and Klein, 2013), compared to only 7.6K entities which corefer to something (anaphora). On the other hand, most pronominals are anaphoric, which makes them easy to resolve as pronouns are single token entities. While
As our representative for "newswire" data, the English portion of the Ontonotes 4.0 contains professionallydelivered weblogs and newsgroups (15%), newswire (46%), broadcast news (15%), and broadcast conversation (15%). 3 Neither of these figures include singleton mentions, as OntoNotes does not have gold tagged singletons. Our dataset has an even higher density when singletons are included.
2

it is easy to obtain a lot of newswire data, the amount of coreferent-heavy mention clusters in such text is not correspondingly high. Second, coreference resolution in news text is trivial for humans because it rarely requires world knowledge or semantic understanding. Systems trained on news media data for a related problem--entity extraction--falter on nonjournalistic texts (Poibeau and Kosseim, 2001). This discrepancy in performance can be attributed to the stylistic conventions of journalism. Journalists are instructed to limit the number of entities mentioned in a sentence, and there are strict rules for referring to individuals (Boyd et al., 2008). Furthermore, writers cannot assume that their readers are familiar with all participants in the story, which requires that each entity is explicitly introduced in the text (Goldstein and Press, 2004). These constraints make for easy reading and, as a side effect, easy coreference resolution. Unlike this simplified "journalistic" coreference, everyday coreference relies heavily on inferring the identities of people and entities in language, which requires substantial world knowledge. While news media contains examples of coreference, the primary goal of a journalist is to convey information, not to challenge the reader's coreference resolution faculty. Our goal is to evaluate coreference systems on data that taxes even human coreference.

3

Quiz Bowl: A Game of Human Coreference

One example of such data comes from a game called quiz bowl. Quiz bowl is a trivia game where questions are structured as a series of sentences, all of which indirectly refer to the answer. Each question has multiple clusters of mutuallycoreferent terms, and one of those clusters is coreferent with the answer. Figure 1 shows an example of a quiz bowl question where all answer coreferences have been marked. A player's job is to determine4 the entity refIn actual competition, it is a race to see which team can identify the coreference faster, but we ignore that aspect here.
4

1109

