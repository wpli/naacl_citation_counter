Berg-Kirkpatrick et al follow Liang et al in using maximum likelihood estimation to estimate their model's parameters (Berg-Kirkpatrick et al actually use L2 -regularised maximum likelihood estimates). As Liang et al note, it's easy to show that the maximum likelihood segmentation leaves each utterance unsegmented, i.e., each utterance is analysed as a single word. To avoid this, Berg-Kirkpatrick et al follow Liang et al by multiplying the word probabilities by a word length penalty term. Thus the likelihood LD they actually maximise is as shown below:
n

0.9

Surface token f-score

0.8 0.7 0.6 0.5 0.4 0.3 1.4 Data Brent Buckeye

Word length penalty

1.5

1.6

1.7

LD (  ) =
i=1

P(wi | ) P(sj | ) exp(-|si |d )

Figure 1: Sensitivity of surface token f-score to word length penalty factor d for the Brent and Buckeye corpora on data with no /d/ or /t/ deletions. Performance is sensitive to the value of the word length penalty d, and the optimal value of d depends on the corpus.

P(w | ) =

s1 ...s j =1 s.t.s1 ...s =w

where d is a constant chosen to optimise segmentation performance. This means that the model is deficient, i.e., sS P(s | ) < 1. (Because our model uses a word length penalty in the same way, it too is deficient). As Figure 1 shows, performance is very sensitive to the word length penalty parameter d: the best word segmentation on the Brent corpus is obtained when d  1.6, while the best segmentation on the Buckeye corpus is obtained when d  1.5. As far as we know there is no principled way to set d in an unsupervised fashion, so this sensitivity to d is perhaps the greatest weakness of this kind of model. Even so, it's interesting that a unigram model without the kind of inter-word dependencies that Goldwater et al. (2009) argues for can do so well. It's possible that the improvement that Goldwater et al found with the bigram model is because modelling individual bigram dependencies splits the data in a way that reduces overlearning (B¨ orschinger et al., 2012).

3

A MaxEnt unigram model of word segmentation and word-final /d/ and /t/ deletion

This section explains how we extend the BergKirkpatrick et al. (2010) model to handle a set P of phonological processes, where a phonological process p  P is a partial, non-deterministic function 306

mapping underlying forms to surface forms. For example, word-final /t/ deletion is the function mapping underlying underlying forms ending in /t/ to surface forms lacking that final segment. Our model is also a unigram model, but it defines a distribution over pairs (s, u) of surface/underlying form pairs, where s is a surface form and u is an underlying form. Below we allow this distribution to condition on phonological properties of the neighbouring surface forms. The set X of possible (s, u) surface/underlying form pairs is defined as follows. For each surface form s  S (the set of length-bounded phone substrings of the data D), (s, s)  X . In addition, if u  S and some phonological alternation p  P maps u to a surface form s  p(u)  S , then (s, u)  X . That is, we require that potential underlying forms appear as surface substrings somewhere in the data D (which means this model cannot handle e.g., absolute neutralisation). In the experiments below, we let P be phonological processes that delete word-final /d/ and /t/ phonemes. Given the Buckeye data, ([l.ih.v], /l.ih.v/), ([l.ih.v], /l.ih.v.d/) and ([l.ih.v], /l.ih.v.t/) are all members of X (i.e., candidate (s, u) pairs), corresponding to "live", "lived" and the non-word "livet" respectively, where the latter two surface forms are generated by final /d/ and /t/ deletion respectively. Word-final /d/ and /t/ deletion depends on various aspects of the phonological context, such as

