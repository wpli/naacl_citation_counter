Feature 8 and 17, the summary is generated by a general unsupervised ILP-based summarization system from the given old data set. The idea of Feature 9 and 10 was first introduced by (Bysani, 2010); here we applied it to bigrams. dfmax is the number of documents in the data set (10 in the TAC data), which can be thought of the maximum value of document frequency for a bigram. Feature 11 is interpolated n-gram document frequency, which was first introduced by (Ng et al., 2012): , where wu and wb are unigrams and bigrams respectively in sentence S . Feature 18 and 19 are variants of Features 11, where instead of document frequency (df in the formula above), bigram and unigram's novelty and uniqueness values are used. Among these features, the feature values of feature 4, 5 and 6 are discrete. In this study, we discretized all the other continuous values into ten categories according to the value range in the training data. To train the model (feature weights), we use the average perceptron strategy (Collins, 2002) to update the feature weights whenever the hypothesis by the ILP decoding process is incorrect. Binary class labels are used for bigrams in the learning process, that is, we only consider whether a bigram is in the system generated summary or human summaries, not their term or document frequency. We use a fixed learning rate (0.1) in training. 2.3 Sentence Reranking on ILP Results In the ILP method, sentence selection is done by considering the concepts that a sentence contains. It is difficult to add indicative features in this framework to explicitly represent the sentence's salience, and more importantly, its novelty for the update summarization task. This information is only captured by the weights of the bigrams using the method described above. Therefore, we propose to use a two-step approach, where an initial ILP module first selects some sentences and then a reranking module uses sentence level features to rerank them to generate the final summary. We expect this step of modeling sentences directly can complement the bigram
Note that we do not use all the sentences in the ILP module. The `relevant' sentences are those that have at least one bigram with document frequency larger than or equal to three.
1



wu S

dfnew (wu )+(1-) |S |

wb S

dfnew (wb )

Bigram Level Features Importance Related Features 1. dfnew (b): document frequency in new data set 2. normalized term frequency in all filtered relevant sentences1 3. sentence frequency in all relevant sentences 4. do bigram words appear in topic's query ? 5. is the bigram in the first 1/2/3 position of that sentence? 6. is the bigram in the last 1/2/3 position of that sentence? Novelty Related Features 7. dfold (b): document frequency in old data set 8. normalized term frequency in the summary from old data set dfnew (b) 9. bigram novelty value n(b) = dfold (b)+dfmax 10. bigram uniqueness value u(b) = 0 if new (b) dfold (b) > 0; otherwise u(b) = df dfmax Sentence Level Features Importance Related Features 11. interpolated n-gram document frequency 12. sentence position in that document 13. is the sentence in the first 1/2/3 position in that document? 14. is the sentence in the last 1/2/3 position in that document? 15. sentence length 16. sentence similarity with topic's query Novelty Related Features 17. sentence similarity with the summary from old data set 18. interpolated n-gram novelty 19. interpolated n-gram uniqueness
Table 1: Features in the supervised ILP model for weighting bigrams.

centric view in the first ILP summarization module. For the first step, we use the ILP framework with our supervised bigram weighting method to obtain a summary of N words (N is greater than the required summary length L). Note that the ILP model selects these output sentences as a set that optimizes the objective function, and there are no scores for each individual sentence. Second, we use sentence level features listed in Table 1 to rerank the can-

1319

