infer its types. We use information in Freebase and external information from Wikipedia to complete the KB. · Entity Type Features: The entity types observed in the training data can be a useful signal to infer missing entity type instances. For example, in our snapshot of Freebase, it is not uncommon to find an entity with the type /people/deceased person but missing the type /people/person. · Freebase Description: Almost all entities in Freebase have a short one paragraph description of the entity. Figure 1 shows the Freebase description of Jean Metellus that can be used to infer the type /book/author which Freebase does not contain as the date of writing this article. · Wikipedia: As external information, we include the Wikipedia full text article of an entity in its feature representation. We consider entities in Freebase that have a link to their Wikipedia article. The Wikipedia full text of an entity gives several clues to predict it's entity types. For example, Figure 2 shows a section of the Wikipedia article of Claire Martin which gives clues to infer the type /award/award winner that Freebase misses.

3

Evaluation Framework

In this section, we propose an evaluation methodology for the task of inferring missing entity type instances in a KB. While we focus on recovering entity types, the proposed framework can be easily adapted to relation extraction as well. First, we discuss our two-snapshot dataset construction strategy. Then we motivate the importance of evaluating KBC algorithms globally and describe the evaluation metrics we employ. 3.1 Two Snapshots Construction In most previous work on KB completion to predict missing relation facts (Mintz et al., 2009; Riedel et al., 2013), the methods are evaluated on a subset of facts from a single KB snapshot, that are hidden while training. However, given that the missing entries are usually selected randomly, the distribution 517

of the selected unknown entries could be very different from the actual missing facts distribution. Also, since any fact could be potentially used for evaluation, the methods could be evaluated on their ability to predict easy facts that are already present in the KB. To overcome this drawback, we construct our train and test set by considering two snapshots of the knowledge base. The train snapshot is taken from an earlier time without special treatment. The test snapshot is taken from a later period, and a KBC algorithm is evaluated by its ability of recovering newly added knowledge in the test snapshot. This enables the methods to be directly evaluated on facts that are missing in a KB snapshot. Note that the facts that are added to the test snapshot, in general, are more subtle than the facts that they already contain and predicting the newly added facts could be harder. Hence, our approach enables a more realistic and challenging evaluation setting than previous work. We use manually constructed Freebase as the KB in our experiments. Notably, Chang et al. (2014) use a two-snapshot strategy for constructing a dataset for relation extraction using automatically constructed NELL as their KB. The new facts that are added to a KB by an automatic method may not have all the characteristics that make the two snapshot strategy more advantageous. We construct our train snapshot 0 by taking the Freebase snapshot on 3rd September, 2013 and consider entities that have a link to their Wikipedia page. KBC algorithms are evaluated by their ability to predict facts that were added to the 1st June, 2014 snapshot of Freebase . To get negative data, we make a closed world assumption treating any unobserved instance in Freebase as a negative example. Unobserved instances in the Freebase snapshot on 3rd September, 2013 and 1st June, 2014 are used as negative examples in training and testing respectively.1 The positive instances in the test data ( - 0 ) are facts that are newly added to the test snapshot . Using the entire set of negative examples in the test data is impractical due to the large number of negative examples. To avoid this we only add the negative types
Note that some of the negative instances used in training could be positive instances in test but we do not remove them during training.
1

