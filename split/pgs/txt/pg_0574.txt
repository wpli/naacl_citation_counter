Algorithm 1 The training algorithm for Linear.Adagrad.
1: Initialize wt = 0, t = 1 . . . |T | 2: for (e, t)  0 do 3: for e  NE (e, t) do T T 4: if wt (e) - wt (e ) - 1 < 0 then 5: AdaGradUpdate(wt , (e ) - (e)) 6: end if 7: end for 8: for t  NT (e, t) do T T 9: if wt (e) - wt (e) - 1 < 0 then 10: AdaGradUpdate(wt , -(e)) 11: AdaGradUpdate(wt , (e)). 12: end if 13: end for 14: end for

Algorithm 2 The training algorithm for the embedding model.
1: Initialize V, U randomly. 2: for (e, t)  0 do 3: for e  NE (e, t) do 4: if s(e, t) - s(e , t) - 1 < 0 then 5: µ  VT (t) 6:   UT ((e ) - (e)) 7: for i  1 . . . d do 8: AdaGradUpdate(Ui , µ[i]((e ) - (e))) 9: AdaGradUpdate(Vi ,  [i](t)) 10: end for 11: end if 12: end for 13: for t  NT (e, t) do 14: if s(e, t) - s(e, t ) - 1 < 0 then 15: µ  VT ((t ) - (t)) 16:   UT (e) 17: for i  1 . . . d do 18: AdaGradUpdate(Ui , µ[i](e)) 19: AdaGradUpdate(Vi ,  [i]((t ) - (t))) 20: end for 21: end if 22: end for 23: end for

4.2

Algorithms

We propose three different algorithms based on the global objective framework for predicting missing entity types. Two algorithms use the linear model and the other one uses the embedding model. Linear Model The scoring function in this model T (e), where is given by s(e, t| = {wt }) = wt d wt  R e is the parameter vector for target type t. The regularization term in Eq. (1) is defined as T w . We use k = 2 in follows: R() = 1/2 t=1 wt t our experiments. Our first algorithm is obtained by using the dual coordinate descent algorithm (Hsieh et al., 2008) to optimize Eq. (1), where we modified the original algorithm to handle multiple weight vectors. We refer to this algorithm as Linear.DCD. While DCD algorithm ensures convergence to the global optimum solution, its convergence can be slow in certain cases. Therefore, we adopt an online algorithm, Adagrad (Duchi et al., 2011). We use the hinge loss function (k = 1) with no regularization (Reg () = ) since it gave best results in our initial experiments. We refer to this algorithm as Linear.Adagrad, which is described in Algorithm 1. Note that AdaGradUpdate(x, g ) is a procedure which updates the vector x with the respect to the gradient g . Embedding Model In this model, vector representations are constructed for entities and types using linear projection matrices. Recall (t)  Rdt is the feature function that maps a type to its feature representation. The scoring function is given by 520

s(e, t| = (U, V)) = (t)T VUT (e), where U  Rde ×d and V  Rdt ×d are projection matrices that embed the entities and types in a ddimensional space. Similarly to the linear classifier model, we use the l1-hinge loss function (k = 1) with no regularization (Reg () = ). Ui and Vi denote the i-th column vector of the matrix U and V, respectively. The algorithm is described in detail in Algorithm 2. The embedding model has more expressive power than the linear model, but the training unlike in the linear model, converges only to a local optimum solution since the objective function is non-convex. 4.3 Relationship to Existing Methods

Many existing methods for relation extraction and entity type prediction can be cast as a special case under the global objective framework. For example, we can consider the work in relation extraction (Mintz et al., 2009; Bordes et al., 2013; Riedel et al., 2013) as models trained with NT (e, t) = . These models are trained only using negative entities which we refer to as Negative Entity (NE) objective. The entity type prediction model in Ling and Weld (2012) is a linear model with NE (e, t) =  which

