matic transcription of the data presented in Table 1 is generated with the corresponding word and time boundaries that are aligned with the reference utterances. This allows us to compute the utterance WER and the features for the various prediction models. 4.3 Features

Our models are trained with the same 52 "blackbox" features proposed by Negri et al. (2014), which can be categorized in three groups: Signal, Hybrid and Textual. The first group aims to capture the difficulty to transcribe the input and is extracted by looking at the signal segment as a whole. Hybrid features provide a more fine-grained way to capture the transcription difficulty, by linking the signal to the output transcription. Textual features aim to capture the plausibility/fluency of a transcription considering its surface word information. 4.4 Evaluation Metrics

Classification. In this setting we also consider two baselines. The first one (Majority) is computed by labeling all the test instances with the most frequent label in the training set and, by definition, corresponds to a score of 0.5 in terms of balanced accuracy. The second classification baseline is the logistic regression (STL LogReg henceforth), also known as maximum entropy algorithm (Hastie et al., 2009). We perform parameter optimization for LogReg using stratified 5-fold cross-validation in a randomized search process (Bergstra and Bengio, 2012). For both STL baselines we selected algorithms7 that induce linear models and use the same loss functions (least squares for regression and logistic regression for classification) of the MTL methods.

5

Results and Discussion

Regression. Our regression models are evaluated in terms of mean absolute error (MAE). The MAE, a standard error measure for regression, is the average of the absolute difference between the prediction y ^i of a model and the gold standard response yi for all instances in the test set. As it is an error measure, lower values indicate better performance. Classification. To handle the imbalanced class distribution, and equally reward the correct classification on both classes, our evaluation is carried out in terms of balanced accuracy (BA ­ the higher the better), which is computed as the average of the accuracies on the two classes (Brodersen et al., 2010). When the distribution of classes is balanced, BA is equal to the accuracy metric. 4.5 Baselines

Regression. We compare the MTL methods against two baselines. The first one, simple but often hard to beat for regression models, is computed by labeling all the test instances with the Mean WER value calculated on the training set. The second baseline is an STL algorithm trained on data from the target domain. The algorithm that we used (STL Elastic henceforth) is the elastic net (Zou and Hastie, 2005). Parameter estimation is performed with 5fold cross-validation. 719

To mitigate the effect of having considerably different amounts of training data in the four domains, and equally weight their contribution to the learning task, all our models (STL and MTL) are trained using the same number of instances from all the domains and, at most, half of the data available for the smallest domain, News (i.e. 362 instances). To analyze performance variations with different amounts of data, we create subsets of the 362 instances, for 10 different sizes ranging from 10% to 100% of the instances for each domain.8 We repeat this process 30 times by randomly shuffling all the data available for each domain. For each of the resulting learning curves, the plots in this section present the confidence intervals9 (at 95%) for the 30 different train/test splits. In addition to the STL model trained only on indomain data, we also experiment with an STL model trained on the concatenation of the training data of all domains. Its results are, on average, statistically comparable to, or worse than, STL in-domain for both regression and classification. Regression. Among the three MTL regression algorithms, RMTL achieves the best results in all our
We used the implementations available in Scikit-learn (Pedregosa et al., 2011). 8 That is, for instance, with 10% of training data from four domains, the total amount of instances is 144 (36*4). 9 The confidence intervals are used to show whether there are statistically significant differences in performance among the models.
7

