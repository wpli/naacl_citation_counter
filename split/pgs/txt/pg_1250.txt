(a) src to piv and piv to tgt phrase tables. Translate the src test sentences to tgt using the sentence translation strategy and evaluate. (Column 2) (b) Triangulate the src-piv and piv-tgt phrase tables to get the src-piv-tgt phrase table. (Column 3) (c) Perform linear interpolation of the src-tgt and src-piv-tgt table using 9:1 weight ratio in equation 5 to get a combined table. (Column 4) (d) Perform linear interpolation of the src-tgt and src-piv-tgt table using the ratio of their BLEU scores as weights in equation 5 to get a combined table. (Column 5) (e) Perform fillup interpolation of the src-tgt (main) and src-piv-tgt table (secondary) to get a combined table. (Column 6) (f) Combine the src-tgt and src-piv-tgt phrase table using MDP (2 paths, 1 for direct and 1 for pivot). (Column 7) 3. Combine all the src-piv-tgt tables into a single table using linear (weights are ratios of BLEU scores) and fillup interpolation independently, giving the phrase tables: linear interp all and fill interp all respectively. Table 3, rows 4 and 5. 4. Perform linear interpolation of the src-tgt and linear interp all tables using 9:1 weight ratio in equation 5 to get a combined table. Table 3, row 6. 5. Perform linear interpolation of the src-tgt and all src-piv-tgt phrase tables using the ratio of their BLEU scores as weights in equation 5 to get a combined table. Table 3, row 7. 6. Perform fillup interpolation of the src-tgt and all src-piv-tgt phrase tables. The priority of the tables is given by the descending order of BLEU scores. Table 3, row 8. 7. Combine the linear interp all with the srctgt phrase table using MDP. Repeat this for fill interp all. Table 3, rows 9 and 10. 8. Combine all the src-piv-tgt phrase tables with the src-tgt phrase table using MDP (8 paths, 1 1196

for direct and 1 for each of the 7 pivots). Table 3, row 11. 9. Combine the top 3 pivot phrase tables with the src-piv-tgt phrase tables with the src-tgt phrase table using MDP (4 paths, 1 for direct and 1 for each of the 3 pivots). The pivot tables with the 3 highest8 standalone BLEU scores are selected. Table 3, row 12.

4

Results and Discussions

BLEU scores obtained after testing the tuned tables are reported. Scores in bold are statistically significant (p<0.05) over the baseline which is the system trained using a direct src-tgt parallel corpus. 4.1 Results The Japanese-Hindi direct translation system gave a BLEU of 33.86 whereas the Hindi-Japanese one gave 37.47. For the rest of the paper these will be the baselines, unless mentioned otherwise. The evaluation scores are split into 3 tables. Table 1 contains the scores for Japanese to Hindi (Table 2 for Hindi to Japanese) translation using each pivot separately and has 7 columns whose details are given in section 3.5 from 2.a to 2.f. Table 3 contains the scores for Japanese to Hindi (and vice versa) translation using all 7 pivots together in various ways. Each row is self explanatory. In row 6, we mean that the direct phrase table has a weight of 0.9 and the remainder 0.1 is distributed amongst the pivot phrase tables in the ratio of their standalone BLEU scores which can be seen in column 3 of tables 1 and 2. It is quite clear that sentence translation strategy is the most inferior technique. 4.2 Observations Below, we give an explanation of the observed scores from various points of views. 4.2.1 On the Pivots Used It is logical to consider that the closeness of a pivot language to the source or target is an important factor in the improvement of translation quality, since Korean helps Japanese-Hindi translation.
8 We chose 3 since our evaluation showed that the BLEU scores for the 3 pivot languages were much larger than the remaining ones.

