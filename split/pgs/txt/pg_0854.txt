lower than that of explicit discourse relations (Pitler et al., 2008). One potential method for reducing the data sparsity problem is through a distantly supervised learning paradigm, which is the direction we take in this work. Distant supervision approaches make use of prior knowledge or heuristics to cheaply obtain weakly labeled data, which potentially contain a small number of false labels. Weakly labeled data can be collected from unannotated data and incorporated in the model training process to supplement manually labeled data. For our task, we can collect instances of explicit discourse relations from unannotated data by some simple heuristics. After dropping the discourse connectives, we should be able to treat them as additional implicit discourse relations. The approach assumes that when the discourse connective is omitted, the discourse relation remains the same, which is a popular assumption in discourse analysis (Fraser, 2006; Schourup, 1999). This assumption turns out to be too strong in many cases as illustrated in (3): (3) [I want to go home for the holiday]Arg1 . Nonetheless, [I will book a flight to Hawaii]Arg2 . If "Nonetheless" is dropped in (3), one can no longer infer the C OMPARISON relation. Instead, one would naturally infer a C ONTINGENCY relation. Dropping the connective and adding the relation as a training sample adds noise to the training set and can only hurt the performance. In addition, certain types of explicit discourse relations have no corresponding implicit discourse relations. For example, discourse relations of the type C ONTINGENY.C ONDITION are almost always expressed with an explicit discourse connective and do not exist in implicit relations. We believe this also explains the lack of success in previous attempts to boost the performance of implicit discourse relation detection with this approach. (Biran and McKeown, 2013; Pitler et al., 2009). This suggests that in order for this approach to work, we need to identify instances of explicit discourse relations that closely match the characteristics of implicit discourse relations. In this paper, we propose two criteria for selecting such explicit discourse relation instances: omission rate and context differential. Our selection criteria 800

first classify discourse connectives by their distributional properties and suggest that not all discourse connectives are truly optional and not all implicit and explicit discourse relations are equivalent, contrary to commonly held beliefs in previous studies of discourse connectives. We show that only the freely omissible discourse connectives gather additional training instances that lead to significant performance gain against a strong baseline. Our approach improves the performance of implicit discourse relations without additional feature engineering in many settings and opens doors to more sophisticated models that require more training data. The rest of the paper is structured as follows. In Section 2, we describe the discourse connective selection criteria. In Section 3, we present our discourse connective classification method and experimental results that demonstrate its impact on inferring implicit discourse relations. We discuss related work and conclude our findings in Section 4 and 5 respectively.

2
2.1

Discourse Connective Classification and Discourse Relation Extraction
Datasets used for selection

We use two datasets for the purposes of extracting and selecting weakly labeled explicit discourse relation instances: the Penn Discourse Treebank 2.0 (Prasad et al., 2008) and the English Gigaword corpus version 3 (Graff et al., 2007). The Penn Discourse Treebank (PDTB) is the largest manually annotated corpus of discourse relations on top of one million word tokens from the Wall Street Journal (Prasad et al., 2008; Prasad et al., 2007). Each discourse relation in the PDTB is annotated with a semantic sense in the PDTB sense hierarchy, which has three levels: C LASS, T YPE and S UBTYPE. In this work, we are primarily concerned with the four top-level C LASS senses: E XPANSION, C OMPARISON, C ONTINGENCY, and T EMPORAL. The distribution of top-level senses of implicit discourse relations is shown in Table 2. The spans of text that participate in the discourse relation are also explicitly annotated. These are called A RG 1 or A RG 2, depending on its relationship with the discourse connective. The PDTB is our corpus of choice for its lexical

