Input: parameter , sentence x Output: segmentations s, POS tags t and dependency tree y
1: 2: 3: 4: 5: 6: 7: 8: 9: 10:

!
Xinhua News Agency

!
Beijing

!
February 13th

!
report

s  SampleSeg (x) t(0)  SampleP os(x, s) y (0)  SampleT ree(x, s, t(0) ) k=0 repeat t(k+1)  HillClimbP OS (x, s, t(k) , y (k) ) y (k+1)  HillClimbT ree(x, s, t(k+1) , y (k) ) k k+1 until no change in this iteration return (s, t(k) , y (k) )

!
Xinhua

!
society

!
February

!
13th

Figure 3: Example lattice structures for the Chinese sentence "         " (Xinhua Press at Beijing reports on February 13th). The token    has two candidate segmentations:   or  + . words with spaces, and thus require word segmentation. The main difference lies in the construction of the lattice structure. We employ a state-of-the-art word segmenter to produce candidate word boundaries. We consider boundaries common across all the top-k candidates as true word boundaries. The remaining tokens (i.e., strings between these boundaries) are treated as words to be further segmented and labeled with POS tags. Figure 3 shows an example of the Chinese word lattice structure we construct. Once the lattice is constructed, the joint prediction model is applied as described above.

Figure 2: The hill-climbing algorithm with random initializations. Details of the sampling and hillclimbing functions in Line 1-3 and 6-7 are provided in Section 3.2. HillClimbTree: We improve the dependency tree y via a similar hill-climbing process. Specifically, we greedily update the head yi,j of each morpheme in a bottom-up order as follows yi,j  arg max score(x, s, t, yi,j , y-(i,j ) )
yi,j Yi,j

(4)

4

Features

where Yi,j is the set of candidate heads such that changing yi,j to any candidate does not violate the tree constraint. 3.3 Training

We learn the parameters  in a max-margin framework, using on-line updates. For each update, we need to compute the segmentations, POS tags and the tree that maximize the cost-augmented score: ~, y (~ s, t ~) = arg max {·f (x, s, t, y )+Err(s, t, y )} (5) where Err(s, t, y ) is the number of errors of (s, t, y ) ^, y against the ground truth (^ s, t ^). The parameters are then updated to guide the selection against the violation. This is done via standard passive-aggressive updates (Crammer et al., 2006). 3.4 Adapting to Chinese Joint Prediction
sS ,tT ,y Y

Segmentation Features For both Arabic and Chinese, each segmentation is represented by its score from the preprocessing system, and by the corresponding morphemes (or words in Chinese). Following previous work (Zhang and Clark, 2010), we also add character-based features for Chinese word segmentation, including the first and the last characters in the word, and the length of the word. POS Tag Features Table 1 summarizes the POS tag features employed by the model. First, we use the feature templates proposed in our previous work on Arabic joint parsing and POS correction (Zhang et al., 2014c). In addition, we incorporate character-based features specifically designed for Chinese. These features are mainly inspired by previous transition-based models on Chinese joint POS tagging and word segmentation (Zhang and Clark, 2010). Dependency Parsing Features The feature templates for dependency parsing are mainly drawn from our previous work (Zhang et al., 2014b). Fig-

In this section we describe how the proposed model can be adapted to languages that do not delineate 45

