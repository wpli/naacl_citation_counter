Initial State Final State

([ ], set(1..n), ) ([ ], , A) Induction Rules: (, , A) ([ |i],  - {i}, A) ([ |j i], , A) ([ |i], , A  {j  i}) ([ |j i], , A) ([ |j ], , A  {j  i})

Algorithm 1: transition-based linearization
Input: C , a set of input syntactic constraints Output: The highest-scored final state candidates  ([ ], set(1..n), ) agenda   for i  1..2n do for s in candidates do for action in G ET P OSSIBLE ACTIONS(s, C ) do agenda  A PPLY(s, action) candidates  T OP -K(agenda) agenda   best  B EST(candidates) return best

1 2 3 4 5 6 7 8 9 10

S HIFT-i-POS L EFTA RC R IGHTA RC

Figure 3: Deduction system for transition-based linearization. Indices i, j do not reflect word order.

3

Transition-Based Linearization

The main difference between linearization and dependency parsing is that the input words are unordered for linearization, which results in an unordered buffer . At a certain state s = (, , A), any word in the buffer  can be shifted onto the stack. In addition, unlike a parser, the vanilla linearization task does not assume that input words are assigned POS. To extend the arc-standard algorithm for linearization, we incorporate word and POS into the S HIFT operation, transforming the arc-standard S HIFT operation to S HIFT-Word-POS, which selects the word Word from the buffer , tags it with POS and shifts it onto the stack. Since the order of words in an output sentence equals to the order in which they are shifted onto the stack, word ordering is performed along with the parsing process. Under such extension, the sentence in Figure 2 can be generated by the transition sequence (S HIFT-Dr. Talcott-NP, S HIFT-led-VBD, S HIFTof-NP, S HIFT-a team-NP, S HIFT-of-IN, S HIFTHarvard University-NP, R IGHTA RC, R IGHTA RC, R IGHTA RC, S HIFT-.-., R IGHTA RC, L EFTA RC), given the unordered bag of words (Dr. Talcott, led, a team, of, Harvard University, .). The deduction system for the linearization algorithm is shown in Figure 3. Given an input bag of n words, this algorithm also takes 2n transition actions to construct an output, by the same reason as the arc-standard parser. 3.1 Search and Learning We apply the learning and search framework of Zhang and Clark (2011a), which gives state-of-the-

art transition-based parsing accuracies and runs in linear time (Zhang and Nivre, 2011). Pseudocode of the search algorithm is shown in Algorithm 1. It performs beam-search by using an agenda to keep the k-best states at each incremental step. When decoding starts, the agenda contains only the initial state. At each step, each state in the agenda is advanced by applying all possible transition actions (G ET P OSSI BLE ACTIONS ), leading to a set of new states. The k best are selected for the new states, and used to replace the current states in the agenda, before the next decoding step starts. Given an input bag of n words, the process repeats for 2n steps, after which all the states in the agenda are terminal states, and the highest-scored state in the agenda is taken for the final output. The complexity of this algorithm is n2 , because it takes a fixed 2n steps to construct an output, and in each step the number of possible S HIFT action is proportional to the size of . The search algorithm ranks search hypotheses, which are sequences of state transitions, by their scores. A global linear model is used to score search hypotheses. Given a hypothesis h, its score is calculated by:  Score(h) = (h) · ,  is the parameter vector of the model and where  (h) is the global feature vector of h, extracted by instantiating the feature templates in Table 2 according to each state in the transition sequence. In the table, S0 represents the first word on the top of the stack, S1 represents the second word on the top of the stack, w represents a word and p rep-

115

