phrase predictions for this baseline, denoted w2v out , were computed as the words most similar to the target word based on dense vector Cosine similarities, ignoring the context (out-of-context). In the best subtasks we used only the top ranked lemmatized paraphrase (best prediction) suggested by each of the compared methods. In the oot subtasks we used the top-10 lemmatized paraphrases. To the best of our knowledge, Biemann and Riedl (2013) is the only prior work that attempted to perform the original SemEval 2007 task on the LS07 dataset, learning only from corpus data like we do. They merged Gigaword (Parker et al., 2011) and LLC (Richter et al., 2006) as their learning corpus, which is similar in size to ours. We denote by Biemannin and Biemannout the reported results for their in-context and out-of-context methods, respectively. There is no previously reported result for this task on LS14. 6.2.3 Results The results are shown in Table 5. First, we note that our out-of-context method significantly outperforms the out-of-context word2vec baseline on all subtasks in both LS07 and LS14, showing that our model performs well on predicting paraphrases even out-of-context. Furthermore, our meaning incontext methods show significant additional gains in performance on LS07, with top-1000 pruning performing a little better than top-100. On LS14 we see smaller gains, which may be due to the fact that its target words are less ambiguous by construction. This behavior is consistent with similar findings in Kremer et al. (2014). Finally, both Biemannout and Biemannin exhibit substantially lower performance on LS07 than our methods, achieving scores that are close to the word2vec baseline. We note that all ten systems that participated in the original SemEval 2007 task on the LS07 dataset followed a two-step scheme (1) generating paraphrase candidates using a manually constructed thesaurus, such as WordNet; (2) ranking the candidates according to the given context based on data from various learning corpora. We stress that in contrast to all these systems, our model does not utilize manually constructed thesauri, and therefore addresses a much harder problem of predicting paraphrase substitutes out of the entire vocabulary, rather 478

Method
in P1000 in P100 P out out w2vskip, 4w Biemannin Biemannout in P1000 in P100 P out out w2vskip, 4w

best best-m LS07 test-set 12.72 21.71 12.25 20.73 10.68 18.29 8.25 13.41 n/a n/a n/a n/a LS14 all 8.07 17.37 7.93 16.97 7.80 16.90 5.99 12.21

oot 36.37 35.54 32.58 29.27 27.48 27.02 26.67 26.24 25.57 22.66

oot-m 52.03 50.98 46.34 39.92 37.19 37.35 46.23 45.58 44.66 36.98

Table 5: best and oot subtasks scores for all compared methods. best-m and oot-m stand for the mode scores.

than merely ranking a small given set of candidates. Even so, in the best subtasks we achieve top results with respect to the reported score range of these systems, 2.98-12.90 for best, and 4.72-20.73 for bestmode. In comparison to the reported oot score range, our results are lower than average. 6.3 6.3.1 Ranking lexical substitutions Task description

Most works that used the LS07 dataset after SemEval 2007, as well as the results reported for LS14, focused only on candidate ranking. Instead of using a thesaurus, they obtained the set of paraphrase candidates for each target type by pooling the annotated gold-standard paraphrases from all of its instances.9 The quality of the rankings with respect to the gold standard was measured using Generalized Average Precision (GAP) (Kishida, 2005). Furthermore, all of the works compared in this section discarded multi-word expression substitutes from the original gold standard, and omitted instances who thus remained with no gold paraphrases. We follow the same evaluation settings for this task. 6.3.2 Compared methods We observe that in this task, the objective is to rank candidates that are known to be semantically similar to the target word in some context. Therefore, we hypothesize that possibly more focus should be given in this case to assessing the
A target type is defined as the pair (word lemma, pos), where pos  {noun, verb, adjective, adverb}.
9

