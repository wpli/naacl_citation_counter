First, the lengths of the source and target utterances are not correlated in the conversational setting, and there is hardly any general tendency towards the relative length of the utterances, as shown in the example conversation below. SMT usually works on the data in which the ratio between the lengths of the source and target utterances stays relatively constant (Och and Ney, 2000). However, conversational setting, in which such constant ratio is absent, jeopardizes the functionality of the usual SMT models to make alignments. Although it is highly probable that some of the semantic elements in the source utterance are reflected in the target utterance, it is rarely on a one-to-one basis. A: Is something going on today? (S1 ) B: Of course, it's dad's birthday. (S2 ) A (most recent stimulus) : What?! (S3 ) B (target) : Oh, you didn't know? (S4 ) Second, it cannot take into account what was previously discussed in the conversation. Unless the most recent utterance brings a completely new topic, or it has sufficient information in itself, such problem is evident. Both problems regarding the context and the alignment become more pronounced especially in cases where the source utterance is short as shown in the above example. Clearly, no meaningful response can be derived from the most recent stimulus alone, and it is highly unclear how the alignments should be made. Indeed, the response generated by applying SMT to the most recent stimulus "What?" in this example is "that," which only mimics the syntactic structure but fails to deliver any meaningful content.

3
3.1

Context-Dependent Model
Overview

In order to deal with the issues of context and the lengths of the utterances without correlation, we work on building a context-dependent model, in which we balance the utterance lengths by selecting contextually important words from the previous part of the conversation, and adding them to the source utterance. For example, applying one of our models to the most recent stimulus (S3 ) of the previous example conversation results in the following utterance, where the words in the parenthesis are newly added: 1346

A: (today birthday) What?! The rationale behind this approach is that the topic of a conversation can be characterized by a number of contextually important words, which provide semantic information to be reflected in the response generation process. This approach seemingly reduces the grammatical integrity of the source utterance, and it may seem as if we risk confusing the translation model and losing grammaticality of the output. However, grammaticality of the output is handled by the language model, and the language model is constructed upon the target language only, which in our case corresponds to the target utterances that remain untouched. Also, the newly added words are of high relevance to the topic, so the new source utterance frequently demonstrates high semantic coherence both within itself, and in parallel with the target utterance. The question now is how to determine which words are contextually important throughout the conversation. Since finding such contextually important words is our main concern, we find simple statistical significance test models more suitable than conventional methods from discourse modeling or dialogue systems (Oh et al., 2002). We examine two approaches, namely the pair-based approach, and the token-based approach. The pairbased approach uses Fisher's Exact Test (Moore, 2004), which is reported to give more accurate pvalues than 2 or G2 when the counts are small (Ritter et al., 2011). This approach takes advantage of the proximity of utterances, and assumes that a utterance whose distance to the source utterance is shorter is likely to be more contextually related to the source utterance, i.e. Sn-1 is more likely than Sn-2 to be semantically relevant to Sn . The tokenbased approach considers at the entire conversation, and selects the words most characteristic of the conversation, using the most widely used term weighting algorithm, tf-idf. 3.2 Pair-Based Model Given a conversation consisting of utterances S1 ,...,Sn+1 , where Sn is the source utterance and Sn+1 is the target utterance, we start by computing the p-value from Fisher's Exact Test for every possible word pair between Sn and Sn-1 . If the p-value

