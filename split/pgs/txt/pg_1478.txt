themselves. In the context of reranking (see Section 3), it means we can learn classifiers if we can calculate dot products K (y1 , y2 ) = T (x1 , y1 ) ·  (x2 , y2 ) for two sentences and candidate parses.2 We first note that such dot products can be expressed as sum of dot products over parts: K (y1 , y2 ) =
py1 p y2

k p, p

Features: Our feature vector will have two parts. One, g (x, y )  Rd1 , consists of features obtained from manually constructed templates. The other, k (x, y )  Rd2 , corresponds to our kernel features. We will not evaluate or store it, but rather use the kernel trick for implicitly learning with it, as explained below. The score of a candidate parse y for sentence x is calculated via the following linear function:  (x, y ) = [g (x, y ) , k (x, y )] hv (x, y ) = v ·  (x, y ) (2) Learning For learning we use the passiveaggressive algorithm (Crammer et al., 2006; McDonald et al., 2005a), and adapt it to use with kernels. Formally, let S = {(xi , K (xi ))}n i=1 be a training set of size n such that K (xi ) = {yi1 , . . . , yik } is the set of k-best candidate trees produced for the sentence xi . Assume that yi1 is the optimal tree in terms of Hamming distance to the gold tree. A key observation to make is that the v generated by the PA algorithm will depend on two parameters. One is a weight vector w  Rd1 , in the manually constructed g feature space. The other is a set of weights ij with i = 1, . . . , n and j = 1, . . . , k corresponding to the j th candidate for the ith sample.4 The score is then given by: fw, (x, y ) = v ·  (x, y ) = w · g (x, y )+ f (x, y ) where: f (x, y ) =
i,j

where k (p, p ) =  (x1 , p) ·  (x2 , p). To calculate k (p, p ) we'll assume for simplicity that p and p are of the same type (otherwise k (p, p ) = 0). Let pij and pij be the values of the i'th property in the j 'th slot in p, p (e.g., for a second order sibling part as in Figure 1, p1,4 will correspond to the label of the edge e2 in p) , and let Cpp  {0, 1}m×s be a binary matrix comparing p and p such that Cpp ij = 1 when pij = pij and 0 otherwise. Simple algebra yields that: k p, p =
j

1T · Cpp

:,j

That is, calculating k (p, p ) amounts to multiplying the sums of the columns in C .3 The runtime of k (p, p ) is then O (m × s) which means the overall runtime of K (y1 , y2 ) is O (|y1 | × |y2 | × |s| × |m|), where |y1 | , |y2 | are the number of parts in y1 and y2 . Finally, note that adding 1 to one of the column counts of C corresponds to a slot that can be skipped to produce a partial template (this simulates a wild card property that is always on).

ij · (K (yi1 , y ) - K (yij , y ))

3

Kernel Reranker

We next show how to use the template kernels within a reranker. In the reranking approach (Collins and Koo, 2005; Charniak and Johnson, 2005), a base parser produces a list of k-best candidate parses for an input sentence and a separately trained reranking model is used to select the best one.
2 For brevity we'll omit x from the kernel parameters and use K (y1 , y2 ) instead of K ((x1 , y1 ), (x2 , y2 )). 3 We omit the proof, but intuitively, the product of column sums is equal to the number of 1 valued paths between elements in the different columns of C . Each such path corresponds to a path in p and p where all the properties have identical values. i.e. it corresponds to a feature that is active in both (x1 , p) and (x2 , p ) and thus contributes 1 towards the dot product.

We can now rewrite the updates of the PA algorithm using w, , as described in Alg 1.5

4

Implementation

The classifier depends on parameters ij , which are updated using the PA algorithm. In the worst case, all nk of these may be non-zero. For large datasets, this may slow down both learning and prediction.
4 This follows from tracing the steps of PA and noting their dependence on dot products. 5 The denominator in line 5 is equal to g (xi , yij ) - g (xi , yi1 ) 2 + K (yij , yij ) - 2K (yij , yi1 )+ K (yi1 , yi1 ) so it can be calculated efficiently using the kernel. yi1 - yij 1 is the hamming distance between yi1 and yij . The updates for  ¯ are equivalent to averaging over all alphas in iterations 1, ..., T . We use this form to save space.

1424

