Table 2: Topics Learned from 20-Newsgroups Dataset Topic 1 (Crime) gun guns weapons control firearms crime police com weapon used Topic 1 (Crime) gun guns crime police weapons firearms criminal criminals people law LDA Topic 2 Topic 3 (Sex) (Sports) sex team men game homosexuality hockey homosexual season gay will sexual year com play homosexuals nhl people games cramer teams Quad-LDA Topic 2 Topic 3 (Sex) (Sports) homosexuality game sex team homosexual play sin games marriage hockey context season people rom sexual period gay goal homosexuals player Topic 4 (Health) government money private people will health tax care insurance program Topic 4 (Health) money insurance columbia pay health tax year private care write Topic 1 (Crime) gun police carry kill killed weapon cops warrant deaths control Topic 1 (Crime) gun guns weapons child police control kill deaths death people DF-LDA Topic 2 Topic 3 (Sex) (Sports) book game men games books players homosexual hockey homosexuality baseball reference fan gay league read played male season homosexuals ball MRF-LDA Topic 2 Topic 3 (Sex) (Sports) men game sex team women hockey homosexual players homosexuality play child player ass fans sexual teams gay fan homosexuals best Topic 4 (Health) money pay insurance policy tax companies today plan health jobs Topic 4 (Health) care insurance private cost health costs company companies tax public

quently, the learned topics are of high coherence. As shown in Table 2, the topics learned by our method are largely better than those learned by the baseline methods. The topics are of high coherence and contain fewer noise and irrelevant words. Our method provides a mechanism to automatically decide under which topic, two words labeled as similar are truly similar. The decision is made flexibly by data according to their fitness to the model, rather than by a hard rule adopted by DF-LDA and Quad-LDA. For instance, according to the external knowledge, the word child is correlated with gun and with men simultaneously. Under a crime topic, child and gun are truly correlated because they cooccur a lot in youth crime news, whereas, child and men are less correlated in this topic. Under a sex topic, child and men are truly correlated whereas child and gun are not. Our method can differentiate this subtlety and successfully put child and gun into the crime topic and put child and men into the sex topic. This is because our method encourages child 731

and gun to be put into the same topic A and encourages child and men to be put into the same topic B , but does not require A and B to be the same. A and B are freely decided by data. Table 3 shows some topics learned on NIPS dataset. The four topics correspond to vision, neural network, speech recognition and electronic circuits respectively. From this table, we observe that the topics learned by our method are better in coherence than those learned from the baseline methods, which again demonstrates the effectiveness of our model. 4.2.2 Quantitative Evaluation

We also evaluate our method in a quantitative manner. Similar to (Xie and Xing, 2013), we use the coherence measure (CM) to assess how coherent the learned topics are. For each topic, we pick up the top 10 candidate words and ask human annotators to judge whether they are relevant to the topic. First, annotators needs to judge whether a topic is interpretable or not. If not, the ten candidate words in this

