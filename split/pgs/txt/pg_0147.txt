Not All Character N -grams Are Created Equal: A Study in Authorship Attribution
Upendra Sapkota and Steven Bethard The University of Alabama at Birmingham 1300 University Boulevard Birmingham, AL 35294, USA {upendra,bethard}@cis.uab.edu Manuel Montes-y-G´ omez Instituto Nacional de Astrof´ isica Optica y Electr´ onica Puebla, Mexico mmontesg@ccc.inaoep.mx

Thamar Solorio University of Houston 4800 Calhoun Rd Houston, TX, 77004, USA solorio@cs.uh.edu Abstract
Character n-grams have been identified as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood. We identify subgroups of character n-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style. We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and a cross-domain setting where multiple topics are present. We demonstrate that character ngrams that capture information about affixes and punctuation account for almost all of the power of character n-grams as features. Our study contributes new insights into the use of n-grams for future AA work and other classification tasks.

tential candidate authors have an important effect on the accuracy of AA approaches (Moore, 2001; Luyckx and Daelemans, 2008; Luyckx and Daelemans, 2010). We can also point out the most common features that have been used successfully in AA work, including: bag-of-words (Madigan et al., 2005; Stamatatos, 2006), stylistic features (Zheng et al., 2006; Stamatatos et al., 2000), and word and character level n-grams (Kjell et al., 1994; Keselj et al., 2003; Peng et al., 2003; Juola, 2006). The utility of bag-of-words features is well understood: they effectively capture correlations between authors and topics (Madigan et al., 2005; Kaster et al., 2005). The discriminative value of these features is thus directly related to the level of content divergence among authors and among train and test sets. The utility of stylistic features is also well understood: they model author preferences for the use of punctuation marks, emoticons, white spaces, and other traces of writing style. Such preferences are less influenced by topic, and directly reflect some of the unique writing patterns of an author. Character n-grams are the single most successful feature in authorship attribution (Koppel et al., 2009; Frantzeskou et al., 2007; Koppel et al., 2011), but the reason for their success is not well understood. One hypothesis is that character n-grams carry a little bit of everything: lexical content, syntactic content, and even style by means of punctuation and white spaces (Koppel et al., 2011). While this argument seems plausible, it falls short of a rigorous explanation. In this paper, we investigate what in the make-up

1

Introduction

Authorship Attribution (AA) tackles the problem of determining who, among a set of authors, wrote the document at hand. AA has relevant applications ranging from plagiarism detection (Stamatatos, 2011) to Forensic Linguistics, such as identifying authorship of threatening emails or malicious code. Applied areas such as law and journalism can also benefit from authorship attribution, where identifying the true author of a piece of text (such as a ransom note) may help save lives or catch the offenders. We know from state of the art research in AA that the length of the documents and the number of po93

Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 93­102, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

