Input: constrained lattices {(x(i) , y ~(i) )}N i=1 , step size  Output: HUCRF parameters  := {,  } 1. Initialize  randomly. 2. Repeatedly select i  {1 . . . N } at random:
 (i)

units in a meaningful way.

3

Mining Click Log Data

We propose using search click logs which consist of queries and their corresponding web documents. (a) y  arg maxyY (x(i) ) p (y |x ) Clicks are an implicit signal for related entities and (b) If y   Y (x(i) , y ~(i) ): information in the searched document. In this work, (i) i. y +  arg maxyY (x(i) ,y ~(i) ) p (y |x ) we will assume that the web document is structured ii. Make parameter updates: and generated from an underlying database. Due to the structured nature of the web, this is not an  +× p (y + , z + |x(i) )- unrealistic assumption (see Adamic and Huberman  (2002) for discussion). Such structural regularities p (y  , z  |x(i) ) make obtaining annotated queries for learning a semantic slot tagger almost cost-free. where the following hidden units are comAs an illustration of how to project annotation, puted in closed-form (see Gelfand et al. consider Figure 3, where we present an example (2010)): taken from queries about video games. In the figz + := arg max p (z |x(i) , y + ) ure, the user queries are connected to a structured z document via a click log, and then the document is z  := arg max p (z |x(i) , y  ) parsed and stored in a structured format. Then annoz tation types are projected to linked queries through structural alignment. In the following subsections Figure 2: A sketch of the perceptron training algorithm we describe each step in our log mining approach in for a partially observed hidden unit CRF. detail. strained lattice. We treat this as the gold label sequence, and perform the perceptron updates accordingly (Gelfand et al., 2010). Even though this training algorithm is quite simple, we demonstrate its effectiveness in our experiments. 2.2.2 Initialization from unlabeled data Rather than initializing the model parameters randomly, we propose an effective initialization scheme (in a similar spirit to the pre-training methods in neural networks) that naturally leverages unlabeled data. First, we cluster observation types in unlabeled data and treat the clusters as labels. Then we train a fully supervised HUCRF on this clustered data to learn parameters  for the interaction between observations and hidden units (x, z ) and  for the interaction between hidden units and labels (z, y ). Finally, for task/domain specific training, we discard  and use the learned  to initialize the algorithm in Figure 2. We hypothesize that if the clusters are nontrivially correlated to the actual labels, we can capture the interaction between observations and hidden 87 3.1 Click Logs

Web search engines keep a record of search queries, clicked document and URLs which reveal the user behavior. Such records are proven to be useful in improving the quality of web search. We focus on utilizing query-to-URL click logs that are essentially a mapping from queries to structured web documents. In this work, we use a year's worth of query logs (from July 2013 to June 2014) at a commercial search engine. We applied a simple URL normalization procedure to our log data including trimming and removal of prefixes, e.g. "www". 3.2 Parsing Structured Web Document

A simple wrapper induction algorithm described in Kushmerick (1997) is applied for parsing web documents. Although it involves manually engineering a rule-based parser and is therefore website-specific, a single wrapper often generates large amounts of data for large structured websites, for example IMDB. Furthermore, it is very scalable to large quantities of data, and the cost of writing such a rule-based sys-

