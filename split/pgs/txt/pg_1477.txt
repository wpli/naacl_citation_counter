2 Template Kernels
For simplicity, we begin with the case where parts p correspond to head modifier pairs (h, m) (i.e. all parts belong to the "arc" part type). The features in (x, p) can then depend on any property of h, m and the sentence x. We denote properties related to h using the prefix h- (e.g., h-pos corresponds to the partof-speech of the head), and similarly for m-. We also use e- to denote properties related to the triplets h, m, x (e.g., the surface distance between h, m in x is denoted by e-dist). Templates defined over the "arc" part type will then combine different properties of h, m and e, to create features. e.g. the template <h-form=?; e-dist=?; m-form=?,m-pos=?>, when applied to a dependency arc, may yield the feature: <hform=dog;e-dist=1;m-form=black,m-pos=JJ>. More generally, a parse tree part can be seen as ordered lists of slots that contain properties (different part types will contain different lists of slots). The feature templates defined over them select one property from each slot (possibly skipping some slots to produce partial templates). A template can thus be thought of as a directed path between the properties it selects in the different slots. Clearly, the number of possible templates in a given part type is exponential in the number of its slots. Figure 1 depicts the process for sibling parts. As discussed in Section 1, manually constructed feature templates consider only a small subset of the combinations of properties (i.e. a small number of "good" paths is manually identified and selected). Our goal is to introduce a kernel that allows us to represent all possible paths for a part type in polynomial time. Formally, let  (x, y ) =  (x, p) be a feature representation which associates a feature with any distinct combination of properties in any of the tree parts in the training set. For a given part p, the effective dimensionality of  (x, p) is thus O (ms ) where s is the number of slots in p, and m is the maximal number of properties in a slot. Explicitly representing  (x, y ) is therefore often impractical. However, the well known "kernel trick" (Shawe-Taylor and Cristianini, 2004) implies that linear classifiers depend only on dot products between feature vectors and not on the feature vectors
p y

Figure 1: Feature template over the second order consecutive siblings part type. The part type contains slots for the head (h), sibling (s) and modifier (m) words, as well as for the two edges (e1 and e2). Each slot is associated with a set of properties. The directed path skips over the edge properties and defines the partial template <h-cpos=?; s-cpos=?; m-gender=?>.

et al., 2006). While training greedy transition-based parsers such as the ones used in (Chen and Manning, 2014) and (Hall et al., 2006) amounts to training a multiclass classifier, the graph-based parsing framework explored in (Lei et al., 2014) and in the present work is a more involved structured-learning task. In this paper we present a kernel based approach to automated feature generation in the context of graph-based parsing. Compared to tensors and neural networks, kernel methods have the attractive properties of a convex objective and well understood generalization bounds (Shawe-Taylor and Cristianini, 2004). We introduce a kernel that allows us to learn the parameters for a representation similar to the tensor representation in (Lei et al., 2014) but without the low rank assumption, and without explicitly instantiating the exponentially many possible features. In contrast to previous works on parsing with kernels (Collins and Duffy, 2002), in which the kernels are defined over trees and count the number of shared subtrees, our focus is on feature combinations. In that sense our work is more closely related to work on tree kernels for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004; Reichartz et al., 2010; Sun and Han, 2014), but the kernel we propose is designed to generate combinations of properties within selected part types and does not involve the all-subtrees representation. 1423

