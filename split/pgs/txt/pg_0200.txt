This encodes our assumption that the video follows the same ordering as the recipe and that background/foreground tokens tend to cluster together. Obviously these assumptions do not always hold, but they are a reasonable approximation. For each recipe, we set  = K/T , the ratio of recipe steps to transcript tokens. This setting corresponds to an a priori belief that each recipe step is aligned with the same number of transcript tokens. The parameter  in our experiments is set by crossvalidation to 0.7 based on a small set of manuallylabeled recipes. For the foreground observation model, we generate the observed word from the corresponding recipe step via:
log p(Y (t) = y |R(t) = k, X (1 : K ), B (t) = 0)  max({WordSimilarity(y, x) : x  X (k)}),

(Yu et al., 2014) filters ASR transcripts by part-ofspeech tag and finds tokens that match a small vocabulary to create a corpus of video clips (extracted from instructional videos), each labeled with an action/object pair. In more detail, we manually define a whitelist of 200 actions (all transitive verbs) of interest, such as "add", "chop", "fry", etc. We then identify when these words are spoken (relying on the POS tags to filter out non-verbs), and extract an 8 second video clip around this timestamp. (Using 2 seconds prior to the action being mentioned, and 6 seconds following.) To extract the object, we take all tokens tagged as "noun" within 5 tokens after the action. 3.3 Hybrid HMM + keyword spotting We cannot use keyword spotting if the goal is to align instructional text to videos. However, if our goal is just to create a labeled corpus of video clips, keyword spotting is a reasonable approach. Unfortunately, we noticed that the quality of the labels (especially the object labels) generated by keyword spotting was not very high, due to errors in the ASR. On the other hand, we also noticed that the recall of the HMM approach was about 5 times lower than using keyword spotting, and furthermore, that the temporal localization accuracy was sometimes worse. To get the best of both worlds, we employ the following hybrid technique. We perform keyword spotting for the action in the ASR transcript as before, but use the HMM alignment to infer the corresponding object. To avoid false positives, we only use the output of the HMM for this video if at least half of the recipe steps are aligned by it to the speech transcript; otherwise we back off to the baseline approach of extracting the noun phrase from the ASR transcript in the window after the verb. 3.4 Temporal refinement using vision In our experiments, we noticed that sometimes the narrator describes an action before actually performing it (this was also noted in (Yu et al., 2014)). To partially combat this problem, we used computer vision to refine candidate video segments as follows. We first trained visual detectors for a large collection of food items (described below). Then, given a candidate video segment annotated with an action/object pair (coming from any of the previous

where X (k ) is the set of words in the k 'th recipe step, and WordSimilarity(s, t) is a measure of similarity between words s and t, based on word vector distance. If this frame is aligned to the background, we generate it from the empirical distribution of words, which is estimated based on pooling all the data: p(Y (t) = y |R(t) = k, B (t) = 1) = p ^(y ). Finally, the prior for p(B (t)) is uniform, and p(R(1)) is set to a delta function on R(1) = 1 (i.e., we assume videos start at step 1 of the recipe). Having defined the model, we "flatten" it to a standard HMM (by taking the cross product of Rt and Bt ), then estimate the MAP sequence using the Viterbi algorithm. See Figure 2 for an example. Finally, we label each segment of the video as follows: use the segmentation induced by the alignment, and extract the action and object from the corresponding recipe step as described in Section 2.2. If the segment was labeled as background by the HMM, we do not apply any label to it. 3.2 Keyword spotting

A simpler approach to labeling video segments is to just search for verbs in the ASR transcript, and then to extract a fixed-sized window around the timestamp where the keyword occurred. We call this approach "keyword spotting". A similar method from 146

