larity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.5K pairs), e.g., pickles are similar to onions, as well as visual similarity (VisSim, Silberer and Lapata (2014), same pairs as SemSim with different human ratings), e.g., pickles look like zucchinis. Alternative Multimodal Models We compare our models against several recent alternatives. We test the vectors made available by Kiela and Bottou (2014). Similarly to us, they derive textual features with the skip-gram model (from a portion of the Wikipedia and the British National Corpus) and use visual representations extracted from the ESP dataset (von Ahn and Dabbish, 2004) through a convolutional neural network (Oquab et al., 2014). They concatenate textual and visual features after normalizing to unit length and centering to zero mean. We also test the vectors that performed best in the evaluation of Bruni et al. (2014), based on textual features extracted from a 3B-token corpus and SIFT-based Bag-of-Visual-Words visual features (Sivic and Zisserman, 2003) extracted from the ESP collection. Bruni and colleagues fuse a weighted concatenation of the two components through SVD. We further reimplement both methods with our own textual and visual embeddings as C ONCATENATION and SVD (with target dimensionality 300, picked without tuning). Finally, we present for comparison the results on SemSim and VisSim reported by Silberer and Lapata (2014), obtained with a stacked-autoencoders architecture run on textual features extracted from Wikipedia with the Strudel algorithm (Baroni et al., 2010) and attribute-based visual features (Farhadi et al., 2009) extracted from ImageNet. All benchmarks contain a fair amount of words for which we did not use direct visual evidence. We are interested in assessing the models both in terms of how they fuse linguistic and visual evidence when they are both available, and for their robustness in lack of full visual coverage. We thus evaluate them in two settings. The visual-coverage columns of Table 1 (those on the right) report results on the subsets for which all compared models have access to direct visual information for both words. We further report results on the full sets ("100%" columns of Table 1) for models that can propagate visual information and that, consequently, can meaningfully be tested 157

on words without direct visual representations. Results The state-of-the-art visual CNN FEA TURES alone perform remarkably well, outperforming the purely textual model (S KIP - GRAM) in two tasks, and achieving the best absolute performance on the visual-coverage subset of Simlex-999. Regarding multimodal fusion (that is, focusing on the visual-coverage subsets), both MMS KIP - GRAM models perform very well, at the top or just below it on all tasks, with comparable results for the two variants. Their performance is also good on the full data sets, where they consistently outperform S KIP - GRAM and SVD (that is much more strongly affected by lack of complete visual information). They're just a few points below the state-of-the-art MEN correlation (0.8), achieved by Baroni et al. (2014) with a corpus 3 larger than ours and extensive tuning. MMS KIP - GRAM -B is close to the state of the art for Simlex-999, reported by the resource creators to be at 0.41 (Hill et al., 2014). Most impressively, MMS KIP - GRAM -A reaches the performance level of the Silberer and Lapata (2014) model on their SemSim and VisSim data sets, despite the fact that the latter has full visual-data coverage and uses attribute-based image representations, requiring supervised learning of attribute classifiers, that achieve performance in the semantic tasks comparable or higher than that of our CNN features (see Table 3 in Silberer and Lapata (2014)). Finally, if the multimodal models (unsurprisingly) bring about a large performance gain over the purely linguistic model on visual similarity, the improvement is consistently large also for the other benchmarks, confirming that multimodality leads to better semantic models in general, that can help in capturing different types of similarity (general relatedness, strictly taxonomic, perceptual). While we defer to further work a better understanding of the relation between multimodal grounding and different similarity relations, Table 2 provides qualitative insights on how injecting visual information changes the structure of semantic space. The top S KIP - GRAM neighbours of donuts are places where you might encounter them, whereas the multimodal models relate them to other take-away food, ranking visually-similar pizzas at the top. The owl example shows how multimodal

