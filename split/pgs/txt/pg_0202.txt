score for that frame over all detectors whose names matched any of the ingredient tokens (after lemmatization and stopword filtering). Finally, the match score of a video segment to an object is computed by taking the average score of all frames within that segment. By then scoring and maximizing over all translations of the candidate segment (of up to three seconds away), we produce a final "refined" segment. 3.5 Quantifying confidence via vision and affordances

The output of the keyword spotting and/or HMM systems is an (action, object) label assigned to certain video clips. In order to estimate how much confidence we have in that label (so that we can trade off precision and recall), we use a linear combination of two quantities: (1) the final match score produced by the visual refinement pipeline, which measures the visibility of the object in the given video segment, and (2) an affordance probability, measuring the probability that o appears as a direct object of a. The affordance model allows us to, for example, prioritize a segment labeled as (peel, garlic) over a segment labeled as (peel, sugar). The probabilities P (object = o|action = a) are estimated by first forming an inverse document frequency matrix capturing action/object co-occurrences (treating actions as documents). To generalize across actions and objects we form a low-rank approximation to this IDF matrix using a singular value decomposition and set affordance probabilities to be proportional to exponentiated entries of the resulting matrix. Figure 3 visualizes these affordance probabilities for a selected subset of frequently used action/object pairs.

Figure 3: Visualization of affordance model. Entries (a, o) are
colored according to P (object = o | action = a).
2

1.5

Score

1
Visual Refinement + recipe Visual Refinement + recipe Visual Refinement (alone) Visual Refinement (alone)

Hybrid (manual)

KW (manual)

Hybrid

0 Actions Objects

Figure 4: Clip quality, as assessed by Mechanical Turk exper-

iments on 900 trials. Figure best viewed in color; see text for details.

4

Evaluation and applications

In this section, we experimentally evaluate how well our methods work. We then briefly demonstrate some prototype applications. 4.1 Evaluating the clip database One of the main outcomes of our process is a set of video clips, each of which is labeled with a verb (action) and a noun (object). We generated 3 such labeled corpora, using 3 different methods: keyword spotting ("KW"), the hybrid HMM + keyword spotting ("Hybrid"), and the hybrid system with visual 148

food detector ("visual refinement"). The total number of clips produced by each method is very similar, approximately 1.4 million. The coverage of the clips is approximately 260k unique (action, noun phrase) pairs. To evaluate the quality of these methods, we created a random subset of 900 clips from each corpus using stratified sampling. That is, we picked an action uniformly at random, and then picked a corresponding object for that action from its support set uniformly at random, and finally picked a clip with that (action, object) label uniformly at random from the clip corpuses produced in Section 3; this ensures

Hybrid

KW (manual)

0.5

Hybrid (manual)

KW

KW

