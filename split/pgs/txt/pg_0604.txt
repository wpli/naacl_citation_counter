it is classified as "negative". The proposed model is a semi-supervised method since it is capable of processing documents without the sentiment label. This property makes the proposed method suitable for co-training. 3.2 Corpus-based Method The deep learning approach, especially Stacked Denoising Auto-encoders (SDA), has been shown highly beneficial for extracting domain-independent knowledge (Glorot et al., 2011). Thus, we use SDA to construct the corpus-based sentiment classifier. The stacked autoencoder method was introduced by Rumelhart, Hinton and Williams (Rumelhart et al., 1985) and its denoising variant was proposed by Vincent et al. (Vincent et al., 2010). Recently, it has become an essential building block in deep learning architectures. A basic denoising autoencoder consists of an input layer, a hidden layer and an output layer. The procedure can be interpreted into two phases, i.e., encode and decode. In the encoding phrase, an encoder function is employed to map input data into a feature vector h. For each sample x from input dataset {x(1) , . . . , x(N ) }, we have h = f (U T (x + ) + b) (8) where f (x) is sigmoid activation function, U is the weight matrix between input layer and hidden layer, bh is the bias of each input layer neuron and is a random Gaussian noise. In the decoding phrase, a decoder function is deployed to remap the feature vector in the feature space back to the input space, producing a reconstruction x ^. The decoder function takes the following form x ^ = f (V h + b )
T

we obtain the stacked denoising autoencoder (SDA). Once trained, their parameters can be used to initialize a supervised learning algorithm. In this paper, SDA is learnt in a greedy layer-wise fashion using stochastic gradient descent. For the first layer, the decoder is activated by a sigmoid function, and the Kullback-Liebler divergence is used as the reconstruction error. For the remaining layers, we use the softplus function for activation. After the SDA parameters are trained (on both labeled and unlabeled data) and the high-level representation of each data instance is obtained, a SVM classifier is employed using the resulting representation (of labeled data) to train a sentiment classifier. 3.3 Combining two Methods with Co-training

Algorithm 1 Co-training with corpus-based and lexiconbased methods · Inputs: labeled training data L, unlabeled training data U

· Create a pool U of examples by choosing u unlabeled
examples at random, then loop for k iterations

­ use L and U to train a corpus-based classifier f1 ,

­

­

then use f1 to label samples from U . Let A1 be the set of p positive and n negative most confidently labeled examples. use L and U to train a lexicon-based classifier f2 , then use f2 to label samples from U . Let A2 be the set of p positive and n negative most confidently labeled examples. Add f1 and f2 to the set C of classifiers and add the self-labeled examples A1  A2 to the labeled dataset L. Randomly choose 2p + 2n examples from U to replenish U

· For testing, run all classifiers in C and output the majority
vote.

(9)

where f (x) is also a sigmoid function, V is the weight matrix between the hidden layer and the output layer, and b' is the bias. The parameters of the SDA models, namely  = {U, V, b, b }, are learned by minimizing the reconstruction error L(x, x ^) over all training instances: J () =
x(t)

L(x(t) , x ^(t) )

(10)

where L(·, ·) is measure of discrepancy. Popular choices of L include squared error and KullbackLiebler divergence. By iteratively adding autoencoders on top of a trained denoising autoencoder, 550

We employ a variant of co-training algorithm to train the classifier with a small number of labeled data and a large number of unlabeled data. The cotraining approach is well known for semi-supervised approach (Blum and Mitchell, 1998). For our problem, the two views of co-training are lexicon-based method (domain-specific knowledge) and corpusbased method (domain-independent knowledge). Initially, both classifiers are trained with the partially available labels, as described by the above two subsections. Then, we use one of the two classifiers to label the unlabeled documents, adding its labels to

