weights are tuned for optimal performance on PI. In CNN-LM, we have combined several architectural elements to pretrain a high-quality sentence analysis NN despite the lack of training data. (i) Similar to PV-DM (Le and Mikolov, 2014), we integrate global context (CNN-SM) and local context (the history of size h) into one model ­ although our global context consists only of a sentence, not of a paragraph or document. (ii) Similar to work on autoencoding (Vincent et al., 2010), the output that is to be predicted is part of the input. Autoencoding is a successful approach to learning representations and we adapt it here to pretrain good sentence representations. (iii) A second successful approach to learning embeddings is neural network language modeling (Bengio et al., 2003; Mikolov, 2012). Again, we adopt this by including in CNNLM an ngram language modeling part to predict the next word. The great advantage of this type of embedding learning is that no labels are needed. (iv) CNN-LM only adds one hidden layer over CNNSM. It keeps simple architecture like PV-DM (Le and Mikolov, 2014), CBOW (Mikolov et al., 2013) and LBL (Mnih and Teh, 2012), enabling the CNNSM as main training target. In summary, the key contribution of CNN-LM is that we pretrain convolutional filters. Architectural elements from the literature are combined to support effective pretraining of convolutional filters.

6
6.1

Experiments
Data set and evaluation metrics

ble 1).2 We also include the majority baseline ("baseline") and MT (Madnani et al., 2012). RAE (Socher et al., 2011) and MT were discussed in Sections 1 and 2. We now briefly describe the other prior work. Blacoe and Lapata (2012) compute the vector representation of a sentence from the neural language model (NLM) embeddings (computed based on (Collobert and Weston, 2008)) of the words of the sentence as the sum of the word embeddings (NLM+), as the element-wise multiplication of the word embeddings (NLM ), or by means of the recursive autoencoder (NLM RAE, Socher et al. (2011)). The representations of the two paraphrase candidates are then concatenated as input to an SVM classifier. See Blacoe and Lapata (2012) for details. The ARC model (Hu et al., 2014) is a convolutional architecture similar to (Collobert and Weston, 2008). ARC-I is a Siamese architecture in which two shared-weight convolutional sentence models are trained on the binary paraphrase detection task. Hu et al. (2014) find that ARC-I is suboptimal in that it defers the interaction between S1 and S2 to the very end of processing: only after the vectors representing S1 and S2 have been computed does an interaction occur. To remedy this problem, they propose ARC-II in which the Siamese architecture is replaced by a multilayer NN that processes a single representation produced by interleaving S1 and S2 . We also evaluate Bi-CNN-MI­, an NN identical to Bi-CNN-MI, except that it is not pretrained in unsupervised training. 6.3 Results

We use the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004; Das and Smith, 2009). The training set contains 2753 true and 1323 false paraphrase pairs; the test set contains 1147 and 578 pairs, respectively. For each triple (label, S1 , S2 ) in the training set we also add (label, S2 , S1 ) to make best use of the training data; these additions are nonredundant because the interaction feature matrices (Section 4.1) are asymmetric. Systems are evaluated by accuracy and F1 . 6.2 Paraphrase detection systems

Table 1 shows that Bi-CNN-MI outperforms all other systems. The comparison with Bi-CNN-MI­ indicates that this is partly due to one major innovation we introduced: unsupervised pretraining. Bi-CNN-MI­, the model without unsupervised pretraining, performs badly. Thus, unsupervised training is helpful to pretrain parameters in paraphrase
A reviewer suggests an additional experiment to directly evaluate the importance of multigranularity: a "system that puts all unigrams, short ngrams, long ngrams, and sentence representations into one interaction matrix." This would indeed be an interesting baseline, but there is no obvious way to conduct this experiment since vectors from different levels are not comparable; e.g., they have different dimensionality.
2

Since we want to show that Bi-CNN-MI performs better than previous NN work, we compare with three NN approaches: NLM, ARC and RAE (Ta-

908

