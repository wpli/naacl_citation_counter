Both datasets are annotated with parses and indocument coreference labels provided by the toolset of Napoles et al. (2012)5 and are available with our code release. Due to the small data size, we use k fold cross validation for both datasets. We choose k = 10 for RF due to its very small size (more folds give more training examples) and k = 5 on EECB to save computation time (amount of training data in EECB is less of a concern). Hyperparameters were chosen by hand using using cross validation on the EECB dataset using F1 as the criteria (rather than Hamming). Figures report averages across these folds. Systems Following Roth and Frank (2012) and Wolfe et al. (2013) we include a Lemma baseline for identifying alignments which will align any two predicates or arguments that have the same lemmatized head word.6 The Local baseline uses the same features as Wolfe et al., but none of our joint factors. In addition to running our joint model with all factors, we measure the efficacy of each individual factor by evaluating each with the local features. For evaluation we use a generous version of F1 that is defined for alignment labels composed of sure, Gs , and possible links, Gp and the system's proposed links H (following Cohn et al. (2008), Roth and Frank (2012) and Wolfe et al. (2013)). P = |H  Gp | |H | R= |H  G s | 2P R F = | Gs | P +R

7

Results

Note that the EECB data does not have a sure and possible distinction, so Gs = Gp , resulting in standard F1. In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements. We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in Koehn (2004)). Cases where p < 0.05 are bolded.
https://github.com/cnap/anno-pipeline The lemma baseline is obviously sensitive to the lemmatizer used. We used the Stanford CoreNLP lemmatizer (Manning et al., 2014) and found it yielded slightly better results than previously reported as the lemma baseline (Roth and Frank, 2012), so we used it for all systems to ensure fairness and that the baseline is as strong as it could be.
6 5

Results for EECB and RF are reported in Table 7. As previously reported, using just local factors (features on pairs) improves over lemma baselines (Wolfe et al., 2013). The joint factors make statistically significant gains over local factors in almost all experiments. Fertility factors provide the largest improvements from any single constraint. A fertility penalty actually allows the pairwise weights to be more optimistic in that they can predict more alignments for reasonable pairs, allowing the fertility penalty to ensure only the best is chosen. This penalty also prevents the "garbage collecting" effect that arises for instances that have rare features (Brown et al., 1993). Temporal constraints are relatively sparse, appearing just 2.8 times on average. Nevertheless, it was very helpful across all experiments, though only statistically significantly on the RF dataset. This is one of the first results to demonstrate benefits of temporal relations affecting an downstream task. Perhaps surprisingly, these improvements result from a a temporal relation system that has relatively poor absolute performance. Despite this, improvements are possibly due to the orthogonal nature of temporal information; no other feature captures this signal. This suggests that future work on temporal relation prediction may yield further improvements and deserves more attention as a useful feature for semantic tasks in NLP. The predicate-centric factors improved performance significantly on both datasets. For the predicate-centric factor, when a predicate was aligned there is a 72.3% chance that there was at least one argument aligned as well, compared to only 14.1% of case of non-aligned predicates. As mentioned before, the reason the former number isn't 100% is primarily due to implicit arguments and errors in argument identification. The argument-centric features helped almost as much as the predicate-centric version, but the improvements were not significant on the EECB dataset. Running the same diagnostic as the predicate-centric feature reveals similar support: in 57.1% of the cases where an argument was aligned, at least one predicate it partook in was aligned too, compared to 7.6% of cases for non-aligned arguments. Both the

17

