For both rank accuracy and MRR, a perfect score is 100. However, MRR places more emphasis on ranking items close to the top of the list, and less on differences in ranking lower in the list. For example, if the correct phrase is always ranked 2, 50 or 100 out of list of 4600, median rank accuracy would be 99.95, 98.91 or 97.83. In contrast, MRR would be 50, 2 or 1. Note that rank accuracy and reciprocal rank produce identical orderings of methods. That is, whatever method performs best in terms of rank accuracy will also perform best in terms of reciprocal rank. MRR simply allows us to discriminate between very accurate models. As we will see, the rank accuracy of all models is very high (> 99%), approaching the rank accuracy ceiling. 3.1.1 Estimation Methods

the phrase. ^(p,:) = 0.5 × (A(i,:) + A(j,:) ) A Crucially, w.addNNSE estimates ,  after learning the latent space A, whereas CNNSE simultaneously learns the latent space A, while taking the composition function into account. Once we have an esti^(p,:) we can use the NNSE and CNNSE solumate A tions for D to estimate the corpus statistics X. ^ (p,:) = A ^(p,:) D X Results for the four methods appear in Table 2. Median rank accuracies were all within half a percentage point of each other. However, MRR shows a striking difference in performance. CNNSE has MRR of 40.64, more than 5 points higher than the second highest MRR score belonging to w.addSVD (35.26). CNNSE ranks the correct phrase in the first position for 26% of phrases, compared to 20% for w.addSVD . Lexfunc ranks the correct phrase first for 20% of the test phrases, w.addNNSE 16%. So, while all models perform quite well in terms of rank accuracy, when we use the more discriminative MRR, CNNSE is the clear winner. Note that the performance of w.addNNSE is much lower than CNNSE. Incorporating a composition constraint into the learning algorithm has produced a latent space that surpasses all methods tested for this task. We were surprised to find that lexfunc performed relatively poorly in our experiments. Dinu et al. (2013) used simple unregularized regression to estimate M . We also replicated that formulation, and found phrase ranking to be worse when compared to the Partial Least Squares method described in Baroni and Zamparelli (2010). In addition, Baroni and Zamparelli use 300 SVD dimensions to estimate M . We found that, for our dataset, using all 1000 dimensions performed slightly better. We hypothesize that our difference in performance could be due to the difference in input corpus statistics (in particular the thresholding of infrequent words and phrases), or due to the fact that we did not specifically create the training and tests sets to evenly distribute the phrases for each adjective. If an adjective i appears only in phrases in the test set, lexfunc cannot estimate Mi using training data (a hindrance not present for other methods, which

We will compare to two other previously studied composition methods: weighted addition (w.addSVD ), and lexfunc (Baroni and Zamparelli, 2010). Weighted addition finds ,  to optimize (X(p,:) - (X(i,:) + X(j,:) ))2 Note that this optimization is performed over the SVD matrix X , rather than on A. To estimate X for a new phrase p = (i, j ) we compute ^ (p,:) = X(i,:) + X(j,:) X Lexfunc finds an adjective-specific matrix Mi that solves X(p,:) = Mi X(j,:) for all phrases p = (i, j ) for adjective i. We solved each adjective-specific problem with Matlab's partial least squares implementation, which uses the SIMPLS algorithm (Dejong, 1993). To estimate X for a new phrase p = (i, j ) we compute ^ (p,:) = Mi X(j,:) X We also optimized the weighted addition composition function over NNSE vectors, which we call w.addNNSE . After optimizing  and  using the training set, we compose the latent word vectors to estimate the held out phrase: ^(p,:) = A(i,:) + A(j,:) A For CNNSE, as in the loss function,  =  = 0.5 so that the average of the word vectors approximates 36 5

