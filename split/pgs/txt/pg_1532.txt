plicit and latent aspects of the document's structure. Theoretical features are those specifically mentioned in the theoretical literature on framing. Finally, the Dictionaries feature set tests whether there might be specific terms that invoke framing. 5.2 Training and Testing

We implemented and tested a variety of different classifiers, including Stochastic Gradient Descent (SGD), Multinomial Na¨ ive Bayes, Perceptron, Nearest Neighbor, Logistic Regression, and Passive Aggressive classifiers. In several tests, the Na¨ ive Bayes classifier performed best, so we used a Na¨ ive Bayes classifier throughout. As described above, our data include three to 11 annotators' annotations for each word in the corpus. To create training data, we aggregated these annotations such that any word highlighted by at least one fourth of the annotators was treated as framingrelated (i.e., true positive) for training and testing purposes, and the remaining words were treated as not frame-related (i.e., true negative). These data, in the combinations of features described above, were used to train our ensemble classifier using scikit-learn (Pedregosa et al., 2011) using ten-fold random shuffle cross-validation, as well as a random dummy classifier based on class distributions in the training set. Performance in terms of f-score, accuracy, precision, and recall was averaged across the ten folds.

Figure 1: Performance of each feature set, as well as the dummy baseline and human annotators, on accuracy, F1, precision, and recall. Three feature sets (Lexical, Theoretical, and All features) match human annotators on F1 and outperform human recall, but humans demonstrate higher precision.

6

Results

This section summarizes results, highlighting some important aspects thereof, while the subsequent Discussion section considers interpretation and broader implications. Figure 1 presents a summary of results for each feature set, including comparison with the dummy baseline and the aggregate human performance. The Document Structure feature set performed very poorly, identifying 0 true positives, so we exclude it from the detailed results report. Since overall accuracy does not vary drastically, we focus on other performance metrics. Except for the Dictionaries, all feature sets perform statistically significantly better than the dummy (ANOVA with Tukey's posthoc p < 0.001). F1 scores among the top three performers (Lexical, Theoretical, and All 1478

features) are statistically indistinguishable. Furthermore, each of these three top performers matches aggregate human annotator performance. Looking at precision and recall, we can see that the classifiers and the human annotators make different trade offs. Precision for all feature sets is around 34% (all statisticially significantly better than the dummy and significantly indistinguishable from one another), while human average precision is 91.5%. On the other hand, the three top performing feature sets (All, Lexical, and Theoretical) all achieve recall around 70%, while average recall for the human annotators is only 49.3%. It is also important to note that human performance is calculated by comparing each individual annotator to the aggregate of all the annotators. Because each individual is part of that aggregate, precision scores for the humans are fairly high. We consider comparison with human performance further in the discussion below. We also examine the most influential features for classifiers using each of the three top performing

