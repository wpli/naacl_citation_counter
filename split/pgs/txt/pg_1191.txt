Approach Commonness Popularity Google Search Supervised State-of-theart

Unsupervised Context Collaborator Approach

Sen. Level Cooccurrence Doc. Level Cooccurrence Human AMR System AMR Human SRL

Unsupervised Combined Approach

Human AMR

Definition based on the popularity measure as described in section 5.2. use the top Wikipedia page returned by Google search using the mention as a key word. supervised re-ranking using multi-level linguistic features for collaborators and collective inference, trained from 20,000 entity mentions from TAC-KBP2009-2014. We combined two systems (Chen and Ji, 2011; Cheng and Roth, 2013) using rules to highlight their strengths. sentence-level co-occurrence based collaborator selection (collaborators limited to human AMR-labeled named entities) document-level co-occurrence based collaborator selection (collaborators limited to human AMR-labeled named entities) using human annotated AMR nodes and edges. using AMR nodes and edges automatically generated by an AMR parser (Flanigan et al., 2014). using human annotated core semantic roles defined in PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004): ARG0, ARG1, ARG2, ARG4 and ARG5. coherence approach used where possible (215 mentions), collaborator approach elsewhere (remaining 1398 mentions), using human annotated AMR nodes and edges.

News 89.76 88.10 93.07

DF 68.99 77.17 87.41

Total 82.20 84.12 91.01

93.17 90.77 90.05 87.51 93.56 90.15 93.27 94.34

73.25 70.31 69.86 69.37 86.88 85.69 71.21 88.25

85.92 83.31 82.69 80.90 91.13 88.52 85.24 92.12

Table 2: Accuracy (%) on Test Set (1613 mentions)

context coherence method is used where possible (i.e., those 215 mentions that are members of coherent sets according to our criteria as described in Section 3.5), and the context collaborator approach based on human AMR annotation is applied elsewhere.

Approach Description Coherence: coherence set built from within-sentence collaborators limited to human AMR-labeled Named Entities. Coherence: coherence set built from human AMR conjunctions (Sec. 3.5) Collaborator: used coherent set based on human AMR as collaborators.

News 72.64 96.73 91.50

DF 76.85 95.16 82.26

All 75.47 96.28 88.84

Table 3: Context Coherence Accuracy (%) on 215 Mentions which Can Form Coherent Sets

6.3

Remaining Error Analysis and Discussion

Figure 4: AMR Annotation Layers Effects on Accuracy

Table 3 focuses on the 215 mentions that met our narrow criteria for forming a coherent set of mentions. We applied the context coherence based reranking method (Section 5.4) to collectively link those mentions. This approach substantially outperforms the co-occurrence baseline, and even outperforms the context collaborator approach applied to those 215 mentions, especially for discussion forum data. 1137

A challenging source of errors pertains to the knowledge gap between the source text and KB. News and social media are source text genres that tend to focus on new information, trending topics, breaking events, or even mundane details about the entity. In contrast, the KB usually provides a snapshot summarizing only the entity's most representative and important facts. A source-KB similarity driven approach alone will not suffice when a mention's context differs substantially from anything on the KB side. AMR annotation's synthesis of words and phrases from the surface texts into concepts only provides a first step toward bridging the knowledge gap. Successful linking may require (1) reasoning using general knowledge, or (2) retrieval of other sources that contain additional useful linking information. Table 4 illustrates two relevant examples

