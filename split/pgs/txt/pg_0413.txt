empirical cumulative distribution functions are defined as: ^ ( ) = 1 F m
m

I{xi   }
i=1

(6)

where I{·} is the indicator function, and  indicates the current value that we are evaluating. Note that the above step is also known as probability integral transform (Diebold et al., 1997), which allows us to convert any given continuous distribution to random variables having a uniform distribution. This is crucial for text: instead of using the raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity. We also use the same procedure to transform the vision features into CDF space to be compatible with text features. The Robust Estimation of Copula Now that we have obtained the marginals, and then the joint distribution can be constructed by applying the copula function that models the stochastic dependencies among marginal CDFs:
^ ^ ^ ^ (f F 1 (X1 ), ..., f1 (Xn ), f (y )) ^ ^ ^X f ^ ^ ^ = C [F 1 (X1 ) , ..., FXn fn (Xn ) , Fy fy (y ) ] 1 (7)

In this work, we apply the parametric Gaussian copula to model the correlations among the text features and the label. Assume xi is the smoothed version of random variable Xi , and y is the smoothed label, we have:
F (x1 , ..., xn , y ) =  -1 [Fx1 (x1 )], ..., , -1 [Fxn (xn )], -1 [Fy (y )] (8)

where  is the joint cumulative distribution function of a multivariate Gaussian with zero mean and  variance. -1 is the inverse CDF of a standard Gaussian. In this parametric part of the model, the parameter estimation boils down to the problem of learning the covariance matrix  of this Gaussian copula. In this work, we perform standard maximum likelihood estimation (MLE) for the  matrix, where we follow the details from prior work (Wang and Hua, 2014). To avoid overfitting, traditionally, one resorts to classic regularization techniques such as Lasso (Tib359

shirani, 1996). While Lasso is widely used, the nondifferentiable nature of the L1 norm often make the objective function difficult to optimize. In this work, we propose dropout training (Hinton et al., 2012) as copula regularization. Dropout was proposed by Hinton et al. as a method to prevent feature coadaptation in the deep learning framework, but recently studies (Wager et al., 2013) also show that its behaviour is similar to L2 regularization, and can be approximated efficiently (Wang and Manning, 2013) in many other machine learning tasks. Another advantage of dropout training is that, unlike Lasso, it does not require all the features for training, and training is "embarrassingly" parallelizable. In Gaussian copula estimation context, we can introduce another dimension : the number of dropout learners, to extend the  into a dropout tensor. Essentially, the task becomes the estimation of 1 , 2 , ...,  where the input feature space for each dropout component is randomly corrupted by (1 -  ) percent of the original dimension. In the inference time, we use geometric mean to average the predictions from each dropout learner, and generate the final prediction. Note that the final  matrix has to be symmetric and positive definite, so we apply tiny random Gaussian noise to maintain the property. Computational Complexity One important question regarding the proposed nonparanormal model is the corresponding computational complexity. This boils down to the es^ matrix (Liu et al., 2012): one timation of the  only needs to calculate the correlation coefficients of n(n - 1)/2 pairs of random variables. Christensen (2005) shows that sorting and balanced binary trees can be used to calculate the correlation coefficients with complexity of O(n log n). Therefore, the computational complexity of MLE for the proposed model is O(n log n). Efficient Approximate Inference In this prediction task, in order to perform the exact inference of the conditional probability distribution p(Fy (y )|Fx1 (x1 ), ..., Fxn (xn )), one needs to solve the mean response ^ (Fy (y )|Fx (x1 ), ..., Fx (x1 )) from a joint E 1 1 distribution of high-dimensional Gaussian copula. Unfortunately, the exact inference can be

