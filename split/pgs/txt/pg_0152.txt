use of punctuation is consistent even when the topic changes. For single-domain AA, mid-word was also a good feature, probably because it captured lexical information that correlates with authors' preferences towards writing about specific topics. Figure 1 shows an alternate view of these results, graphing the rank of each n-gram type. For computing the rank, the accuracies of the ten different n-gram type classifiers are sorted in decreasing order and ranked from 1 to 10 respectively with ties getting the same rank. For the Guardian corpora, the average rank of each n-gram category was computed by averaging its rank across the 12 possible test/train cross-domain combinations. In both of the single-domain CCAT corpora, the classifier based on prefix n-grams had the top accuracy (rank 1), and the classifier based on mid-punct had the worst accuracy (rank 10). In both of the cross-domain Guardian corpora, on the other hand, mid-punct was among the top-ranked n-gram categories. This suggests that punctuation features generalize the best across topic, but if AA is more of a topic classification task (as in the single-domain CCAT corpora), then punctuation adds little over other features that more directly capture the topic. Since our cross-domain datasets are small, we performed a small number of planned comparisons using a two-tailed t-test over the accuracies on the Guardian1 and Guardian2 corpora. We found that in both corpora, the best punctuation category (punctmid) is better than the best word category (wholeword) with p < 0.001. In the Guardian2 corpus, the best affix category (space-prefix) is also better than the best word category (whole-word) with p < 0.05, but this does not hold in the Guardian1 corpus (p = 0.14). Also, we observed that in both Guardian1 and Guardian2 datasets, both punct-mid and spaceprefix are better than multi-word (p < 0.01). Overall, we see that affix n-grams are generally effective in both single-domain and cross-domain settings, punctuation n-grams are effective in crossdomain settings, and mid-word is the only effective word n-gram, and only in the single-domain setting. 5.2 Do Different Classifiers Agree on the Importance of Different n-gram Types?

Comparison CCAT Guardian Weka SVM vs LibSVM 0.93 0.81 Weka SVM vs Naive Bayes 0.73 0.57 LibSVM vs Naive Bayes 0.77 0.44 Table 6: Spearman's rank correlation coefficient () for each pair of classifiers on the single-domain (CCAT) and cross-domain (Guardian) settings.

tasks, that mid-word n-grams are good predictors in single-domain settings, and that beg-punct n-grams are good predictors in cross-domain settings. But are these facts about the n-gram types themselves, or are these results only true for the specific SVM classifiers we trained? To see whether certain types of n-grams are fundamentally good or bad, regardless of the classifier, we compare performance of the different n-gram types for three classifiers: Weka SVM classifiers (as used in our other experiments), LibSVM classifiers and Weka's naive Bayes classifiers1 . Figure 2 shows the n-gram category rankings for all these classifiers2 for both the single-domain CCAT and the cross-domain Guardian settings. Across the different classifiers, the pattern of feature rankings are similar. Table 6 shows the Spearman's rank correlation coefficient () for the per-ngram-type accuracies of each pair of classifiers. We observe fairly high correlations, with  above 0.70 for all single-domain pairings, and between 0.44 and 0.81 for cross-domain pairings. As in Section 5.1, prefix and space-prefix are among the most predictive n-gram types. In the single-domain settings, we again see that suffix and mid-word are also highly predictive, while in the cross-domain settings, we again see that beg-punct and mid-punct are highly predictive. These results all confirm that some types of n-grams are fundamentally more predictive than others, and our results are not specific to the particular type of classifier used.
Weka SVM and LibSVM are both support vector machine classifiers, but Weka uses Platt's sequential minimal optimization algorithm while LibSVM uses working set selection with second order information. The result is that they achieve different performance on our AA tasks. 2 We also tried a decision tree classifier, C4.5 (J48) from WEKA, and it produced similar patterns (not shown).
1

The previous experiments have shown, for example, that prefix n-grams are universally predictive in AA 98

