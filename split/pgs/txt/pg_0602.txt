sentiment classification, and the two classifiers from corpus-based method and lexicon-based method are combined. The overall structure of the model is illustrated by Figure 1. 3.1 Lexicon-based Approach

distribution is drawn from a biased Dirichlet prior (p) (p) (p) w  Dir(w ). The vector w  RV is constructed by for k  {1, . . . , K } (1) We set w = 1 if the word w is a positive seed word, otherwise, we set w = 0. The scalars 0 and 1 are hyperparameters. Intuitively, the biased prior enforces a positive seed word more probably drawn from a positive sentiment topic. The distri(n) (n) (u) (u) butions w  Dir(w ) and w  Dir(w ) for negative and neutral sentiment topics are similarly constructed. Once the topic is determined, the word is generated from a multinomial distribution that associates with the topic. We summarize the generative process of the ssLDA model as below: 1. For each word w in the vocabulary, draw the distributions of topics for three sentiment (p) (p) (n) (n) classes: w  Dir(w ), w  Dir(w ) (u) (u) and w  Dir(w ). 2. For each topic k , draw the distribution over (p) (n) words: k  Dir(), k  Dir() and (u) k  Dir(). 3. For each document in the corpus (a) Draw sentiment class distribution p from either Dir((p) ), Dir((n) ) or Dir((u) ) based on the document's sentiment label. (b) For each word in document, Draw sentiment class indicator c  Mult(p), then generate the word's topic z from (c) Mult(w ), and generate the word w from (c) Mult(z ). Given hyper-parameters , , and { (s) ,  (n) ,  (u) }, our goal is to estimate the latent variables in the ssLDA model. We present a collapsed Gibbssampling algorithm, which iteratively takes a word w from the corpus and samples the topic that the word belongs to. The reader may refer to (Yang et al., 2014) for a detailed derivation of the sampling procedure. Let the whole corpus excluding the cur(p) (n) rent word be denoted by D. Let ni,w (or nj,w , or nk,w ) indicate the number of occurrences of positive sentiment topic i(p) (or negative sentiment topic j (n) , or neutral sentiment topic k (u) ) with word w in
(u)

w,k := 0 (1 - w ) + 1 w

(p)

For building the lexicon-based model, the key challenge is that a single word can carry multiple sentiment meanings in different domains, so that a general-purpose sentiment lexicon is less accurate than domain-specific lexicons. To solve this problem, we build a domain-specific sentiment lexicon by semi-supervised sentiment-aware LDA (ssLDA). The ssLDA method takes semi-supervised data as input. 3.1.1 Semi-supervised Sentiment-aware LDA In this section, we describe how each word of the corpus is generated by the ssLDA model, then illustrate its inference method. Each document has three classes of topics: K (p) positive sentiment topics, K (n) negative sentiment topics, and K (u) neutral sentiment topics. Each document is a mixture of the three classes of topics. Each topic is associated with a multinomial distribution over words. To prevent conceptual confusion, we use a superscript "(p)" and "(n)" to indicate variables relating to positive and negative sentiment topics, and a superscript "(u)" to indicate variables relating to neutral sentiment topics. In addition, we assume that the vocabulary consists of V distinct words indexed by {1, . . . , V }. For each word w, there is a multinomial distribution determining which class of topics that w belongs to. This prior distribution is sampled from a Dirichlet distribution Dir(), where  = ((p) , (n) , (u) ) is a vector of three scalars. For documents with different sentiment labels, we choose different values of , so that words in the document with a positive label has a higher probability belonging to positive topics, and vice versa. In the semi-supervised setting, a document usually doesn't have a sentiment label. In that case, the 1 1 value of  is equal to ( 1 3 , 3 , 3 ). Given the class of topics, there is another multinomial distribution indicating the particular topic that the word belongs to. If it turns out that the word belongs to a positive sentiment class, then its topic 548

