fits the following relationship: "a is to b as c is to d," given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa - qb + qc and returning the vector from Q which has the highest cosine similarity to q . Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is "rug  {sofa, ottoman, carpet, hallway}", with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarsegrained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We train an 2 -regularized logistic regression classifier on the average of the word vectors of a given sentence to predict the coarse-grained sentiment tag at the sentence level, and report the testset accuracy of the classifier.

6

Experiments

with Glove and SG vectors). For the extrinsic sentiment analysis task, we observe improvements using all the lexicons and gain 1.4% (absolute) in accuracy for the Multi vectors over the baseline. This increase is statistically significant (p < 0.01, McNemar). We observe improvements over Glove and SG vectors, which were trained on billions of tokens on all tasks except for SYN-REL. For stronger baselines (Glove and Multi) we observe smaller improvements as compared to lower baseline scores (SG and GC). We believe that FrameNet does not perform as well as the other lexicons because its frames group words based on very abstract concepts; often words with seemingly distantly related meanings (e.g., push and growth) can evoke the same frame. Interestingly, we almost never improve on the SYN-REL task, especially with higher baselines, this can be attributed to the fact that SYN-REL is inherently a syntactic task and during retrofitting we are incorporating additional semantic information in the vectors. In summary, we find that PPDB gives the best improvement maximum number of times aggreagted over different vetor types, closely followed by WNall , and retrofitting gives gains across tasks and vectors. An ensemble lexicon, in which the graph is the union of the WNall and PPDB lexicons, on average performed slightly worse than PPDB; we omit those results here for brevity. 6.2 Semantic Lexicons during Learning To incorporate lexicon information during training, and compare its performance against retrofitting, we train log-bilinear (LBL) vectors (Mnih and Teh, 2012). These vectors are trained to optimize the log-likelihood of a language model which predicts a word token w's vector given the set of words in its context (h), also represented as vectors: p(w | h; Q)  exp
i h

We first show experiments measuring improvements from the retrofitting method (§6.1), followed by comparisons to using lexicons during MAP learning (§6.2) and other published methods (§6.3). We then test how well retrofitting generalizes to other languages (§6.4). 6.1 Retrofitting

We use Eq. 1 to retrofit word vectors (§3) using graphs derived from semantic lexicons (§4). Results. Table 2 shows the absolute changes in performance on different tasks (as columns) with different semantic lexicons (as rows). All of the lexicons offer high improvements on the word similarity tasks (the first three columns). On the TOEFL task, we observe large improvements of the order of 10 absolute points in accuracy for all lexicons except for FrameNet. FrameNet's performance is weaker, in some cases leading to worse performance (e.g., 1610

qi qj + bj

(3)

We optimize the above likelihood combined with the prior defined in Eq. 2 using the lazy and periodic techniques described in §2. Since it is costly to compute the partition function over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model (Mnih and Teh, 2012) using AdaGrad (Duchi et al., 2010) with a learning rate of 0.05.

