score. We considered then some variations of the 2L interaction model, to investigate the importance of each of its components. In 2L direct (Figure 1a), we removed the interaction layer, making the model score a weighted combination of the mapped vectors. The 2L interaction direct model (Figure 1c) computes the final score through a weighted combination of both the mapped representations and their interaction vector. The 1L models (Figures 1d, 1e and 1f) are analogous to the corresponding 2L models, but removing the feature mapping layer, thus operating directly on the distributional vectors.

Model 1L direct 1L interaction 1L int. direct 2L direct 2L interaction 2L int. direct 1L mono 2L mono Cosine

corr. r 50 51 49 49 72 67 35 35 36

P 59 50 52 51 76 71 31 32 29

comp. R F1 55 57 61 55 57 54 58 54 58 66 58 64 57 41 64 43 58 38

incomp. P R F1 80 83 72 80 77 79 80 79 80 81 79 80 84 90 87 82 85 84 79 77 78 80 72 76 78 71 74

4

Experiments

Table 1: Experimental results. Correlation with human ratings measured by Pearson r. (In)compatibility detection scored by the F1 measure. We compared the supervised measures to the cosine of pairs directly represented by their DSM vectors (with thresholds tuned on the training set). We expected this baseline to fare relatively well on incompatibility detection, since many of our randomly generated pairs were both incompatible and dissimilar (e.g., bag/bus). Also, we controlled for the portion of the data that can be accounted just by looking at one of the words of the relation (for example, the presence of a word might indicate that the relation is incompatible). To this end, we included two models that look at only one of the words in the pair. 1L mono is a logistic regression model that only looks at the first word of the pair while 2L mono is an analogous neural network with one hidden layer. Results are reported in Table 1. As it can be seen, all the supervised models from Figure 1 strongly outperform the cosine (that, as expected, is nevertheless quite good at detecting incompatibles). Also, they outperform the mono models (with the only exception of 1L direct on incompatibility), showing that the data they account for cannot be reduced to properties of individual lexical items. Importantly, the 2L interaction model is way ahead of all other models, confirming our expectations. To gain some insight into the features learned by the best model, we labeled the words of our input vocabulary with one of the following general category tags: animal, artefact, general-function, human, organic-and-food and place. The distribution

Since compatibility is a symmetric relation, we first duplicated each pair in the benchmark by swapping the two words. We then split it into training, testing and development sections. To make the task more challenging, we enforced disjoint vocabularies in each of them. For example, drummer only occurs in the training set, while ant, only in the test set. We use about 1/10th of the vocabulary (29 words) on the development set and the rest was split equally between train and test (135 words each). The resulting partitions contain 7,228 (train), 7,336 (test) and 312 (development) pairs, respectively. To train the models, we used the scores they generate in three sub-tasks: approximation of average ratings, classification of compatibles and classification of incompatibles. We used mean square error as cost function for the first sub-task, cross-entropy for the latter two. We implemented the models in Torch7 (Collobert et al., 2011).5 We trained them for 120 epochs with adagrad, with a batch size of 150 items and adopting an emphasizing scheme (LeCun et al., 2012), where compatibles, incompatibles and middle-ground items appear in equal proportions. We fixed hidden-layer size to 100 dimensions, while we tuned a coefficient for a L2-norm regularization term on the development data. We evaluated the models ability to predict human compatibility ratings as well as to detect compatible and incompatible items.
5 We make the code available at https://github.com/ germank/compatibility-naacl2015

967

