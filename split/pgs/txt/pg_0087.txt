2

Method

Typically, word usage statistics used to create a VSM form a sparse matrix with many columns, too unwieldy to be practical. Thus, most models use some form of dimensionality reduction to compress the full matrix. For example, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) uses Singular Value Decomposition (SVD) to create a compact VSM. SVD often produces matrices where, for the vast majority of the dimensions, it is difficult to interpret what a high or low score entails for the semantics of a given word. In addition, the SVD factorization does not take into account the phrasal relationships between the input words. 2.1 Non-negative Sparse Embeddings Our method is inspired by Non-negative Sparse Embeddings (NNSEs) (Murphy et al., 2012). NNSE promotes interpretability by including sparsity and non-negativity constraints into a matrix factorization algorithm. The result is a VSM with extremely coherent dimensions, as quantified by a behavioral task (Murphy et al., 2012). The output of NNSE is a matrix with rows corresponding to words and columns corresponding to latent dimensions. To interpret a particular latent dimension, we can examine the words with the highest numerical values in that dimension (i.e. identify rows with the highest values for a particular column). Though the representations in Table 1 were created with our new method, CNNSE, we will use them to illustrate the interpretability of both NNSE and CNNSE, as the form of the learned representations is similar. One of the dimensions in Table 1 has top scoring words guidance, advice and assistance - words related to help and support. We will refer to these word list summaries as the dimension's interpretable summarization. To interpret the meaning of a particular word, we can select its highest scoring dimensions (i.e. choose columns with maximum values for a particular row). For example, the interpretable summarizations for the top scoring dimensions of the word military include both positions in the military (e.g. commandos), and military groups (e.g. paramilitary). More examples in Supplementary Material (http://www.cs.cmu.edu/~fmri/ papers/naacl2015/). NNSE is an algorithm which seeks a lower di33

mensional representation for w words using the cdimensional corpus statistics in a matrix X  Rw×c . The solution is two matrices: A  Rw× that is sparse, non-negative, and represents word semantics in an -dimensional latent space, and D  R ×c : the encoding of corpus statistics in the latent space. NNSE minimizes the following objective: 1 argmin 2 A,D st:
w

Xi,: - Ai,: × D
i=1

2

+ 1 Ai,:

1

(1)  1,  1  i  (2) (3)

T Di,: Di, :

Ai,j  0, 1  i  w, 1  j 

where Ai,j indicates the entry at the ith row and j th column of matrix A, and Ai,: indicates the ith row of the matrix. The L1 constraint encourages sparsity in A; 1 is a hyperparameter. Equation 2 constrains D to eliminate solutions where the elements of A are made arbitrarily small by making the norm of D arbitrarily large. Equation 3 ensures that A is nonnegative. Together, A and D factor the original corpus statistics matrix X to minimize reconstruction error. One may tune and 1 to vary the sparsity of the final solution. Murphy et al. (2012) solved this system of constraints using the Online Dictionary Learning algorithm described in Mairal et al. (2010). Though Equations 1-3 represent a non-convex system, when solving for A with D fixed (and vice versa) the loss function is convex. Mairal et al. break the problem into two alternating optimization steps (solving for A and D) and find the system converges to a stationary solution. The solution for A is found with a LARS implementation for lasso regression (Efron et al., 2004); D is found via gradient descent. Though the final solution may not be globally optimal, this method is capable of handling large amounts of data and has been shown to produce useful solutions in practice (Mairal et al., 2010; Murphy et al., 2012). 2.2 Compositional NNSE We add an additional constraint to the NNSE loss function that allows us to learn a latent representation that respects the notion of semantic composition. As we will see, this change to the loss function has a huge effect on the learned latent space. Just as

2

