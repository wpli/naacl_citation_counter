3.4

EPA lexicon Induction

The method described in the last section relies on a lexicon that maps words to EPA values. The ACT lexicon we used originally contains 2,293 words (original-EPA-lexicon) (MacKinnon, 2006). We augmented this lexicon by adding the Affective Norm for English Words (ANEW) (Bradley and Lang, 2010) data-set that contains 2,476 English words, and the Warriner et al. data-set (Warriner et al., 2013) that contains 13,915 words. ANEW and Warriner et al. data-sets were both scaled from the range of [1,9] to the range of [-4.3,4.3] using maxmin scaling formula (Han, 2012). The min-max normalization preforms a linear transformation for a given value xi of A with a minimum and maximum value of [minA , maxA ] to xi in range of [minB , maxB ] given this formula: xi = xi - minA (maxB - minB ) + minB maxA - minA

We initialized the labeled nodes/words with the EPA value of the words observed in the training set, and the unlabeled nodes/words with zeroes. We computed the weight matrix using the WordNet similarity (Wu and Palmer, 1994) between the two words xi and xj . Wu and Palmer's similarity is equal to the depth of the least common subsumer (LCS, the least common ancestor) divided by the summation of the depth of the two words in the WordNet taxonomy. simwup (w1 , w2 ) = 2  depth(LCS ) depth(w1 ) + depth(w2 )

Each edge E  (vi , vj ) has an associated weight Tij , which is the row normalized weight wij of the edge between vi and vj . The labels are then propagated to adjacent nodes by computing Y  T Y . After each iteration the labeled nodes Yl are reset to their initial values (see Algorithm 1). Algorithm 1 ACT Label Propagation procedure L ABEL PROPAGATION(Synests C ) Construct a Graph G = {V, E, W } Initialize T 0 and Y 0 matrices, i  0 repeat Y i  T Y i-1 Yl  L until Y converges end procedure The label propagation algorithm generated 167 adverbs, 3,809 adjectives, 11,531 verbs, and 81,347 nouns, with their EPA ratings in which 10,249 of them are in the testing-EPA-lexicon. To evaluate the validity of this approach, we compare our generated EPA ratings for these 10,249 words with those from the testing-EPA-lexicon (from ACT/ANEW/Warriner datasets). The results are shown in Tables 1 and 2. The resulting EPA are equally distributed between -4.3 and +4.3. We used two metrics to compare the results: root mean squared error (RMSE) and mean absolute error (MAE). These metrics were both close to 1.0 for E, P, and A, suggesting that there is a reasonable degree of agreement between the induced and manually labeled EPA values. It is worth mentioning that due to the limited numbers of adverbs in the ACT

Adding ANEW and Warriner lexicons generated lexicon of 17,347 words (extended-EPA-lexicon). We then randomly divided the extended-EPAlexicon into a training-EPA-lexicon and a testingEPA-lexicon, with 5,782 and 11,565 words, respectively. We used the training-EPA-lexicon to add more words, using a graph-based semi-supervised learning method called label-propagation, a technique that has been commonly used for NLP (Chen et al., 2006; Niu et al., 2005; Rao and Ravichandran, 2009; Zhu and Ghahramani, 2002; BlairGoldensohn et al., 2008), and image annotation (Cao et al., 2008; Heckemann et al., 2006). Labelpropagation is a transductive algorithm that propagates information from a set of labeled nodes (seed sets) to the rest of the graph through its edges (Zhu and Ghahramani, 2002). The label-propagation algorithm starts by adding all the synonyms and lemmas in WordNet of a specific part-of-speech (verb, noun, adjective, or adverbs) to the training-EPAlexicon. This generates a set of labeled words L = (Xl , Yl ) (the 5,782 words with EPA labels), and unlabeled words U = (Xu , Yu ), from which we constructed undirected weighted graph G = {E, V, W }, where V is a set of vertices (all the words in the set), E is the weighted edges, and W is an n × n weight matrix (affinity matrix) n equal to the size vocabulary |V |. 1553

