the entire document. 4. Large Contiguous Passages Entirely Annotated -- Large, unbroken annotations rarely occurred in our pilot study. Such annotations may also have indicated that the annotator was not attending to the entire content being annotated. Thus, we require that the longest contiguous annotation be no more than 120 words long. 5. Annotating Solely Stop Words -- A number of annotations include only very common words, such as articles, prepositions, forms of the verb "to be," etc. Single words such as "but," "all," or "not" could arguably be related to framing. However, accounts from our pilot study participants made us less likely to believe that other single words, such as "an," "of," or "that," instantiated framing. Thus, we require that no more than 3 annotated passages consistent entirely of such stop words1. We also encountered two situations in which pairs of MTurk workers submitted virtually identical annotations for multiple documents. Following up with the workers, we discovered that in one situation a husband and wife team had actually been working together. Based on our pilot study, we would expect annotators to agree to some degree, but we would not want such agreement to arise because annotators were collaborating. Thus, we also exclude these annotations where we suspect the possibility of collaboration. We only include in our dataset documents with at least three valid annotations. In total, the resulting data set includes 74 articles containing 53,878 total words (M=728.1 words per particle), each with three to 11 valid annotations (Mdn=6). The data set includes 59,948 annotated words across 507 annotations from 372 annotators (M=122.8 annotated words per article). The full data set is available at http://hdl.handle.net/1813/39216.

and the training and testing methods. 5.1 Features and Subsets We treat each word as a data point to be classified as framing-related or not. Feature extraction began with splitting each article into sentences with NLTK (Bird et al., 2009) then using Stanford CoreNLP (Klein and Manning, 2003; De Marneffe et al., 2006) to parse each sentence, obtain POSs and lemmatized forms for each word, etc. We then construct a feature vector for each non-stop word. Table 1 lists all features used. The remainder of this subsection describes the justificaton for each feature, as well as several subsets of features. Framing is often instantiated by specific "keywords, stock phrases," (Entman, 1993, p. 52) or "catchphrases" (Gamson and Modigliani, 1989, p. 3). Therefore, we include lexical features that capture the specific words used. Furthermore, many of our pilot study participants pointed out that a given word might be seen as related to framing because of the other words near which it appears. Therefore, each of these features includes a contextual window of up to two words before and after the word being classified (Recasens et al., 2013). Participants in our pilot study said physical locations, such as the names of states, were often not related to framing. However, they sometimes saw names of political figures or experts as indicating framing. Thus, we include the named entity type as a feature, both of the word itself and of its context. Pilot study participants also mentioned that a word's relationship to the remainder of a document and its overall thesis played important roles. A number of similar structural aspects of the document, both explicit and latent, may help draw out these relationships. Several specific types of terms may be indicative of framing. For example, "depictions," "visual images," and figurative language such as metahpor (Gamson and Modigliani, 1989) often invoke frames. For imagery, we use previously established imagery ratings for 1818 common words (Paivio et al., 1968; van der Veur, 1975). Words that appear in this list are given an imagery rating, either low (first quantile), medium (second and third quantile), or high (fourth quantile) imagery. The context feature represents the average imagery of the

5

Classifier Design and Implementation

As argued above, drawing attention to framingrelated language may both mitigate frame effects (Baumer et al., 2015) and facilitate frame reflection (Sch¨ on and Rein, 1994). This section describes the features used to train a classifier to identify framing along with justifications for each, different subsets of features that were tested, our classifier selection, 1476

