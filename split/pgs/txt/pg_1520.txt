Cross-lingual Text Classification Using Topic-Dependent Word Probabilities
Daniel Andrade Akihiro Tamura Masaaki Tsuchida Kunihiko Sadamasa Knowledge Discovery Research Laboratories, NEC Corporation, Japan {s-andrade@cj, a-tamura@ah, m-tsuchida@cq, k-sadamasa@az}.jp.nec.com

Abstract
Cross-lingual text classification is a major challenge in natural language processing, since often training data is available in only one language (target language), but not available for the language of the document we want to classify (source language). Here, we propose a method that only requires a bilingual dictionary to bridge the language gap. Our proposed probabilistic model allows us to estimate translation probabilities that are conditioned on the whole source document. The assumption of our probabilistic model is that each document can be characterized by a distribution over topics that help to solve the translation ambiguity of single words. Using the derived translation probabilities, we then calculate the expected word frequency of each word type in the target language. Finally, these expected word frequencies can be used to classify the source text with any classifier that was trained using only target language documents. Our experiments confirm the usefulness of our proposed method.

1 Introduction
Text classification is ubiquitous in natural language processing. It's applications range from simple topic detection, like articles about sport vs articles about computers, to sentimental analysis, and subtle discrimination of Tweets that report the abuse of drugs or the metaphoric use of drugs ("love is like a drug"). Text classification hugely relies on manually annotated training data in one language. However, creating training data for each language is expensive, and therefore, we are interested in using training data given in only one language (e.g.

English, denoted as target language) to classify text written in a different language (e.g. Chinese, or Japanese, denoted as source language). Our approach addresses this issue by using a simple bilingual dictionary. Bilingual dictionaries have the great advantage that they are available often for free1 , and have good coverage for major languages, like Chinese and Japanese. With the help of the dictionary, we calculate the expected frequency of each word in the target language. Finally, we create a feature vector in the target language that is used as input for the text classifier. However, due to the translation ambiguity of a word in the source language, it is important to carefully choose the translation probability for calculating the expected frequencies of the target words. For example, consider a Japanese news article that contains the word (restrict, restrain, in custody), and we want to find out whether the article is about "foreign policy" or not. The most simple method is to use all its English translations, and assume a uniform distribution over them, i.e. {0.33, 0.33 and 0.33}. However, depending on the topic of the news article, the translation "in custody" is more appropriate. For example, if the article reports about a crime/crime suspect, the translation "in custody" is more likely than "restrict" and "restrain". Conversely, if the article is about "military", the translation "in custody" is less likely. Moreover, an article that is about the topic "military" is more likely to belong to the class "foreign policy". This example demonstrates the importance of estimating good translation probabilities in order to improve the clasFor example from Wikitionay.org under Creative Commons Licence.
1

1466
Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1466­1471, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

