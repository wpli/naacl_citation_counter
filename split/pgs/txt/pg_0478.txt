Configuration No edit All edits ADDITION PUNCT WORD BEVERB DETERMINER OTHER SUBJECT REPLACEMENT PUNCT WORD CAPITALIZATION CONTRACTION OTHER SLANG REMOVAL PUNCT WORD OTHER TWITTER

Count 8479 3411 993 437 556 137 103 141 175 1797 312 1485 634 246 176 429 621 120 501 172 329

Table 1: Token counts for each type of normalization edit.

4

Evaluation

In this section, we present our examination of the effect of normalization edits on downstream NLP applications. To get a broad understanding of these effects, we examine three very different cases: dependency parsing, named entity recognition (NER), and text-to-speech (TTS) synthesis. We chose these tasks because they each require the extraction of different information from the text. For instance, named entity recognition requires only a shallow syntactic analysis, in contrast to the deeper understanding required for dependency parsing. Similarly, only speech synthesis requires phoneme production, while the other tasks do not. Despite their differences, each of these tasks is relevant to larger applications that would benefit from improved performance on Twitter data, and each has garnered attention in the normalization and Twitter-adaptation literature (Beaufort et al., 2010; Liu et al., 2011b; Zhang et al., 2013). Although the differences in these tasks also dictates that they be evaluated somewhat differently, we examine them within a common evaluation structure. In all cases, to examine the effects of each nor424

malization edit we model our analyses as ablation studies. That is, for every category in the taxonomy, we examine the effect of performing all normalization edits except the relevant case. This allows us to measure the drop in performance solely attributable to each category; the greater the performance drop observed when a given normalization edit is not performed, the greater the importance of performing that edit. To aid analysis, results are presented in two ways: 1) as raw performance numbers, and 2) as an error rate per-token. These metrics give two different views of the relevance of each edit type. The raw numbers give a sense of the overall impact of a given category, and as such may be impacted by the size of the category, with common edits becoming more important simply by virtue of their frequency. In contrast, the per-token error rate highlights the cost of failing to perform a single instance of a given normalization edit, independent of the frequency of the edit. Both of these measures are likely to be relevant when attempting to improve the performance of a normalization system. Note that since the first measure is one of overall performance, smaller numbers reflect larger performance drops when removing a given type of edit, so that the smaller the number the more critical the need to perform the given type of normalization. In contrast, the latter judgment is one of error rate, and thus interpretation is reversed; the larger the error rate when it is removed, the more critical the normalization edit. Another commonality among the analyses is that performance is measured relative to the top performance of the tool, not the task. That is, following Zhang et al. (2013), we consider the output produced by the tool (e.g., the dependency parser) on the grammatically correct data to be gold standard performance. This means that some output based on our gold standard may in fact be incorrect relative to human judgment, simply because the tool used does not have perfect performance even if the text if fully grammatical. Since the goal is to understand how normalization edits impact the performance, this style of evaluation is appropriate; it considers mistakes attributable to normalization edits as erroneous, but ignores those mistakes attributable to the limitations of the tool. Finally, to maximize the relevance of the analyses

