System LBL MLBL Image Keyword Template SMT

BLEU METEOR 7.33 17.76 12.30 20.40 12.80 21.77 14.70 26.60 40.30 30.40 43.70 35.60

Resp YES NO MAYBE

1st 75.5 18.0 6.5

2nd 65.8 24.8 9.50

3rd 53.0 31.2 15.8

4th 44.7 31.3 15.8

5th 44.0 37.5 18.5

6th 37.5 58.0 4.50

Table 6: Proportion of SMT descriptions deemed accurate and relevant. System output evaluated for rank placements 1. . . 6.

Table 4: Model comparison on scene description task using automatic measures.

System Keyword Template SMT Human

1st 0.24 0.25 0.53 0.57

2nd 0.13 0.16 0.24 0.27

3rd 0.22 0.13 0.12 0.12

4th AvgRank 0.41 2.22 0.46 2.21 0.11 3.19 0.04 3.36

Table 5: Rankings (shown as proportions) and mean ratings given to systems by human participants.

dataset. The model essentially implements a feedforward neural network to predict the next word given the image and previous words.4 Images were associated with feature representations obtained from the output of a convolutional network, following the feature learning procedure outlined in Kiros et al. (2014).

6

Results

We evaluated system output automatically using (smoothed) BLUE and METEOR as calculated by NIST's MultEval software5 using the human-written descriptions as reference. Elliott and Keller (2014) find that both metrics correlate well with human judgments. For a fair comparison, we force our model to output one description, i.e., the most relevant one. Our results are summarized in Table 4. As can be seen, our model (SMT) performs best both in terms of BLEU and METEOR. The templatebased generator (Template) obtains competitive performance which is not surprising, it incorporates some of the ingredients of the SMT system such as
4 We used the implementation at http://www.cs. toronto.edu/~rkiros/multimodal.html. 5 ftp://jaguar.ncsl.nist.gov/mt/resources/ mteval-v13a-20091001.tar.gz

word-to-clipart alignments, a language model, and is guaranteed to produce grammatical output. The performance of the multimodal log-bilinear model (MLBL) keyword- and image-based retrieval systems is inferior. We conjecture that the image features, and similarity functions used in these models are not fine-grained enough to capture the subtle differences in scenes which humans detect and express in the descriptions. Finally, notice that visual information is critical in doing well on the description generation task. A log-bilinear language model (LBL) trained solely on the descriptions performs poorly (see the top row in Table 4). We further evaluated system output eliciting human judgments for 100 randomly selected test scenes. Participants were presented with a scene and descriptions generated by our system, the templatebased model, the best-performing sentence retrieval model, and a randomly selected human description. Subjects were asked to rank the four descriptions from best to worst (ties are allowed) in order of informativeness (does the description capture what is shown in the scene?) and fluency (is the description written in well-formed English?). We elicited rankings using Amazon's Mechanical Turk crowdsourcing platform. Participants (self-reported native English speakers) saw 10 scenes per session. We collected 5 responses per item. The results of our human evaluation study are shown in Table 5. Specifically, we show, proportionally, how often our participants ranked each system 1st, 2nd and so on. Perhaps unsurprisingly, the human-written descriptions were considered best (and ranked 1st 57% of the time). Our model is ranked best 0.53% of the time, followed by the template and keyword-based retrieval systems which are only ranked first 25% of the time.6 We further
6 Percentages

do not sum to 100% because ties are allowed.

1512

