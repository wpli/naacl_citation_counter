6

Evaluating Word Representations

6.2.2

Compared methods

Models of word meaning in context are commonly evaluated in lexical substitution tasks on predicting paraphrases of a target word that preserve its meaning in a given context. Conveniently, our paraphrase vector representations for words include exactly these predictions. We evaluated our model on two lexical substitution datasets under two types of tasks and compared it to the state-of-the-art as described next. 6.1 Lexical substitution datasets

The dataset introduced in the lexical substitution task of SemEval 2007 (McCarthy and Navigli, 2007), denoted here LS07, is the most widely used for the evaluation of lexical substitution. It consists of 10 sentences extracted from a web corpus for each of 201 target words (nouns, verbs, adjectives and adverbs), or altogether 2,010 word instances in sentential context, split into 300 trial sentences and 1,710 test sentences. The gold standard provided with this dataset is a weighted lemmatized paraphrase list for each word instance, based on manual annotations. A more recent dataset (Kremer et al., 2014), denoted LS14, provides the same kind of data as LS07, but instead of target words that were specifically selected to be ambiguous as in LS07, the target words here are simply all the content words in text documents extracted from news and fiction corpora. LS14 is also much larger than LS07 with over 15K target word instances. 6.2 6.2.1 Predicting lexical substitutions Task description

In SemEval 2007 the lexical substitution task organizers evaluated participant systems on their ability to predict the paraphrases in the gold standard of the LS07 test-set in a few subtasks (1) best and bestmode - evaluate the quality of the best predictions (2) oot and oot-mode (out of ten) - evaluate the coverage of the gold paraphrase list by the top ten best predictions.5 We performed this evaluation on both the LS07 and LS14 datasets.
5 For brevity we do not describe the details of these subtasks. We report only recall scores as in this task recall=precision for all methods that predict paraphrases to all of the instances in the dataset, as we did.

We used the same learning corpus and substitute vector generation procedure as described in Section 5. For every target word type u (not lemmatized) in the LS07 and LS14 datasets, we sampled a collection of 20K sentence contexts from our learning corpus (or less for word types with lower frequency), denoted Cu .6 Next, we generated substitute vectors, pruned to top-100 entries, for these contexts. For every target word type u, we discarded the contexts in Cu where u itself is not in the top100 predicted substitutes, assuming that either these contexts are not typical to u, or that the quality of the predicted substitutes is low. This omits approximately 25% of all contexts. Finally, we generated top-100 and top-1000 substitute vectors for all of the instances in the LS07 and LS14 datasets. We used a generalization of PPMI, called Shifted PPMI (Levy and Goldberg, 2014), as our substitute vector fitness weights: SP P M I (v, c; s) = max(0, P P M I (v, c) - s), where s is a global shift constant. Levy and Goldberg (2014) showed that SPPMI outperformed PPMI on various semantic tasks. We tuned the value of s  {0.0, 1.0, 2.0, 3.0} on the trial portion of the LS07 dataset and used the best value, 2.0, on the LS07 test set and on LS14. We omit results based on conditional probability weights for brevity as they were substantially worse. Next, for every LS07/LS14 instance of word u in context c we generated a paraphrase vector according to Equation (4). We used the paraphrase vectors sorted by entry scores as our paraphrase predictions. in (in-context) denotes this method, where n is the Pn pruning factor used for generating the substitute vectors of the lexical substitution datasets.7 As baseline, we generated our meaning outof-context paraphrase vectors according to Equation (3), denoted P out . We also used word2vec (Mikolov et al., 2013) to generate dense word vectors for all word types in our learning corpus.8 Para6 In general, we observed that the results improve the more contexts are sampled up to  10K contexts per word type. 7 For the learning corpus contexts we always used top-100 substitute vectors to reduce computational complexity. 8 We experimented with 600-dimension vectors, negative sampling value 15, both skip and cbow options, and various window sizes, and tuned these parameters on the trial portion of the LS07 dataset.

477

