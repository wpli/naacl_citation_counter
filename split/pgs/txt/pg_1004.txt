Model D IREC TL+ Reranking Joint Joint + Reranking + Lexicon

EnJa 51.5 56.8 56.4 57.0 -

JaEn 19.7 30.3 38.8 44.6 53.1

EnHi 43.4 50.8 51.6 53.0 -

HiEn 42.6 48.9 51.1 57.2 61.7

Table 6: Transliteration accuracy with supplemental information.

5.3

Multi-lingual transliterations

The generated transcriptions of supplemental transliterations discussed in the previous section are quite inaccurate because of small and noisy G2P training data. In addition, we are prevented from taking advantage of supplemental transliterations from other languages by the lack of the G2P training data. In order to circumvent these limitations, we propose to directly incorporate supplemental transliterations into the generation process. Specifically, we train our generalized joint model on the graphemes of the source word, as well as on the graphemes of supplemental transliterations. The experiments that we have conducted so far suggest two additional methods of improving the transliteration accuracy. We have observed that nbest lists produced by our joint model contain the correct transliteration more often than the baseline models. Therefore, we follow the joint generation with a reranking step, in order to boost the top-1 accuracy. We apply the reranking algorithm of Bhargava and Kondrak (2011), except that our joint model is the base system for reranking. In order to ensure fair comparison, the held-out sets for training the rerankers are subtracted from the original training sets. Another observation that we aim to exploit is that a substantial number of the outputs generated by our joint model are very close to gold-standard transliterations. In fact, news writers often use slightly different transliterations of the same name, which makes the model's task more difficult. Therefore, we rerank the model outputs using a target-language lexicon, which is a list of words together with their frequencies collected from a raw corpus. We follow Cherry and Suzuki (2009) in extracting lexicon features for a given word according to coarse bins, i.e., [< 2000], [< 200], [< 20], [< 2], [< 1]. For 950

example, a word with the frequency 194 will cause the features [< 2000] and [< 200] to fire. We conduct our final experiment on forward and backward transliteration. We utilize supplemental transliterations from all eight languages in the NEWS 2010 dataset. The English-Japanese and English-Hindi datasets contain 33,540 and 13,483 entries, of which 23,613 and 12,131 have at least one supplemental transliteration, respectively. These sets are split into training/development/test sets. The entries that have no supplemental transliterations are removed from the test sets, which results in 2,321 and 1,226 test entries. In addition, we extract an English lexicon comprising 7.5M word types from the English gigaword monolingual corpus (LDC2012T21) for the back-transliteration experiments. We evaluate the following models: (1) the baseline D IREC TL+ model trained on source graphemes; (2) the reranking model of Bhargava and Kondrak (2011)3 , with D IREC TL+ as the base system; (3) our joint model described in Section 3.5; (4) "combination", which is a reranking model with our joint model as the base system; and (5) a reranking model that uses the English target lexicon and model (4) as the base system. Table 6 present the results. We see that our joint model performs much better by directly incorporating the supplemental transliterations than by using the corresponding phonetic transcriptions. This is consistent with our experiments in Section 4 that show the importance of the orthographic information. We also observe that our joint model achieves substantial improvements over the baseline on the back-transliteration tasks from Japanese and Hindi into English. This result suggests the orthographic information from the supplemental transliterations is
3

Code from http://www.cs.toronto.edu/~aditya/g2p-tl-rr/

