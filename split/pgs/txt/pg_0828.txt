Data Pair a. Ar(Manl.Sent) - Ar(Auto.Sent) b. Ar(Manl.Sent) - En(Manl.Trans., Manl.Sent) c. Ar(Manl.Sent) - En(Manl.Trans., Auto.Sent) d. Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent) e. Ar(Manl.Sent) - En(Auto.Trans., Auto.Sent) f. En(Manl.Trans., Manl.Sent) - En(Auto.Trans., Manl.Sent) g. En(Manl.Trans., Manl.Sent) - En(Manl.Trans., Auto.Sent) h. En(Auto.Trans., Manl.Sent) - En(Auto.Trans., Auto.Sent)

Match % 63.89 71.31 68.65 57.21 62.49 60.08 66.51 69.58

Table 4: Match percentage between pairs of sentiment labelled BBN datasets. Data Pair a. Ar(Manl.Sent) - Ar(Auto.Sent) b. Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent) c. Ar(Manl.Sent) - En(Auto.Trans.-Auto.Sent) d. En(Auto.Trans, Manl.Sent) - En(Auto.Trans., Auto.Sent) Match % 78.65 71.05 78.11 78.80

Table 5: Match percentage between pairs of sentiment labelled Syria datasets.

that the English sentiment system is performing rather well. (One would not expect it to get a match greater than 71.31%.) More importantly, the English sentiment system shows a competitive result of 62.49% when run on the automatically translated text (row e.), which makes this choice a viable option for sentiment analysis of non-English texts. This result is inline with previous findings in Information Retrieval (Nie et al., 1999) and Text Classification (Amini and Goutte, 2010). Rows d. and e. compare Ar(Manl.Sent.) with manual and automatic sentiment labeling of automatic translations. Since automatic translation from Arabic to English is fairly difficult, we expect these match percentages to be lower than those in rows b. and c., and that is exactly what we observe. However, it is unexpected to find the number for row e. to be higher than that of row d. We find the same pattern for corresponding data pairs in the Syrian tweets as well (rows b. and c. in Table 6). This suggests that certain attributes of automatically translated text mislead humans with regards to the true sentiment of the source text. However, these same attributes do not seem to affect the automatic sentiment analysis system as much. Since the NRC sentiment analysis system is largely reliant on word-sentiment associations and does not use syntax-based features, it is possible that syntactic abnormalities introduced by automatic translation impact human perception of sentiment. However, this supposition needs to be validated by future work. 774

Row f. shows that manual and automatic translation lead to only about 60% match in manually annotated sentiment labels with each other. Row g. shows accuracy of the English automatic sentiment analysis system on the manually translated text (assuming the English sentiment labels as gold). The result of 66.51% is very close to human agreement on manually translated data (68%), which demonstrates the high quality of the English sentiment analysis system. Row h. shows accuracy of the English automatic sentiment analysis system on the automatically translated text (assuming the English sentiment labels as gold). In this case, the system's accuracy of 69.58% is higher than the human agreement on automatically translated text (65.7%), which again shows that automatic translation greatly impacts sentiment perceived by humans. We manually examined several tweets from the BBN dataset to understand why humans incorrectly annotate a tweet's automatic translation. Most of the cases were due to bad translation where sentiment words either disappeared or were replaced with words of opposite sentiment. In some cases, the translation was affected by typos on the Arabic side. Table 6 shows some examples. Often the mistranslations occurred due to word sense ambiguity. For example, has two meanings: scorpions and clock arms. In example 1 (metaphorically stating that relatives can hurt like scorpion bites), the word is mistranslated, leading to neutral (instead of negative) sentiment.

