the results of the classifiers as a stand-alone module, assuming that the previous module provides a perfect output.
Spanish Dev.set Hypernode identification Lemma generation Intra-hypernode dep. generation Inter-hypernode dep. generation Spanish Test set Hyper-node identification Lemma generation Intra-hypernode dep. generation Inter-hypernode dep. generation # 3327/3437 724/767 756/756 2628/2931 # 5640/5878 1556/1640 1622/1622 4572/5029 % 96.80 94.39 100.00 89.66 % 95.95 94.88 100.00 90.91

Test Set surface gen. baseline deep gen. deep gen.

BLEU 0.91 0.69 0.77

NIST 15.26 13.71 14.42

Exact 56.02 % 12.38 % 21.05 %

Table 4: Overview of the results on the English test set excluding punctuation marks after the linearization

4.2

Discussion and Error Analysis

Table 1: Results of the evaluation of the SVMs for the non-isomorphic transition for the Spanish DSyntS development and test sets

English Test set Hyper-node identification Lemma generation Intra-hypernode dep. generation Inter-hypernode dep. generation

# 42103/43245 6726/7199 6754/7179 35922/40699

% 97.36 93.43 94.08 88.26

Table 2: Results of the evaluation of the SVMs for the non-isomorphic transition for the English DSyntS test set

To have the entire generation pipeline in place, we carried out several linearization experiments, starting from: (i) the SSyntS gold standard, (ii) SSyntSs generated by the rule-based baselines, and (iii) SSyntSs generated by the data-driven deep generator; cf. surface gen., baseline deep gen, and deep gen. respectively in Tables 3 and 4).12
Development Set surface gen. baseline deep gen. deep gen. Test Set surface gen. baseline deep gen. deep gen. BLEU 0.754 0.547 0.582 BLEU 0.762 0.515 0.542 NIST 11.29 9.98 10.78 NIST 12.08 10.60 11.24 Exact 24.20 % 10.96 % 12.33 % Exact 15.89 % 2.33 % 3.49 %

Table 3: Overview of the results on the Spanish development and test sets excluding punctuation marks after the linearization
Following (Langkilde-Geary, 2002; Belz et al., 2011) and other works on statistical text generation, we access the quality of the linearization module via BLEU score, NIST and exactly matched sentences.
12

In general, Tables 1­4 show that the quality of the presented deep data-driven generator is rather good both during the individual stages of the DSyntS­ SSyntS transition and as part of the DSyntS­ linearized sentence pipeline. Two main problems impede an even better performance figures than those reflected in Tables 1 and 2. First, the introduction of prepositions causes most errors in hypernode detection and lemma generation: when a preposition should be introduced or not and which preposition should be introduced depends exclusively on the subcategorization frame of the governor of the DSyntS node. A corpus of a limited size does not capture the subcategorization frames of ALL predicates. This is especially true for our Spanish treebank, which is particularly small. Second, the inter-hypernode dependency suffers from the fact that the SSyntS tagset is quite fine-grained, at least in the case of Spanish, which makes the task of the classifiers harder (e.g., there are nine different types of verbal objects). In spite of these problems, each set of classifiers achieves results above 88% on the test sets. The results of deep generation in Tables 3 and 4 can be explained by the fact of error propagation: while (only) about 1 out of 10 hypernodes and about 1 out of 10 lemmas are not correct and very little information is lost in the stage of the intrahypernode dependencies determination, already almost 1.75 out of 10 inter-hypernode dependencies, and finally 1 out 10 linear orderings are incorrect for English and more than 2 out 10 for Spanish. As already mentioned above, the size of the training corpus strongly affects the results. Thus, for English, for which the size of the training dataset has been 10 times bigger than for Spanish, the datadriven generator provides, without any tuning, more than 0.2 BLEU points more that for Spanish. A bigger corpus also covers more linguistic phenomena

394

