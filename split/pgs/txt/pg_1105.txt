The syntactic and semantic features were extracted using the ClearNLP parser.4 We used the default models and options for the parser. We treat this model as a strong baseline to which we will add reference-based features. 3.2 Reference-based
SVR #1

predicted score SVR #2 Dense Features (reference-based)  BLEU    w2v cosine w2v align    WordNet align

Our implementation of the reference-based approach ("ref" in 4) uses SVR to estimate a model to predict human scores from various measures of the similarity between the response and information from the scoring guidelines provided to the human scorers. Specifically, we use the following information from 2: (a) sentences expressing key concepts that should be present in correct responses, and (b) small sets of exemplar responses for each score level. For each type of reference, we use the following similarity metrics:  BLEU: the BLEU machine translation metric (Papineni et al., 2002), with the student response as the translation hypothesis. When using BLEU to compare the student response to the (much shorter) sentences containing key concepts, we ignore the brevity penalty.  word2vec cosine: the cosine similarity between the averages of the word2vec vectors (Mikolov et al., 2013) of content words in the response and reference texts (e.g., exemplar), respectively.5,6  word2vec alignment: the alignment method below with word2vec word similarities.  WordNet alignment: the alignment method below with the Wu and Palmer (1994) WordNet (Miller, 1995) similarity score. The WordNet and word2vec alignment metrics are computed as follows, where S is a student response, R is one of a set of reference texts, Ws and Wr are content words in S and R, respectively, and Sim(Ws , Wr ) is the word similarity function:
http://www.clearnlp.com, v2.0.2 The word2vec model was trained on the English Wikipedia as of June 2012, using gensim (http://radimrehurek. com/gensim/) with 100 dimensions, a context window of 5, a minimum count of 5 for vocabulary items, and the default skip-gram architecture. 6 We define content words as ones whose POS tags begin with "N" (nouns), "V" (verbs), "J" (adjectives), or "R" (adverbs).
5 4

Sparse Features (response-based)  Char n-gram      Word n-gram Response length   Dependency    Semantic role

Figure 1: Stacking model for short answer scoring

1 len(S )

Ws

Wr R

max Sim(Ws , Wr )

(1)

When R is one of a set of reference texts (e.g., one of multiple exemplars available for a given score point), we use the maximum similarity over available values of R. In our data, there are multiple exemplars per score point, but only one text (usually, a single sentence) per key concept. In other words, we select the most similar exemplar response for each score level. 3.3 Simple Model Combination One obvious way to combine the response- and reference-based models is to simply train a single model that uses both the sparse features of the student response and the dense, real-valued similarity features. Our experiments (4) include such a model as a strong baseline, using SVR to estimate feature weights. 3.4 Model Combination with Stacking In preliminary experiments with the training data, we observed no gains for the simple combination model over the component models. One potential challenge of combining the two scoring approaches is that the weights for the dense, referencebased features may be difficult to properly esti-

1051

