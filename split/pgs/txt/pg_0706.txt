techniques to the task of unsupervised parsing is, unfortunately, not trivial. The reason is that those parsers are optimally designed for being trained on manually annotated data. If we use existing unsupervised training methods (like EM), learning could be easily misled by a large amount of ambiguity naturally embedded in unannotated training data. Moreover, the computational cost could rapidly increase if the training algorithm is not designed properly. To overcome these difficulties we propose a framework, iterated reranking (IR), where existing supervised parsers are trained without the need of manually annotated data, starting with dependency trees provided by an existing unsupervised parser as initialiser. Using this framework, we can employ the work of Le and Zuidema (2014) to build a new system that outperforms the SOTA unsupervised parser of Spitkovsky et al. (2013) on the WSJ corpus. The contribution of this paper is twofold. First, we show the benefit of using lexical semantics for the unsupervised parsing task. Second, our work is a bridge connecting the two research areas unsupervised parsing and its supervised counterpart. Before going to the next section, in order to avoid confusion introduced by names, it is worth noting that we use un-trained existing supervised parsers which will be trained on automatically annotated treebanks.

training. Relying on the fact that natural language grammars must be unambiguous in the sense that a sentence should have very few correct parses, Tu and Honavar (2012) incorporate unambiguity regularisation to posterior probabilities. Spitkovsky et al. (2012) bootstrap the learning by slicing up all input sentences at punctuation. Spitkovsky et al. (2013) propose a complete deterministic learning framework for breaking out of local optima using count transforms and model recombination. Marecek and Straka (2013) make use of a large raw text corpus (e.g., Wikipedia) to estimate stop probabilities, using the reducibility principle. Differing from those works, Bisk and Hockenmaier (2012) rely on Combinatory Categorial Grammars with a small number of hand-crafted general linguistic principles; whereas Blunsom and Cohn (2010) use Tree Substitution Grammars with a hierarchical non-parametric Pitman-Yor process prior biasing the learning to a small grammar. 2.2 Reranking

2
2.1

Related Work
Unsupervised Dependency Parsing

The first breakthrough was set by Klein and Manning (2004) with their dependency model with valence (DMV), the first model to outperform the right-branching baseline on the DDA metric: 43.2% vs 33.6% on sentences up to length 10 in the WSJ corpus. Nine years later, Spitkovsky et al. (2013) achieved much higher DDAs: 72.0% on sentences up to length 10, and 64.4% on all sentences in section 23. During this period, many approaches have been proposed to attempt the challenge. Naseem and Barzilay (2011), Tu and Honavar (2012), Spitkovsky et al. (2012), Spitkovsky et al. (2013), and Marecek and Straka (2013) employ extensions of the DMV but with different learning strategies. Naseem and Barzilay (2011) use semantic cues, which are event annotations from an outof-domain annotated corpus, in their model during 652

Our work relies on reranking which is a technique widely used in (semi-)supervised parsing. Reranking requires two components: a k -best parser and a reranker. Given a sentence, the parser generates a list of k best candidates, the reranker then rescores those candidates and picks the one that has the highest score. Reranking was first successfully applied to supervised constituent parsing (Collins, 2000; Charniak and Johnson, 2005). It was then employed in the supervised dependency parsing approaches of Sangati et al. (2009), Hayashi et al. (2013), and Le and Zuidema (2014). Closest to our work is the work series on semisupervised constituent parsing of McClosky and colleagues, e.g. McClosky et al. (2006), using selftraining. They use a k -best generative parser and a discriminative reranker to parse unannotated sentences, then add resulting parses to the training treebank and re-train the reranker. Different from their work, our work is for unsupervised dependency parsing, without manually annotated data, and uses iterated reranking instead of single reranking. In addition, both two components, k -best parser and reranker, are re-trained after each iteration.

