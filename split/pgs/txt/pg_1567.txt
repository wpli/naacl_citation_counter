Jenny is holding a hot dog. Jenny is sitting in the sandbox. Jenny is wearing a witch hat. Jenny is wearing purple sunglasses. Jenny is scared of the snake. The cat is sitting in the sandbox. The snake is under the pine tree. The cat is watching Jenny.

"Mike and Jenny decide to make hot dogs on the grill.", "It's a rainstorm and Jenny runs away to stay dry.", "Mike stays beside the fire.", "Jenny is standing next to the tree.", "Mike is sitting next to the fire.", "The hot dog is on the pit."

Figure 5: Examples of descriptions generated by the SMT model for two scenes.

Figure 6: Right scene is generated by SMT model (left scene is the original) given descriptions (bottom) as input.

converted the ranks to ratings on a scale of 1 to 4 (assigning ratings 4. . . 1 to rank placements 1. . . 4). This allowed us to perform Analysis of Variance (ANOVA) which revealed a reliable effect of system type. Specifically, post-hoc Tukey tests showed that our SMT model is significantly ( p < 0.01) better than the other two comparison systems but does not differ significantly from the human goldstandard. We also evaluated more thoroughly our content selection mechanism. Since our system can in principle generate multiple descriptions for a scene, we were interested to see how many of these are indeed relevant. We let the system generate the six best descriptions per scene and asked AMT participants to assess whether they were accurate (are the people, objects and actions mentioned in the description shown in the scene?) and appropriate (is the description relevant for the scene?). Participants answered with "yes", "no", or "maybe". Again we used 100 items from the test set, and elicited 5 responses per item. Table 6 shows the outcome of this study. The majority of first-best descriptions (75.5%) returned by our system are perceived as relevant and scene appropriate. The same is true for 2nd and 3rd best descriptions, whereas the quality of descriptions deteriorates with lower ranks. This suggests that we could generate short discourses describing different viewpoints in a scene. Figure 5 illustrates the descriptions produced by our model for two scenes, whereas Figure 6 shows example output when the system is run in reverse, i.e., it takes descriptions as input and generates a scene. This can be done straightforwardly, without any additional effort, however note that the model is 1513

unaware of the absolute position of objects, it places the cloud next to Jenny.

7

Conclusions

In this paper we presented proof of concept that an SMT-based approach is successful at generating human-like scene descriptions provided that (a) there is a large enough parallel corpus to learn from and (b) a content selection component identifies important scene content. Our results further indicate that instilling some degree of structural information in visual scenes (via the VDG) is beneficial. It allows to describe visual content more accurately and facilitates its rendering in natural language (since the two modalities are structurally similar). The template-based, retrieval, and language modeling systems do not use this structural information, and even though their descriptions are largely grammatical, they are not as felicitous. Our results also point to difficulty of the task. Even when computer vision is taken out of the equation, and the description language is simple, human-written text is still preferable (see Table 5). In the future, we would like to develop better content selection models (e.g., identify surprising aspects in a scene) and more accurate grounding strategies (e.g., via discriminative alignment). Acknowledgments We are grateful to Lukas Dirzys for his help with the LBL and MLBL models. Special thanks to Frank Keller for his comments on an earlier version of this paper and Larry Zitnick whose talk at the UW MSR Summer Institute 2013 insipred this work.

