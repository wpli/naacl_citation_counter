· During the error analysis we found out that our algorithm predicted more specific and finegrained categories compared to those provided by humans.

2

Product Categorization Task Definition
Task Definition: Given a set of titles describing products and a product taxonomy of 319 nodes organized into 6 levels, the goal is to build a multi-class classifier, which can accurately predict the product category of a new unlabeled product title.

We define our task as:

et al., 2009): one-against-all (OAA) and error correction tournament (ECT). OAA reduces the Kway multi-classification problem into multiple binary classification tasks by iteratively classifying each product title for category K and comparing it against all other categories. ECT also reduces the problem to binary classification but employs a single-elimination tournament strategy to compare a set of K players and repeats this process for O(log K ) rounds to determine the multi-class label.

4

Feature Modeling

The algorithm takes as input a product title "MB22B 22 piece with bonus travel/storage bag" and returns as output the whole product category hierarchy "Home and Garden >Kitchen&Dining>Kitchen Appliances>Blenders" as illustrated in Figure 1.
!!!!!!!!!!!!!!!!!!!MB22B!22!Piece!With!Bonus!Travel!/Storage!Bag!

Next we describe the set of features we used to train our model. 4.1 Lexical Information

N-grams are commonly used features in text classification. As a baseline system, we use unigram and bigram features. 4.2 Mutual Information Dictionary

product(( categoriza.on(

Home%&%Garden%>%% %Kitchen%&%Dining%>%% % %Kitchen%Appliances%>% % % % %Blenders%

Figure 1: Example of Product Title Categorization.

3

Classification Methods

We model the product categorization task as classification problem, where for a given collection of labeled training examples P , the objective is to learn a classification function f : pi  ci . Here, pi is a product title and ci  {1, ..., K } is its corresponding category (one of 319 product taxonomy classes). We learn a linear classifier model f (parametrized by a weight vector w) that minimizes the misclassification error on the training corpus P : min  (ci = f (w, pi )) + ||w||2 2
w

where,  (.) is an indicator function which is 1 iff the prediction matches the true class and  is a regularization parameter. For our experiments, we used two multiclassification algorithms from the large scale machine learning toolkit Vowpal Wabbit (Beygelzimer 1330

pi P

Lexical features require very large amount of training data to produce accurate predictions. To generalize the categorization models, we use semantic dictionaries, which capture the presence of a term with a product category. Ideally, we would like to use existing dictionaries for each product category, however such information is not available. For instance, WordNet provides at most synonyms/hyponyms/hypernyms for a given category name, but it does not provide products, brand names and the meaning of abbreviations. We decided to generate our own dictionaries, by taking all product titles and estimating the mutual inf (w,Ci ) formation M I (w, Ci ) = log (f (w, ).f (,Ci ) of every word w and product category Ci . For the dictionary, we keep all word-category pairs with MI above 5. During feature generation, for each title we estimate the percentage of words found with each category Ci according to our automatically generated dictionary. The dimensions of the feature vector is equal to the total number of categories. The size of the generated lexicon is 34, 337 word-category pairs. 4.3 LDA Topics

We also incorporate latent information associated with product titles using topic modeling techniques.

