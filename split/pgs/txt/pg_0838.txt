5.2.3 The Effect of Features Next we evaluate the contributions of different features. We show results for four experiments: (i) use just one type of features; (ii) combine the internal features with features from just one external resource; (iii) incrementally add external resources one by one; (iv) leave out each feature type. Table 3 shows the ROUGE-2 results when we only apply one type of features. First, we can see that the system with the internal features has already outperformed the baseline which used document frequency as the weight. It shows that the other chosen internal features (beyond document frequency) are useful. Second, when we use the features from only one external resource, the results from some resources are competitive compared to that from the system using internal features. In particular, when using the LM scores, Wiki or Word Embedding features, the results are slightly better than the internal features. Using DBpedia or SentiWordNet has worse results than the internal features. This is because the SentiWordNet features themselves are not very discriminative. For DBpedia, since it only has feature values for the bigrams containing name entities, it will only assign weights for those bigrams. Therefore, only considering DBpedia features means that the ILP decoder would prefer to choose bigrams that are name entities with positive weights.
Internal LM Word Embedding Wikipedia DBpedia WordNet SentiwordNet 2008 10.40 10.58 10.67 10.61 8.35 10.39 9.90 2009 11.76 11.86 11.96 11.90 9.85 11.76 10.80 2010 10.42 10.48 10.58 10.52 9.46 10.40 10.08 2011 12.91 12.94 13.02 13.00 11.00 12.86 12.50

itself, therefore they provide topic relevant background information. The LM score features are extracted from large amounts of news article data, and are good representation of the general importance of bigrams for the test domain. In contrast, WordNet information is collected from a more general aspect, which may not be a very good choice for this task. Also notice that even though the features from DBpedia and sentiwordnet do not perform well by themselves, after the combination with internal features, there is significant improvement. This proves that the features from DBpedia and sentiwordnet provide additional information not captured by the internal features from the documents.
Internal +LM +Word Embedding +Wikipedia +WordNet +SentiwordNet +DBpedia 2008 10.40 10.76 10.92 10.81 10.68 10.60 10.69 2009 11.76 12.03 12.12 12.08 11.96 11.96 12.00 2010 10.42 10.80 10.85 10.76 10.71 10.63 10.70 2011 12.91 13.11 13.24 13.17 12.99 12.96 13.07

Table 4: ROUGE-2 results using internal features combined with features from just one external resource.

Table 5 shows the results when adding features one by one. The order is based on its individual impact when combined with internal features. The results show that Wiki, LM and DBpedia features give more improvement than WordNet and SentiWordNet features. This shows the different impact of the external resources. We can see there is consistent improvement when more features are added.
1: Internal +Word Embedding 2: 1+Wiki 3: 2+LM 4: 3+DBpedia 5: 4+WordNet 6: 5+SentiWordNet 2008 10.92 11.22 11.41 11.65 11.75 11.84 2009 12.12 12.25 12.41 12.60 12.67 12.77 2010 10.85 11.15 11.37 11.61 11.70 11.78 2011 13.24 13.47 13.68 13.77 13.90 13.97

Table 3: ROUGE-2 results using one feature type.

Table 4 shows the results when combining the internal features with features from one external resource. We can see that the features from Word Embedding model outperform others, suggesting the effectiveness of this semantic similarity measure. Features from the LM scores and Wiki are also quite useful. Wiki pages are extracted for the test topic

Table 5: ROUGE-2 results using features incrementally combined.

Table 6 shows the feature ablation results, that is, each row means that the corresponding features are

784

