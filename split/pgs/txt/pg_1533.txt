feature sets. For the Lexical feature set, the most informative features for negative cases are bigrams that begin with quotation marks. Since we manually assign all punctuation, including quotation marks, as true negatives, this is perhaps unsurprising. For positive cases, many of the "offset" style features emerge as informative. For example, words that appear one or two words before a comma or period are more likely to involve framing. Similarly, words that occur just before or just after prepositions or conjunctions -- to, and, in, of, etc. -- are more likely classified as framing (i.e., positive). This result aligns with some of our pilot study findings about the relationships between such function terms the words surrounding them. Interestingly, though, these features do not resemble the catchphrases and keywords described in the framing literature (Entman, 1993; Gamson and Modigliani, 1989). For the Theoretically Informed feature set, which adds in imagery, figurativeness, and other dictionary-based features, roughly the same kinds of lexical features are most important in identifying negative cases. For positive cases, constructs such as descriptiveness and abstractness play important roles, but mostly when words in the context do not appear in these dictionaries, calling into question the importance of imagery, metaphor, and other figurative language (Gamson and Modigliani, 1989). Other dictionary-based features, such as being an entailment word, having an entailment in context, having a bias term in context, or having a subjective term in context, become important in identifying positive cases. Some of the lexical features, such as proximity to a comma or period, also remain informative. With All features, which adds features for document structure and grammatical structure, two major differences occur. First, some elements of document structure become important features for positive cases, including sentence length and TFIDF. Second, various part of speech tags, mostly NN and NNP, become important to identifying framing. These findings suggest that the choice of terms used to label concepts or entities can indicate framing. Indeed, entity type, both of the word and its context, also emerges as an informative feature. 1479

7

Discussion

Chiefly important among the results, three of our feature sets were able to achieve F1 scores on par with those of human annotators. This result provides encouraging evidence that a machine classifier can effectively accomplish the task of identifying the language that invokes framing in political news coverage. That said, examining the results more closely exposes that, in order to achieve this level of performance, the classifier makes a trade-off. Specifically, the classifier is far more aggressive than humans, resulting in significantly higher recall but lower precision. We did experiment with different post-hoc decision thresholds to make the classifier less aggressive. However, as we the decision threshold rose, recall fell much more quickly than precision increased, resulting in lower overall F1 scores. Moreover, this precision-recall trade off may have different ramifications in different applications. For example, in terms of supporting frame reflection, would it be harmful or distracting for a machine classifier to mark too many words as frameinvoking, thereby potentially overwhelming a potential user? Or would it be worse if the classifier were too sparse, missing certain important key words or phrases? These are questions for later empirical work that incorporates the results of this classifier into interactive systems to support frame reflection. Also, our results above identifying important features within the classifier contribute to and build on prior computational work. For example, Recasens et al. (Recasens et al., 2013) find entailment (Berant et al., 2012), implicature (Karttunen, 1971), subjectivity (Riloff and Wiebe, 2003), and other related constructs helpful in identifying bias. The results above suggest that, when included, such features also emerge as important for identifying the language of framing. However, the feature sets that include dictionaries of these terms do not perform statistically significantly better than those feature sets without them. This result supports our initial assertion that bias and framing, while conceptually related, are separate constructs that are each perceived and instantiated via different linguistic cues. These findings also relate to recent efforts at identifying which frame(s) are operating in a text (e.g.,

