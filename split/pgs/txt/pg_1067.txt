tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today's commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage of NNs in preordering for SMT.

P REORDER PARSE T REE 1 for each node n  N, |Cn | > 1 2 F  G ET F EATURES(n) 3 for each pair of nodes i, j  Cn , i = j 4 F  F  G ET F EATURES(i) 5 F  F  G ET F EATURES(j ) 6 Fc  F EATURE C OMBINATIONS(F ) 7 pn (i, j ) = L OG R EG P REDICT(F, Fc ) 8 end for 9 n  S EARCH P ERMUTATION(pn ) 10 S ORT(Cn , n )
Figure 1: Pseudocode for the preordering scheme of Jehl et al (2014)

in the aligned parallel corpus. 2.1 Applying Neural Networks "A (feedforward) neural network is a series of logistic regression models stacked on top of each other, with the final layer being either another logistic regression model or a linear regression model" (Murphy, 2012). Given this, we propose a straightforward alternative to the above framework: replace the linear logistic regression model by a neural network (NN). This way a superior modeling performance of the nodeswapping phenomenon is to be expected. Additionally, feature combination need not be engineered anymore because that is learnt by the NN in training (line 6 in Figure 1 is skipped). Training the neural network requires the same labeled samples that were used by Jehl et al (2014). We use the NPLM toolkit out-of-the-box (Vaswani et al., 2013). The architecture is a feed-forward neural network (Bengio et al., 2003) with four layers. The first layer i contains the input embeddings. The next two hidden layers (h1 , h2 ) use rectified linear units; the last one is the softmax layer (o). We did not experiment with deeper NNs. For our purposes, the input vocabulary of the NN is the set of all possible feature indicator names that are used for preordering1 . There are no OOVs. Given the sequence of  20 features seen by the
Using a vocabulary of the 5K top-frequency English words, 50 word classes, approximately 40 POS tags and 50 dependency labels, the largest input vocabulary in our experiments is roughly 30,000.
1

2

Preordering as node-pair swapping

Jehl et al (2014) describe a preordering scheme based on a logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted in order to have a more monotonically-aligned parallel corpus. Their method can be briefly summarised by the pseudocode of Figure 1. Let N be the set of nodes in the source tree, and let Cn be the set of children nodes of node n. For each node with at least two children, first extract the node features (lines 1-2). Then, for each pair of its children nodes: extract their respective features (lines 4-5), produce all relevant feature combinations (line 6), and store the node-pair swapping probability predicted by a logistic regression model based on all available features (line 7). Once all pair-wise probabilities are stored, search for the best global permutation and sort Cn accordingly (lines 9-10). As features, Jehl et al (2014) use POS tags and dependency labels, as well as the identity and class of the head word (for the parent node) or the left/rightmost word (for children nodes). These are combined into conjunctions of 2 or 3 features to create new features. For logistic regression, they train a L1regularised linear model using LIBLINEAR (Fan et al., 2008). The training samples are either positive/negative depending on whether swapping the nodes reduces/increases the number of crossed links 1013

