20 Newsgroups
CONFLICT 1.0 LOW MED HIGH algorithm 0.8 0.6 0.4 CONFLICT 1.0 0.8 0.6 0.4 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 0 5 MomResp LogResp

Accuracy

20 Newsgroups
LOW 10 15 0 5 MED 10 15 0 5 HIGH 10 15

Majority

Test Accuracy

Number of annotated instances x 1,000

algorithm MomResp LogResp

Number of annotated instances x 1,000

Figure 3: Top row: Inferred label accuracy on three-deep annotations. A majority vote baseline is shown for reference. Bottom row: Generalization accuracy on a test set. Majority vote is not shown since it does not generate test set predictions. Each column uses the indicated simulated annotator pool. M OM R ESP benefits significantly from MF. We suspect that this disparity could be reduced via hyperparameter optimization as indicated by Asuncion et al. (2009). However, that investigation is beyond the scope of the current work. L OG R ESP using MF is compared with L OG R ESP using expectation maximization (EM) as in (Raykar et al., 2010). L OG R ESP with MF displays minor improvements over L OG R ESP with EM. This is consistent with the modest gains that Liu et al. (2012) reported when comparing variational and EM inference for the I TEM R ESP model. 5.4 Discriminative (L OG R ESP) versus Generative (M OM R ESP) Figure 3 plots the accuracy of labels inferred from annotations. The second row of Figure 3 plots generalization accuracy using the inferred model parameters  (and  in the case of M OM R ESP) on held-out test sets with no annotations. The generalization accuracy curves of M OM R ESP and L OG R ESP may be compared with those of na¨ ive Bayes and logistic regression, respectively. Recall that in the special case where annotations are both flawless and trusted (via diagonal confusion matrices  ) then M OM R ESP and L OG R ESP simplify to semi-supervised na¨ ive Bayes and logistic regression classifiers, respectively. Notice that M OM R ESP climbs more steeply than L OG R ESP in all cases. This observation is in keeping with previous work in supervised learning. Ng and Jordan (2001) argue that generative and discriminative models have complementary strengths: generative models tend to have steeper learning curves and converge in terms of parameter values after only log n training examples, whereas discriminative models tend to achieve higher asymptotic levels but converge more slowly after n training examples. The second row of Figure 3 shows that even after training on three-deep annotations over the entire 20 newsgroups dataset, L OG R ESP's data model does not approach its asymptotic level of performance. The

We run M OM R ESP and L OG R ESP with MF inference on the cross product of datasets and annotator pools. Inferred label accuracy on items that have been annotated is the primary task of crowdsourcing; we track this measure accordingly. However, the ability of these models to generalize on unannotated data is also of interest and allows better comparison with traditional non-crowdsourcing models. Figure 3 plots learning curves for each annotator pool on the 20 Newsgroups dataset; results on other datasets are summarized in Table 2. The first row of 888

