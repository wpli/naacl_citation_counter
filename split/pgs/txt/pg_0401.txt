character probabilities. The CTC collapsing function achieves this by introducing a special blank symbol, which we denote using " ", and collapsing any repeating characters in the original length T output. This output symbol contains the notion of junk or other so as to not produce a character in the final output hypothesis. Our transcripts W come from some set of symbols  but we reason over  =   . We denote the collapsing function by (·) which takes an input string and produces the unique collapsed version of that string. As an example, here are the set of strings Z of length T = 3 such that (z ) = hi, z  Z : Z = {hhi, hii, hi, h i, hi }. There are a large number of possible length T sequences corresponding to a final length  transcript hypothesis. The CTC objective function LCTC (X, W ) is a likelihood of the correct final transcript W which requires integrating over the probabilities of all length T character sequences CW = {C : (C ) = W } consistent with W after applying the collapsing function, LCTC (X, W ) =
CW T

p(c|x) h(b) h(f ) h
(1)

W (s) +

W (b) +

W (s)

W (b) +

W (s)

W (f ) W (2) W (1)

W (f ) W (2) W (1)

W (2) W (1) t+1

x t-1 t

Figure 1: Deep bi-directional recurrent neural network to map input audio features X to a distribution p(c|xt ) over output characters at each timestep t. The network contains two hidden layers with the second layer having bi-directional temporal recurrence. A DBRNN computes the distribution p(c|xt ) using a series of hidden layers followed by an output layer. Given an input vector xt the first hidden layer activations are a vector computed as, h(1) =  (W (1)T xt + b(1) ), (3)

p(C |X ) (2) p(ct |X ).
CW t=1

=

Using a dynamic programming approach we can exactly compute this loss function efficiently as well as its gradient with respect to our probabilities p(ct |X ). 2.2 Deep Bi-Directional Recurrent Neural Networks

Our loss function requires at each time t a probability distribution p(c|xt ) over characters c given input features xt . We model this distribution using a DBRNN because it provides an expressive model which explicitly accounts for the sequential relationships that should exist in our task. Moreover, the DBRNN is a relatively straightforward neural network architecture to specify, and allows us to learn parameters from data rather than more explicitly specifying how to convert audio features into characters. Figure 1 shows a DBRNN with two hidden layers. 347

where the matrix W (1) and vector b(1) are the weight matrix and bias vector. The function  (·) is a point-wise nonlinearity. We use  (z ) = min(max(z, 0), µ). This is a rectified linear activation function clipped to a maximum possible activation of µ to prevent overflow. Rectified linear hidden units have been show to work well in general for deep neural networks, as well as for acoustic modeling of speech data (Glorot et al., 2011; Zeiler et al., 2013; Dahl et al., 2013; Maas et al., 2013) We select a single hidden layer j of the network to have temporal connections. Our temporal hidden layer representation h(j ) is the sum of two partial hidden layer representations, ht
(j )

= ht

(f )

+ ht .

(b)

(4)

The representation h(f ) uses a weight matrix W (f ) to propagate information forwards in time. Similarly, the representation h(b) propagates information backwards in time using a weight matrix W (b) . These partial hidden representations both take input from the previous hidden layer h(j -1) using a weight

