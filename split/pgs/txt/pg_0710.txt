N(Di ) is. Moreover, the diversity of N(Di ) is inversely proportional to the fitness. When the fitness decreases, patterns existing in the training treebank become less certain to the parser, patterns that do not exist in the training treebank thus have more chances to appear in k -best candidates. This leads to high diversity of N(Di ). We blindly set k = 10 in all of our experiments. With the MSTParser, there are two hyperparameters: itersMST , the number of epochs, and training-kMST , the k -best parse set size to create constraints during training. training-kMST is always 1 because constraints from k -best parses with almost incorrect training parses are useless. Because itersMST controls the fitness of the parser to training treebank Di , it, as pointed out above, determines the distance from N(Di ) to Di and the diversity of the former. Therefore, if we want to encourage the local search to explore more distant areas, we should set itersMST low. In our experiments, we test two strategies: (i) MaxEnc, itersMST = 1, maximal encouragement, and (ii) MinEnc, itersMST = 10, minimal encouragement. 5.2 Tuning Reranker R Tuning the reranker R is to set values for dimIORNN , the dimensions of inner and outer representations, and itersIORNN , the number of epochs to train the IORNN. Because the -order model is very expressive and feed-forward neural networks are universal approximators (Cybenko, 1989), the reranker is capable of perfectly remembering all training parses. In order to avoid this, we set dimIORNN = 50, and set itersIORNN = 5 for very early stopping. 5.3 Tuning multi-phase IR Because Marecek and Straka (2013)'s parser does not distinguish training data from test data, we postulate S0 = S1 . Our system has N phases such that S0 , S1 contain all sentences up to length l1 = 15, Si (i = 2..N ) contains all sentences up to length li = li-1 + 1, and SN contains all sentences up to length 25. Phase 1 halts after 100 iterations whereas all the following phases run with one iteration. Note that we force the local search in phase 1 to run intensively because we hypothesise that most of the important patterns for dependency parsing can be found within short sentences. 656

6
6.1

Experiments
Setting

We use the Penn Treebank WSJ corpus: sections 02-21 for training, and section 23 for testing. We then apply the standard pre-processing5 for unsupervised dependency parsing task (Klein and Manning, 2004): we strip off all empty sub-trees, punctuation, and terminals (tagged # and $) not pronounced where they appear; we then convert the remaining trees to dependencies using Collins's head rules (Collins, 2003b). Both word forms and gold POS tags are used. The directed dependency accuracy (DDA) metric is used for evaluation. The vocabulary is taken as a list of words occurring more than two times in the training data. All other words are labelled `UNKNOWN' and every digit is replaced by `0'. We initialise the IORNN with the 50-dim word embeddings from Collobert et al. (2011) 6 , and train it with the learning rate 0.1, 6.2 Results We compare our system against recent systems (Table 1 and Section 2.1). Our system with the two encouragement levels, MinEnc and MaxEnc, achieves the highest reported DDAs on section 23: 1.8% and 1.2% higher than Spitkovsky et al. (2013) on all sentences and up to length 10, respectively. Our improvements over the system's initialiser (Marecek and Straka, 2013) are 9.1% and 4.4%. 6.3 Analysis In this section, we analyse our system along two aspects. First, we examine three factors which determine the performance of the whole system: encouragement level, lexical semantics, and starting point. We then search for what IR (with the MaxEnc option) contributes to the overall performance by comparing the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by its initialier, i.e. Marecek and Straka (2013). The effect of encouragement level Figure 2 shows the differences in DDA between using MaxEnc and MinEnc in each phase: we comhttp://www.cs.famaf.unc.edu.ar/ ~francolq/en/proyectos/dmvccm 6 http://ml.nec-labs.com/senna/. These word embeddings were unsupervisedly learnt from Wikipedia.
5

