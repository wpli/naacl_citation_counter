the test documents, and features extracted from external resources. The goal is to better predict the importance of a bigram, which we expect will help the ILP module better determine whether to include the bigram in the summary. 4.1 Internal Features These features are generated from the provided test documents (note our task is multi-document summarization, and there is a given query topic. See Section 5 for the description of tasks and data).  Frequency of the bigram in the entire set.  Frequency of the bigram in related sentences.1  Document frequency of the bigram in the entire set.  Is this bigram in the first 1/2/3 sentence?  Is this bigram in the last 1/2/3 sentence?  Similarity with the topic title, calculated by the number of common tokens in these two strings, divided by the length of the longer string. 4.2 Importance Score based on Language Models

where (LMA ) and (LMO ) are the LMs from the abstracts and the original news articles. Note that one difference from (Hong and Nenkova, 2014) is that we calculate these scores for a bigram, not a word. As (Hong and Nenkova, 2014) showed, a higher value from the score in Formula 8 means the words are favored in the summaries, and vice verse in Formula 9. In addition to the above features, we also include the likelihood P rA (b) and P rO (b) based on the two LMs, and the absolute and relative difference between them: P rA (b) - P rO (b), P rA (b)/P rO (b). 4.3 Similarity based on Word Embedding Representation

Given the recent success of the continuous representation for words, we propose to use an unsupervised method to induce dense real-valued low dimensional word embedding, and then use the inner product as a measure of semantic similarity between two strings. In the word embedding model, every word can be represented by a vector w. We define the similarity between two sequences S 1 = x1 , x2 , ...xk and sequence S 2 = y1 , y2 , ...yl as the average pairwise similarity between any two words in them: Sim(S 1, S 2) =
k i=1 l j =1 xi

The idea is to train two language models (LMs), one from the original documents, and the other one from the summaries, and compare the likelihood of a bigram generated by these two LMs, which can indicate how often a bigram is used in a summary. Similar to previous work in (Hong and Nenkova, 2014), we leveraged The New York Times Annotated Corpus (LDC Catalog No: LDC2008T19), which has the original news articles and human generated abstracts. We build two language models, from the news articles and the corresponding summaries respectively. We used about 160K abstract-original pairs. The KL scores for a bigram are defined as follows: P rA (b) KL(LMA |LMO )(b) = P rA (b)  ln P rO (b) P rO (b) KL(LMO |LMA )(b) = P rO (b)  ln P rA (b)
1

 yj

kl

(10)

Based on such word embedding models, we derive two similarity features: (1) similarity between a bigram and the topic query, and (2) similarity between a bigram and top-k most frequent unigrams in this topic. We trained two word embedding models, from the abstract and news article collections in the New York Times Annotated Corpus, and thus have two sets of the above similarity features. We use the continuous bag-of-words model introduced by (Mikolov et al., 2013), and the tool word2vec2 to obtain the word embeddings. 4.4 Similarity based on WordNet3 Similar to the above method, here we still focus on measuring the similarity between a bigram and the topic query, but based on WordNet. We use WordNet to identify the synonyms of nouns, verbs,
2 3

(8) (9)

Note that we do not use all the sentences in the ILP module. The `relevant' sentences are those that have at least one bigram with document frequency larger than or equal to three.

https://code.google.com/p/word2vec/ http://wordnet.princeton.edu/

781

