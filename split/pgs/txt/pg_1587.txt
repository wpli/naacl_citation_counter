Baseline SLP w/ PMI SLP w/ Cont. Repr. GLP LLP LLP w/ backoff

Tune 39.33 40.93 41.31 40.46 41.17 41.48

Test 38.09 39.16 39.34 38.68 39.57 39.70

Time (hr) 10,000 120+200 20+200 30+200 30+200

Table 3: Arabic-English translation accuracy of structured label propagation with PMI (SLP) and with continuous representations (SLP w/ PMI), the global linear projection (GLP), our local linear projection (LLP) and with an added backoff scheme (LLP w/ backoff). For applicable methods, we list the running time to compute distributional representations as a separate term in the time column. This is usually only required once per language which is why we report it separately.

the bin; RBV: hypercube width and number of bins for each dimension). Table 2 shows that RBV gives significantly better performance than LSH, both in terms of accuracy and speed. RBV reduces the false negative ratio by 1/3 compared to LSH and is 3.6 times faster. This is in line with Goldstein et al. (2005) who observed that the performance of LSH degrades in high dimensional space. We therefore use RBV in the following experiments. 5.3 Evaluation of Rule Generation Next, we evaluate the quality of the generated translation rules for Arabic-English translation (Table 3) using either SLP, the global linear projection (GLP), or the local linear projection (LLP). Our baseline system is an in-house phrase-based system similar to Moses with a 4-gram language model. The underlying log-linear model comprises of 13 features: two maximum likelihood translation probabilities, two lexicalized translation probabilities, five hierarchical reordering model features (Galley and Manning, 2008), one language model, word penalty, phrase length, and distortion penalty), and is tuned with minimum error rate training (MERT; Och 2003). Translation quality is measured with BLEU (Papineni et al., 2002). For comparison, we reimplemented the graphbased method in Saluja et al. (2014). This method calculates the pairwise mutual information (PMI) between phrases, and employs all the techniques mentioned in Saluja et al. (2014) to speedup the 1533

computations. Our reimplementation achieves similar performance to Saluja et al. (2014) (with a negligible  0.06 drop in BLEU). We parallelized the algorithm on a cluster since a single core implementation would run for  10k hours.5 Our continuous phrase based version of SLP is orders of magnitudes faster than the SLP variant of Saluja et al. (2014) because it replaces the computationally expensive PMI calculation by an approximated k -NN query in distributional space. Moreover, our variant of SLP even improves translation quality by 0.2-0.3 BLEU. Overall, our version of SLP improves the baseline by 2.0 BLEU on the tuning set and by 1.3 BLEU on the test set. The linear projection based methods, GLP and LLP, are in turn again several times faster than SLP with continuous representations. This is because they require significantly fewer k -NN queries. For both GLP and LLP, we retrieve the 200 nearest neighbors of the projected point. For LLP, the local projection is calculated based on the 500 nearest labeled neighbors of the infrequent source phrase. LLP achieves slightly better accuracy on the test set than PMI-based SLP but at four times the speed. GLP is the fastest method but also the least accurate, improving the baseline only by about 0.6 BLEU. We explore this result in more detail in the next section. Overall, our local projection outperforms the global projection by 0.9 BLEU on the test set. For some infrequent source phrases, approximated k -NN query does not retrieve enough ( d) neighbors to learn a local linear projection. For these phrases, we employ a backoff strategy that uses the translations of their neighbors as additional translation candidates. This strategy provides helpful additional rules for LLP (Table 3).6 5.4 Evaluation of Global Linear Projection

To learn why GLP does not generate high quality translation rules, we run an extra experiment to measure the projection quality of GLP. We train a global linear projection on an increasConfirmed with the authors of Saluja et al. (2014) from personal communication. 6 The backoff scheme in the Arabic-English setting generates around 15% of the translations rules, which adds 0.13 BLEU on the test set. This is not a big improvement and so we did not employ this scheme for our Urdu-English experiments.
5

