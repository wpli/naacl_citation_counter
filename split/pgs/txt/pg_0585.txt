Latin, we removed punctuation because PROIEL does not contain punctuation. We also split off the clitics ne, que and ve if the resulting token was accepted by L AT M OR (Springmann et al. (2014)). Following common practice, we normalized the text by replacing digits with 0s.6 In our experiments, we extract representations for the 250,000 most frequent word types. This vocabulary size is comparable to other work; e.g., Turian et al. (2010) use 269,000 types. This threshold yields low fractions of uncovered tokens7 for English and Latin (.009 and .02). For the other languages, this fraction rises to .04. We also extract the morphological readings of the words in this vocabulary using MAGYARLANC (Hungarian, Zsibrita et al. (2013)), F REELING (English and Spanish, Padr´ o and Stanilovsky (2012)), SMOR (German, Schmid et al. (2004)), an MA from Charles University (Czech, Haji c (2001)) and L AT M OR (Latin, Springmann et al. (2014)). Throughout this paper we extract one feature for each cluster id or MA reading of the current word form. For example, SMOR produces two readings for the German word form erhielt `received': <1><S G ><P A S T ><I N D > and <2><S G ><P A S T ><I N D >, we thus fire two features representing the respective tags whenever erhielt is seen in the data. We also experimented with cluster indexes of neighboring uni/bigrams, but obtained no consistent improvement. For the dense embeddings we analogously extract the vector of the current word form.

across six languages (M¨ uller et al., 2013). In order to make sure that it is also robust in an OOD setup we compare it to the two popular taggers SVMTool (Gim´ enez and Marquez, 2004) and Morfette (Chrupala et al., 2008). The results are summarized in Table 1. MarMoT uses stochastic gradient descent and produces different results in each training run. We therefore always report the average of five runs. The OOD numbers are macro-averages over the different OOD data sets of a language. 9 The tables in this paper are based on the development sets; the only exception to this is Table 5, which is based on the test set. MarMoT outperforms SVMTool and Morfette on every language and setup (ID / OOD) except for the Spanish OOD data set. For Czech, German and Latin the improvements over the best baseline are >1. Different orders of MarMoT behave as expected: higher-order models (order>1) outperform first-order models. The only exception to this is Latin. This suggests a drastic difference of the tag transition probabilities between the Latin ID and OOD data sets. Given the results in Table 1 and for simplicity we use an second-order MarMoT model in all subsequent experiments. LM-based clustering. We first compare different implementations of LM-based clustering. The implementation of Brown clustering by Liang (2005) is most commonly used. Its hierarchical binary structure can be used to extract clusterings of varying granularity by selecting different prefixes of the path from the root to a specific word form. Following other work (Ratinov and Roth, 2009; Turian et al., 2010), we induce 1000 clusters and select path lengths 4, 6, 10 and 20. We call this representation Brown path. We compare Brown path to mkcls10 (Och, 1999) and MarLiN. These implementations just induce flat clusterings of a certain size; we thus run them for cluster sizes 100, 200, 500 and 1000 to also obtain cluster ids of different sizes. The cluster sizes roughly resemble the granularity obtained in Brownpath . We call the corresponding mod9 Throughout this paper we use the approximate randomization test (Yeh, 2000) to establish significance. To this end, we compare the output of the medians of the five independent models. We regard p-values <.05 as significant. 10 mkcls implements a similar training algorithm as MarLiN, but uses simulated annealing, not greedy maximization.

5

Experiments

For all our experiments we use MarMoT (M¨ uller et al., 2013) a joint POS and morphological tagger.8 The CRF tagger employs a pruning strategy on forward-backward lattices to efficiently handle big tag sets and higher orders. Its feature set is similar to Ratnaparkhi (1996) and Toutanova et al. (2003) and includes prefixes, suffixes, immediate lexical context and shape features based on capitalization, special characters and digits. MarMoT was shown to be a competitive POS and morphological tagger
czechtok/ 6 For statistics of the unlabeled data sets cf. Table 3 of the appendix. 7 Cf. Table 4 in the appendix.
8

http://cistern.cis.lmu.de/marmot/

531

