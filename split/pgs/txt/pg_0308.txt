Original CCA-1 DCCA-1 (MostBeat) Figure 2: t-SNE visualization of synonyms (green) and antonyms (red, capitalized) of dangerous. d (w) - (c (w) + o (w)). If (w) < 0, then word pair w was closer to the human ranking using DCCA. Table 3 shows word pairs from SimLex-999 with high human similarity ratings ( 7 out of 10); column 1 shows pairs with smallest  values, and column 2 shows pairs with largest  values. Among pairs in column 1, many contain words with several senses. Using bilingual information is likely to focus on the most frequent sense in the bitext, due to our use of the most frequently-aligned German word in each training pair. By contrast, using only monolingual context is expected to find an embedding that blends the contextual information across all word senses. Several pairs from column 2 show hypernym rather than paraphrase relationships, e.g., authorcreator and leader-manager. Though these pairs are rated as highly similar by annotators, linear CCA made them less similar than the original vectors, and DCCA made them less similar still. This matches our intuition that bilingual information should encourage paraphrase-like similarity and thereby discourage the similarity of hypernym-hyponym pairs. Visualizations We visualized several synonymantonym word lists and often found that DCCA more cleanly separated synonyms from antonyms than CCA or the original vectors. An example of the clearest improvement is shown in Fig. 2. vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Ko cisk` y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013).

6 Conclusion
We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks. Future work could compare DCCA to other nonlinear approaches discussed in §5, compare different languages as multiview context, and extend to aligned phrase pairs, and to unaligned data.

5 Related work
Previous work has successfully used translational context for word representations (Diab and Resnik, 2002; Zhao et al., 2005; T¨ ackstr¨ om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´ o, 2010; Sumita, 2000) or via unsuper254

Acknowledgments
We are grateful to Manaal Faruqui for sharing resources, and to Chris Dyer, David Sontag, Lyle Ungar, and anonymous reviewers for helpful input.

