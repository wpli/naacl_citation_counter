Implementation and Complexity Analysis.3 We implemented the above model on top of the hierarchical phrase-based decoder cdec (Dyer et al., 2010), but there are no limitations for applying this approach to phrase-based systems (Koehn et al., 2007). Procedurally, after cdec yields the translation forest, we compute the overlap of IR feature activations between edges in the forest and the document candidates. The Inside algorithm is only carried out for documents that activate at least one IR feature in the search space. For documents with no activation we can skip the computation of scores and assign the SMT Viterbi score, which constitutes a lower bound on the model score. For a single query q , forced decoding requires a single pass over the topologically sorted search space to find IR feature activations along hyperedges, yielding a complexity of O(|V | + |E |). The dynamic programming procedure that computes a score for a document requires another pass over the forest evaluating the extended edge weight (4) for every edge e  E , where the dot product for translation features is already precomputed by cdec , and the retrieval part depends on the number of active IR features,  := |ir (r(e), d)|. Overall complexity for a single query and all documents d  C is thus O |V | + |E | + |V | + |E |    |C| . As noted above, we can reduce the quantity |C| by checking if a document candidate shares any IR features with the search space and avoid superfluous executions of the Inside algorithm. In our experiments on Wikipedia data, we found that this check reduces |C| to about 64% of its original size. This pre-filtering is similar to the coarse query approach of Dong et al. (2014), who score only documents that contain at least one term in the query lattice. We further reduce runtime of the inference procedure by using approximate decoding. We experimented with using a beam search approach to limit the number of weight evaluations in (4) for incoming edges at each node. The max operation of the tropical semiring is discontinued once the number of considered incoming edges at a node exceeds the size of the beam.
The complexity of the construction of the translation forest including the language model is common to BOW-FD and the other baselines and thus not included in the following analysis.
3

4

Learning to Decode for Retrieval

We now turn to the problem of learning parameter weights for the BOW-FD model. The objective is to prefer a relevant document d+ over an irrelevant one d- by assigning a higher score to d+ than to d- , score(q, d+ ; w) > score(q, d- ; w). We sample a set of preference pairs P = {(d+ , d- )|rl(d+ , q ) > rl(d- , q )} from relevance-annotated data where rl(d, q ) indicates the relevance level of a document given query. Furthermore, we require the difference of scores to satisfy a certain margin: score(q, d+ ; w) > score(q, d- ; w) + , where the margin is defined as  = rl(d+ , q ) - rl(d- , q ). Our final objective is a margin-rescaled hinge-loss L(P ) = score(q, d- ; w) - score(q, d+ ; w) + 
d+ ,d- P +

where []+ = max(0, ). We use stochastic (sub)gradient descent optimization using the Adadelta (Zeiler, 2012) update rule. Adadelta does not require manual tuning of a global learning rate and requires only two hyperparameters that have shown to be quite robust to changes: the sliding window decay rate  = 0.95 and a constant = 10-6 were set to the default parameters given in the original paper. We furthermore use the distributed learning technique of Iterative Parameter Mixing (McDonald et al., 2010), where multiple models on several shards of the training data are trained in parallel and parameters are averaged after each epoch. We perform incremental optimization using a cyclic order of the data sequence (Bertsekas, 2011), that is, the learner steps through a fixed sequence of pairs, query by query, and relevant document by relevant document, without randomization after epochs. This allows us to cache consecutive query search spaces and feature vectors for relevant documents. Regularization is done by early stopping where the best iteration is found on a held-out development set.

1176

