Unigrams S0 w; S0 p; S0,l w; S0,l p; S0,r w; S0,r p; S0,l2 w; S0,l2 p; S0,r2 w; S0,r2 p; S1 w; S1 p; S1,l w; S1,l p; S1,r w; S1,r p; S1,l2 w; S1,l2 p; S1,r2 w; S1,r2 p; Bigram S0 wS0,l w; S0 wS0,l p; S0 pS0,l w; S0 pS0,l p; S0 wS0,r w; S0 wS0,r p; S0 pS0,r w; S0 pS0,r p; S1 wS1,l w; S1 wS1,l p; S1 pS1,l w; S1 pS1,l p; S1 wS1,r w; S1 wS1,r p; S1 pS1,r w; S1 pS1,r p; S0 wS1 w; S0 wS1 p; S0 pS1 w; S0 pS1 p Trigram S0 wS0 pS0,l w; S0 wS0,l wS0,l p; S0 wS0 pS0,l p; S0 pS0,l wS0,l p; S0 wS0 pS0,r w; S0 wS0,l wS0,r p; S0 wS0 pS0,r p; S0 pS0,r wS0,r p; S1 wS1 pS1,l w; S1 wS1,l wS1,l p; S1 wS1 pS1,l p; S1 pS1,l wS1,l p; S1 wS1 pS1,r w; S1 wS1,l wS1,r p; S1 wS1 pS1,r p; S1 pS1,r wS1,r p; Linearizion w0 ; p0 ; w-1 w0 ; p-1 p0 ; w-2 w-1 w0 ; p-2 p-1 p0 ; S0,l wS0,l2 w; S0,l pS0,l2 p; S0,r2 wS0,r w; S0,r2 pS0,r p; S1,l wS1,l2 w; S1,l pS1,l2 p; S1,r2 wS1,r w; S1,r2 pS1,r p;

NP . VBD . Dr. Talcott . .2 1 led

NP . . NP . .IN a team . 3 of.4 Harvard University . 5

.. .6 .

Figure 4: Example partial tree. Words in the same sub dependency trees are grouped by rounded boxes. Word indices do not specify their orders. Base phrases (e.g. Dr. Talcott) are treated as single words. in learning a dependency language model. Zhang and Clark (2011b) take supertags as constraints to a CCG linearizer. Zhang (2013) demonstrates the possibility of partial-tree linearization, which allows a whole spectrum of input syntactic constraints. In practice, input syntactic constraints, including POS and dependency relations, can be obtained from earlier stage of a generation pipeline, such as lexical transfer results in machine translation. It is relatively straightforward to apply input constraints to a best-first system (Zhang, 2013), but less so for beam-search. In this section, we utilize the input syntactic constraints by letting the information decide the possible actions for each state, namely the return value of G ET P OSSIBLE ACTIONS in Algorithm 1, thus, when input POS-tags and dependencies are given, the generation system can achieve more specified outputs. 3.2.1 POS Constraints POS is the simplest form of constraints to the transition-based linearization system. When the POS of an input word is given, the POS-tag component in S HIFT-Word-POS operation is fixed, and the number of S HIFT actions for the word is reduced from the number of all POS to 1. 3.2.2 Partial Tree Constraints In partial tree linearization, a set of dependency arcs that form a partial dependency tree is given to the linearization system as input constraints. Figure 4 illustrate an example. The search space can be reduced by ignoring the transition sequences that do not result in a dependency tree that is consistent with the input constraints. Take the partial tree in Figure 4 for example. At the state s = ([Harvard University5 ], set(1..n)-{5}, ), it is illegal to shift the base phrase a team3 onto the stack, be-

Table 2: Feature templates. resent a POS-tag. The feature templates can be classified into four types: unigram, bigram, trigram and linearization. The first three types are taken from the dependency parser of Zhang and Nivre (2011), which capture context information for S0 , S1 and their modifiers. The original feature templates of Zhang and Nivre (2011) also contain information of the front words on the buffer. However, since the buffer is unordered for linearization, we do not include these features. The linearization feature templates are specific for linearization, and captures surface ngram information. Each search state represents a partially linearized sentence. We represents the last word in the partially linearized sentence as w0 and the second last as w-1 . Given a set of labeled training examples, the averaged perceptron (Collins, 2002) with early update (Collins and Roark, 2004; Zhang and Nivre, 2011)  of the model. is used to train the parameters  3.2 Input Syntactic Constraints The use of syntactic constraints to achieve better linearization performance has been studied in previous work. Wan et al. (2009) employ POS constraints

116

