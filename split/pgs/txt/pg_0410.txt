trained on a large meme corpus that is almost 90 times larger. Our contributions are three-fold: · We are the first to study the "language of memes" combining NLP, CV, and machine learning techniques, and show that combining the visual and textual signals helps identifying popular meme descriptions; · Our approach empowers Internet users to select better wordings and generate new memes automatically; · Our proposed robust nonparanormal model outperforms competitive baselines for predicting and generating popular meme descriptions. In the next section, we outline related work. In Section 3, we introduce the theory of copula, and our nonparanormal approach. In Section 4, we describe the datasets. We show the prediction and generation results in Section 5 and Section 6. Finally, we conclude in Section 7.

2

Related Work

Although the language of Internet memes is a relatively new research topic, our work is broadly related to studies on predicting popular social media messages (Hong et al., 2011; Bakshy et al., 2011; Artzi et al., 2012). Most recently, Tan et al. (2014) study the effect on wordings for Tweets. However, none of the above studies have investigated multimodal approaches that combine text and vision. Recently, there has been growing interests in inter-disciplinary research on generating image descriptions. Gupta el al. (2009) have studied the problem of constructing plots from video understanding. The work by Farhadi et al. (2010) is among the first to generate sentences from images. Kulkarni et al. (2011) use linguistic constraints and a conditional random field model for the task, whereas Mitchell et al. (2012) leverage syntactic information and co-occurrence statistics and Dodge et al. (2012) use a large text corpus and CV algorithms for detecting visual text. With the surge of interests in deep learning techniques in NLP (Socher et al., 2013; Devlin et al., 2014) and CV (Krizhevsky et al., 2012; Oquab et al., 2013), there have been several unrefereed manuscripts on parsing images and generating text descriptions lately (Vinyals et al., 2014; Chen 356

and Zitnick, 2014; Donahue et al., 2014; Fang et al., 2014; Karpathy and Fei-Fei, 2014) using neural network models. Although the above studies have shown interesting results, our task is arguably more complex than generating text descriptions: in addition to the visual and textual signals, we have to model the popular votes as a third dimension for learning. For example, we cannot simply train a convolutional neural network image parser on billions of images, and use recurrent neural networks to generate texts such as "There is a white cat sitting next to a laptop." for Figure 1. Additionally, since not all images are suitable as meme images, collecting training images is also more challenging in our task. In contrast to prior work, we take a very different approach: we investigate copula methods (Schweizer and Sklar, 1983; Nelsen, 1999), in particular, the nonparanormals (Liu et al., 2009), for joint modeling of raw images, text descriptions, and popular votes. Copula is a statistical framework for analyzing random variables from Statistics (Liu et al., 2012), and often used in Economics (Chen and Fan, 2006). Only until very recently, researchers from the machine learning and information retrieval communities (Ghahramani et al., 2012; Han et al., 2012; Eickhoff et al., 2013). start to understand the theory and the predictive power of copula models. Wang and Hua (2014) are the first to introduce semiparametric Gaussian copula (a.k.a. nonparanormals) for text prediction. However, their approach may be prone to overfitting. In this work, we generalize Wang and Hua's method to jointly model text and vision features with popular votes, while scaling up the model using effective dropout regularization.

3

Our Approach

A key challenge for joint modeling of text and vision is that, because textual features are often relatively sparse and discrete, while visual features are typically dense and continuous, it is difficult to model them jointly in a principled way. To avoid comparing "apple and oranges" in the same probabilistic space, we propose the nonparanormal approach, which extends the Gaussian graphical model by transforming its variables by smooth functions. More specifically, for each dimension of textual and visual features, instead of

