heuristic to set [Wj ]ii = (Kww  v ), essentially removing all words that did not appear in v views before doing GCCA. Table 7 shows that changes in v are largely inconsequential for performance. Test Set MEN RW SCWS AN-SYN AN-SEM 16 70.4 39.9 67.0 56.0 34.6 17 70.4 38.8 66.8 55.8 34.3 21 70.2 39.7 66.5 55.9 34.0 25 70.1 37.2 66.4 56.4 34.3 29 70.0 33.5 65.7 56.0 34.3

Table 7: Performance versus minimum view support threshold v , The other hyperparameters were nj = 1 Count 4 , m = 300, t = 100K . Though a clear best setting did not emerge, we chose v = 25 as the middle ground.

rj : The regularization parameter ensures that all the inverses exist at all points in our method. We found that the performance of our procedure was invariant to r over a large range from 1 to 1e-10. This was because even the 1000th singular value of our data was much higher than 1. Contribution of different sources of data Table 8 shows an ablative analysis of performance where we remove individual views or some combination of them and measure the performance. It is clear by comparing the last column to the second column that adding in more views improves performance. Also we can see that the Dependency based views and the Bitext based views give a larger boost than the morphology and FrameNet based views, probably because the latter are so sparse. Comparison to other word representation creation methods There are a large number of methods of creating representations both multilingual and monolingual. There are many new methods such as by Yu and Dredze (2014), Faruqui et al. (2014), Hill and Korhonen (2014), and Weston et al. (2014) that are performing multiview learning and could be considered here as baselines: however it is not straightforward to use those systems to handle the variety of data that we are using. Therefore, we directly compare our method to the Glove and the SkipGram model of Word2Vec as the performance of those systems is considered state of the art. We trained these two systems on the English portion of the Polyglot 562

Wikipedia dataset.5 We also combined their outputs using MVLSA to create MV-G-WSG) embeddings. We trained our best MVLSA system with data from all views and by using the individual best settings of the hyper-parameters. Specifically the configuration we used was as follows: nj = 1 Count 4 , t = 12.5K, m = 500, k = 300, v = 16. To make a fair comparison we also provide results where we used only the views derived from the Polyglot Wikipedia corpus. See column MVLSA (All Views) and MVLSA (Wiki) respectively. It is clearly visible that MVLSA on the monolingual data itself is competitive with Glove but worse than Word2Vec on the word similarity datasets and it is substantially worse than both the systems on the AN-SYN and AN-SEM datasets. However with the addition of multiple views MVLSA makes substantial gains, shown in column MV Gain, and after consuming the Glove and WSG embeddings it again improves performance by some margins, as shown in column GWSG Gain, and outperforms the original systems. Using GCCA itself for system combination provides closure for the MVLSA algorithm since multiple distinct approaches can now be simply fused using this method. Finally we contrast the Spearman correlations rs with Glove and Word2Vec before and after including them in the GCCA procedure. The values demonstrate that including Glove and WSG during GCCA actually increased the correlation between them and the learnt embeddings, which supports our motivation for performing GCCA in the first place.

6

Previous Work

Vector space representations of words have been created using diverse frameworks including Spectral methods (Dhillon et al., 2011; Dhillon et al., 2012), 6 Neural Networks (Mikolov et al., 2013b; Collobert and Lebret, 2013), and Random Projections (Ravichandran et al., 2005; Bhagat and RavichanWe explicitly provided the vocabulary file to Glove and Word2Vec and set the truncation threshold for Word2Vec to 10. Glove was trained for 25 iterations. Glove was provided a window of 15 previous words and Word2Vec used a symmetric window of 10 words. 6 cis.upenn.edu/~ungar/eigenwords
5

