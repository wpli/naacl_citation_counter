tially obvious fashion in opinion articles. In ostensibly "straight" news, though, framing may be harder to identify. Thus, this context would benefit more from a classifier that could automatically draw attention to frame-invoking language. For training data, we wanted political texts where lay readers had indicated the words and phrases they perceived as most related to framing. Lay annonators were used instead of experts because the classifier's purpose is to support frame reflection among the lay public. Thus, the words and phrases the classifier highlights should align with that population's perception of framing. To our knowledge, no such data set exists. So, we used Mechanical Turk (Snow et al., 2008) and university students to build an annotated dataset that could be used for training and testing. 4.1 Collection We began by collecting political news articles from top 15 online sources of news, as determined by Alexa rankings (http://www.alexa.com/topsites/category/Top/News). We excluded sources outside the US (e.g., BBC), news aggregators (e.g., Yahoo News, Google News), blogs (Huffington Post), and sites without a dedicated politics feed (e.g., USA Today, weather.com). Doing so left eight sources CNN, NYT, Fox, NBC, Washington Post, ABC, LA Times, Reuters. For each source, we collected all items on their politics-specific RSS feed on two separate days roughly six months apart to provide content about diverse issues and events: Tuesday November 12, 2013, and Thursday May 15, 2014. We manually removed duplicate posts, "round-up" style posts that simply summarized and linked to other stories, video-only posts, and other non-textual content, resulting in a total of 205 documents. Of these, we randomly selected 75 to be annotated. 4.2 Annotation Each article was annotated by five to 13 annotators, who were either Mechanical Turk (MTurk) workers or students at one of two major US universities. The task first asked the annotator to read the article, then gave the same directions from the framing prompt described above. To encourage MTurk workers to pay attention to 1475

the task and complete high-quality work, we provided a scheme for bonus payments. Every word a worker annotated that was also annotated by at least two others (i.e., a majority of the 5 workers annotating each document) would earn the worker a $0.02 bonus (two cents). Each word s/he annotated that was annotated by no other work would reduce the bonus by $0.005 (half a cent). Workers were then linked to our web tool where they could complete and submit their annotations. Student annotators received no agreement-based incentive but were granted extra course credit. 4.3 Quality Assurance Crowd workers do not always provide reliable annotations (Snow et al., 2008). For example, we noted multiple instances where annotators had only annotated about a dozen words in an article of several hundred words. These seemed likely to be cases in which the annotator was completing the task as quickly as possible without paying much attention. By comparing the annotations with those collected during our pilot study, in which participants' justifications during the debriefing ensured higher attention and quality, we developed the following criteria for identifying questionable annotations. Those that did not pass at least three of these five requirements were removed from the analysis. 1. All Annotations Short -- While some annotations of short words, such as conjunctions, could be meaningful (see example above), we encountered a number of annotations where every annotated phrase consisted of only one or two words at a time. Thus, we required that the average annotated contiguous segment be at least 3 words long. 2. Few Words Annotated -- When very few words in a document are annotated, we suspect the annotator may have been completing the task as quickly as possible and paying little attention. Thus, we required that each annotator's work include enough annotated words to total more than 5% of the document's length. 3. Large Contiguous Passages without Annotation -- While pilot study participants pointed out portions of a text that consisted of "facts and figures," these were generally relatively short. Thus, we require the longest block of text without any annotations to be no more than one third the length of

