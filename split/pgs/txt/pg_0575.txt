70 types 500 types 2.2M 2.2M Training Data Statistics (0 ) positive example 4.5M 6.2M max #ent for a type 1.1M 1.1M 6732 32 min #ent for a type Test Data Statistics ( - 0 ) positive examples 163K 240K negative examples 17.1M 132M negative/positive ratio 105.22 554.44 Entities Table 1: Statistics of our dataset. 0 is our training snapshot and  is our test snapshot. An example is an entitytype pair.

that most of the facts in the test data are about entities that are not very well-known or famous. The high negative to positive examples ratio in the test data makes this dataset very challenging. 5.2 Automatic Evaluation Results Table 2 shows automatic evaluation results where we give results on 70 types and 500 types. We compare different aspects of the system on 70 types empirically. Adagrad Vs DCD We first study the linear models by comparing Linear.DCD and Linear.AdaGrad. Table 2a shows that Linear.AdaGrad consistently performs better for our task. Impact of Features We compare the effect of different features on the final performance using Linear.AdaGrad in Table 2b. Types are represented by boolean features while Freebase description and Wikipedia full text are represented using tfidf weighting. The best MAP results are obtained by using all the information (T+D+W) while best GAP results are obtained by using the Freebase description and Wikipedia article of the entity. Note that the features are simply concatenated when multiple resources are used. We tried to use idf weighting on type features and on all features, but they did not yield improvements. The Importance of Global Objective Table 2c and 2d compares global training objective with NE and NT training objective. Note that all the three methods use the same number of negative examples. More precisely, for each (e, t)  0 , |NE (e, t)| + |NT (e, t)| = m + n = 2. The results show that the global training objective achieves best scores on both MAP and GAP for classifiers and lowdimensional embedding models. Among NE and NT, NE performs better on the type-based metric while NT performs better on the global metric. Linear Model Vs Embedding Model Finally, we compare the linear classifier model with the embedding model in Table 2e. The linear classifier model performs better than the embedding model in both MAP and GAP. We perform large-scale evaluation on 500 types with the description features (as experiments are expensive) and the results are shown in Table 2f.

we refer to as the Negative Type (NT) objective. The embedding model described in Weston et al. (2011) developed for image retrieval is also a special case of our model trained with the NT objective. While the N E or N T objective functions could be suitable for some classification tasks (Weston et al., 2011), the choice of objective functions for the KBC tasks has not been well motivated. Often the choice is made neither with theoretical foundation nor with empirical support. To the best of our knowledge, the global objective function, which includes both NE (e, t) and NT (e, t), has not been considered previously by KBC methods.

5

Experiments

In this section, we give details about our dataset and discuss our experimental results. Finally, we perform manual evaluation on a small subset of the data. 5.1 Data First, we evaluate our methods on 70 entity types with the most observed facts in the training data.5 We also perform large-scale evaluation by testing the methods on 500 types with the most observed facts in the training data. Table 1 shows statistics of our dataset. The number of positive examples is much larger in the training data compared to that in the test data since the test set contains only facts that were added to the more recent snapshot. An additional effect of this is
We removed few entity types that were trivial to predict in the test data.
5

521

