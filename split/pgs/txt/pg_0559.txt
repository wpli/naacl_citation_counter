Butterflies are self propelled flowers (198) butterfly resting on a flower (26) can you spot the butterfly (88) Evening walk along the beach (9) Chillaxing at the beach (20) Walk by the beach (557)

The sun sets for another day (12) Sun is going to bed (21) After the sun has set (9) In the sky (1013) The sky looks like it is on fire (58) Rippled sky (44)

Figure 1: The image-caption association graph of D´ ej` a Image-Captions. Solid lines represent original captions and

dotted lines represent paraphrase captions. This corpus reflects a rich spectrum of everyday narratives people use in online activities including figurative language (e.g., "Sun is going to bed"), casual language (e.g., Chillaxing at the beach"), and conversational language (e.g., "Can you spot the butterfly"). The numbers in the parenthesis show the cardinality of images associated with each caption. Surprisingly, some of these descriptions are highly expressive, almost creative, and yet not unique -- as all these captions are repeated almost verbatim by different individuals describing different images.

can complement those based on prompted human annotations. The latter in general are literal and mechanical readings of the visual scenes, while the former reflect a rich spectrum of natural language utterances in everyday narratives, including figurative, pragmatic, and conversational language, e.g., "can you spot the butterfly" (Figure 1). Therefore, this dataset offers unique opportunities for grounding figurative and metaphoric expressions using visual context. In conjunction with the new corpus, publicly shared at http://www.cs.stonybrook. edu/~jianchen/deja.html, we also present three new tasks: visually situated paraphrases (§5); creative image captioning (§7), and creative visual paraphrasing (§7). The central algorithm component in addressing all these tasks is a simple and yet effective approach to image caption transfer that exploits the unique association structure of the resulting corpus (§3). Our empirical results collectively demonstrate that when the web data is available at such scale, it is possible to obtain a large-scale, high-quality dataset with significantly less noise. We hope that our approach would be only one of the first attempts, and inspire future research to develop better ways of making use of ever-growing multimodal web data. Although it is unlikely that the automatically gathered datasets can completely replace the curated descriptions written in a controlled setting, our hope is to find ways to complement human annotated datasets in terms of both the scale and also the diversity of the domain and language. 505

The remainder of this paper is organized as follows. First we describe the dataset collection procedure and insights (§2). We then present a new approach to image caption transfer based on the association structure of the corpus (§3) followed by experimental results (§4). After then we present new conceptual tasks: visual paraphrasing (§5), creative image captioning, and creative visual paraphrasing (§7), interleaved with corresponding experimental results (§6, §8).

2 Dataset - Captions in Repetition
Our corpus consists of three components (Table 1): MAIN SET The first step is to crawl as many image-caption pairs as possible. We use flickr.com search API to crawl 760 million pairs in total. The API allows searching images within a given time window, which enables exhaustive search over any time span. To ensure visual correspondence between images and captions, we set query terms using 693 most frequent nouns from the dataset of Ordonez et al. (2011), and systematically slide time windows over the year 2013.1 For each image, we segment its title and the first line of its description into sentences. The crawled dataset at this point includes a lot of noise in the captions. Hence we apply initial filtering rules to reduce the noise. We retain only those image-sentence pairs in which the sentence contains the query noun, and does not contain personal information indicators such as first-person pronouns. We
To ensure enough number of images are associated with each caption, we further search captions with no more than 10 associated images across all years.
1

