# Doc # Train # Test # Mention # Pronoun # Predictions for Pronoun Winograd 1886 1212 674 5658 1886 1348 WinoCoref 1886 1212 674 6404 2595 2118 ACE 375 268 107 23247 3862 13836 OntoNotes 3150 2802 348 175324 58952 37846 Table 5: Statistics of Winograd, WinoCoref, ACE and OntoNotes. We give the total number of mentions and pronouns, while the number of predictions for pronoun is specific for the test data. We added 746 mentions (709 among them are pronouns) to WinoCoref compared to Winograd. Given the polarity values Po(predu ) and Po(predv ), we construct the score vector Spol (u, v ) following Table 4.
Systems Illinois IlliCons KnowFeat KnowCons KnowComb Learning Method BLMP BLMP BLMP+SF BLMP BLMP+SF Inference Method BLL ILP BLL ILP+SC ILP+SC

5

Experiments

In this section, we evaluate our system for both hard coreference problems and general coreference problems, and provide detailed anaylsis on the impact of our proposed Predicate Schemas. Since we treat resolving hard pronouns as part of the general coreference problems, we extend the Winograd dataset with a more complete annotation to get a new dataset. We evaluate our system on both datasets, and show significant improvemnt over the baseline system and over the results reported in Rahman and Ng (2012). Moreover, we show that, at the same time, our system achieves the state-of-art performance on standard coreference datasets. 5.1 Experimental Setup Datasets: Since we aim to solve hard coreference problems, we choose to test our system on the Winograd dataset12 (Rahman and Ng, 2012). It is a challenging pronoun resolution dataset which consists of sentence pairs based on Winograd schemas. The original annotation only specifies one pronoun and two entites in each sentence, and it is considered as a binary decision for each pronoun. As our target is to model and solve them as general coreference problems, we expand the annotation to include all pronouns and their linked entities as mentions (We call this new re-annotated dataset WinoCoref 13 ). Ex.3 in Section 1 is from the Winograd dataset. It originally only specifies he as the pronoun in question, and we added him and his as additional target pronouns. We also use two standard coreference resolution
12

Table 6: Summary of learning and inference methods for all systems. SF stands for schema features while SC represents constraints from schema knowledge. datasets ACE(2004) (NIST, 2004) and OntoNotes5.0 (Pradhan et al., 2011) for evaluation. Statistics of the datasets are provided in Table 5. Baseline Systems: We use the state-of-art Illinois coreference system as our baseline system (Chang et al., 2013). It includes two different versions. One employs Best-Left-Link (BLL) inference method (Ng and Cardie, 2002b), and we name it Illinois14 ; while the other uses ILP with constraints for inference, and we name it IlliCons. Both systems use Best-Link Mention-Pair (BLMP) model for training. On Winograd dataset, we also treat the reported result from Rahman and Ng (2012) as a baseline. Developed Systems: We present three variations of the Predicate Schemas based system developed here. We inject Predicate Schemas knowledge as mentionpair features and retrain the system (KnowFeat). We use the original coreference model and Predicate Schemas knowledge as constraints during inference (KnowCons). We also have a combined system (KnowComb), which uses the schema knowledge to add features for learning as well as constraints for inference. A summary of all systems is provided in Table 6.
14

In implementation, we use the L3 M model proposed in Available at http://www.hlt.utdallas.edu/~vince/data/emnlp12/ Chang et al. (2013), which is slightly different. It can be seen 13 Available at http://cogcomp.cs.illinois.edu/page/data/ as an extension of BLL inference method.

815

