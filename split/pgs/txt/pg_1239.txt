might expect an addition or subtraction of 1 depending on whether or not the metric's scores agreed with those of a human. Instead, however, the following is added: (max(|A > B |, |A < B |) - min(|A > B |, |A < B |)) × d where: |A > B | = # human judgments where A was preferred over B |A < B | = # human judgments where B was preferred over A 1 if |A < B | > |A > B | d = -1 if |A < B | < |A > B | For example, translations of segment 971 for Czechto-English systems uedin-heafield and uedin-wmt13 were compared by human assessors a total of 12 times: the first system was judged to be best 4 times, the second system was judged to be best 2 times, and the two systems were judged to be equal 6 times. This results in a score of 4-2 for a system-level metric that scores the uedin-heafield translation higher than uedin-wmt13 (tied judgments are omitted), or score of 2 - 4 in the converse case. Another challenge is how to deal with relative preference judgments where two translations are deemed equal quality (as opposed to strictly better or worse). In the current setup, tied translation pairs are excluded from the data, meaning that the ability for evaluation metrics to evaluate similar translations is not directly evaluated, and a metric that manages to score two equal quality translations closer, does not receive credit. A segment-level metric that can accurately predict not just disparities between translations but also similarities is likely to have high utility for MT system optimization, and is possibly the strongest motivation for developing segment-level metrics in the first place. In W MT-13, however, 24% of all relative preference judgments were omitted on the basis of ties, broken down as follows: · Spanish-to-English: 28% · French-to-English: 26% · German-to-English: 27% · Czech-to-English: 25% · Russian-to-English: 24% 1185

· · · · ·

English-to-Spanish: 23% English-to-French: 23% English-to-German: 20% English-to-Czech: 16% English-to-Russian: 27%

Although significance tests for evaluation of MT systems and document-level metrics have been identified (Koehn, 2004; Graham and Baldwin, 2014; Graham et al., 2014b), no such test has been proposed for segment-level metrics, and it is unfortunately common to conclude success without taking into account the fact that an increase in correlation can occur simply by chance. In the rare cases where significance tests have been applied, tests or confidence intervals for individual correlations form the basis for drawing conclusions (Aziz et al., 2012; Machacek and Bojar, 2014). However, such tests do not provide insight into whether or not a metric outperforms another, as all that's required for rejection of the null hypothesis with such a test is a likelihood that an individual metric's correlation with human judgment is not equal to zero. In addition, data sets for evaluation in both document and segment-level metrics are not independent and the correlation that exists between pairs of metrics should also be taken into account by significance tests.

3

Segment-Level Human Evaluation

Many human evaluation methodologies attempt to elicit precisely the same quality judgment for individual translations from all assessors, and inevitably produce large numbers of conflicting assessments in the process, including from the same individual human judge (Callison-Burch et al., 2007; CallisonBurch et al., 2008; Callison-Burch et al., 2009). An alternative approach is to take into account the fact that different judges may genuinely disagree, and allow assessments provided by individuals to each contribute to an overall estimate of the quality of a given translation. In an ideal world in which we had access to assessments provided by the entire population of qualified human assessors, for example, the mean of those assessments would provide a statistic that, in theory at least, would provide a meaningful segment-level human score for translations. If it were possible to collect assessments from the entire

