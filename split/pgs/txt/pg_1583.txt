source
los gatos el gato un gato

target

source
los gatos

target

the cats cats the cat a cat

el gato un gato

the cats cats the cat a cat

(a) Global Linear Projection

(b) Local Linear Projection

Figure 2: (a) Illustration of the global linear projection mapping the unlabeled Spanish phrase "un gato" to the target space. The neighbors of the projected point serve as translation candidates and are fetched via a k -NN query. (b) A local linear projection is learned individually for "un gato" based on the translations "the cats", "the cat" of the labeled neighbors "los gatos", "el gato".

fast k -NN query algorithms to retrieve this set (§4). Figure 2 (a) illustrates the method. The translation probability for each translation candidate e  N (¯ e) is based on the similarity to ¯: the projected point e P (e|f ) = ¯ )} exp{sim(e, e . ¯)} e N (¯ e) exp{sim(e , e (1)

3.2

Local Linear Projection

Note that we normalize over the neighbor set N (¯ e) ¯ of foreign phrase f . This of the projected point e ¯ and e which uses the similarity sim(¯ e, e) between e is defined symmetrically as sim(¯ e, e) = 1 , ¯-e 1+ e (2)

The global linear projection uses a single projection matrix for all unlabeled source phrases. This is simple and fast but assumes that we can capture all relations between the source and target representation space with a single Rd×d mapping. We show later that this is clearly not the case (§5.4) and that a single projection struggles particularly with infrequent phrases - the precise situation in which we would like our projection to be robust. We therefore propose to learn many local linear projections which are individually trained for each unlabeled source phrase. Specifically, for each unlabeled source phrase f , we learn a mapping Wf  Rd×d based on the translations of m of f 's labeled neighbors: (f1 , e1 ), (f2 , e2 ), . . . , (fm , em ), fi  N (f ), 1  i  m, m  d (see Figure 2 (b)). Compared to the global projection, we require an additional k -NN query to find the labeled neighbors for each unlabeled source phrase. However, this extra computation takes only a negligible amount of time, since the number of labeled phrases on the source side is significantly smaller than the number of phrases on the target side. Our approach of learning many different mappings is similar to the locality preserving projections method of He and Niyogi (2004), which also construct a locally precise projection in order to map to another space.

¯ - e is the Euclidean distance between where e ¯ and e. vectors e Before adding the generated candidate translations to the MT system, we also calculate the backward maximum likelihood translation probability using Bayes' Theorem: P(f |e) = P(e|f )P(f ) , P(e)

where the marginal probabilities are based on the counts of phrases seen in the monolingual corpora. Similar to Saluja et al. (2014), we use word-based translation probabilities from the baseline system to obtain forward and backward lexicalized translation probabilities. 1529

