sole document model, and measure the cross entropy of multiple query models from it (one for each citing sentence in that paper's citation summary). Citing sentences whose Maximum Likelihood Estimation (MLE) language models are closest to the paper's KPLM are taken as building blocks of the summary. Formally, let S be a citing sentence and let c(W, S ) denote the number of occurrences of word W in S . The MLE language model Mmle of S is the relative frequency of word W in S :
P (W |M mle )=
c(W,S ) |S |

(4)

Subsequently, the score for a citing sentence S is given by its negative cross entropy with the Mkp :
Score (S )= H (Mmle ||Mkp ) P = W 2V P (W |Mmle ) log(P (W |Mkp ))

using citing paper authority and sentence proximity to the citation anchor in the citing paper, it is still largely based on actual word occurrences. In contrast, KPLM directly models words' salience in characterising a paper's main contributions using its keyword profile, with expectedly more discriminative power. Last, Mei and Zhai's estimation of an impact language model for a paper assumes the reliable estimation of its citing papers' authority, which cannot always be guaranteed, for example when a paper receives citations from new papers that themselves have not been cited enough. Furthermore, while a citation network can be unavailable, the estimation of KPLM requires only the citation summaries of papers, which is arguably more robust. 3.3.2 Top Sentence Re-ranking As discussed in Section 3.2, a good summary should capture the most salient keywords of a paper, but also cover as many non-redundant keywords as possible. A summary built using our method is likely to contain citing sentences that concentrate on and repetitively cover salient keywords of the target paper, which may fall short in keywords diversity. Indeed, we can see in the top part of Table 4 that the summary of paper P05-1012 repetitively covers a single keyword, "Minimum Spanning Tree", while it fails to capture other key concepts. To leverage the diversity in keywords captured in a summary, a simple heuristic is to select the next sentence from a pool of top ranked sentences least similar to the existing summary. From an information theoretic point of view, this amounts to choosing the next sentence that carries the most extra information (i.e., statistical surprise), w.r.t. the current contents of the summary. This formulation intuitively suggests that cross entropy, as a natural measure of statistical surprise, could again be employed. We first need to abstract a citing sentence and the citation summary into probabilistic distributions before their cross entropy can be measured. Again we use unigram language modelling. Since both texts are small in size, data sparseness becomes a major issue, as nul dimensions in the MLE language models would make cross entropy not measurable. Smoothing as a way to alleviate data sparseness is thus required. Another issue that also arises from the texts' small size is the non-negligible amount of cross entropy contributed from non-content words in both texts (English stop words plus the two pseudo tokens: "targetanchor" and "otheranchor"). We therefore remove those non-content words prior to

(5)

The larger a citing sentence's score, the closer it is to the cited paper's KPLM, thus the higher the citing sentence would be ranked. To summarise a paper, one can just pick the top k ranked citing sentences where k is the imposed summary length limit. We are not the first to cast the task of summarisation as document retrieval. Mei and Zhai (2008) pioneered in utilising language models and divergence based IR to select sentences to build summaries. While similar in the fundamental methodology, our approach should be distinguished from this work. First, Mei and Zhai cast the task as ad-hoc retrieval, using the "impact language model" of a paper as sole query, while the paper's sentences are treated as documents whose Kullback-Leibler divergence (Kullback and Leibler, 1951) with the query model is measured in turn. Estimating reliable language models for short documents is challenging due to data sparseness and thus requires prudent smoothing. We purposefully reversed the roles of sentence model and document model, using the shorter sentences as queries and measuring their cross entropy with a sole document model (the KPLM)4 . This represents a more natural formulation resulting in simpler language models that require fewer parameter estimations. Second, while the impact language model in (Mei and Zhai, 2008) is partially weighted
4 Kullback-Leibler divergence, used in (Mei and Zhai, 2008), is unsuitable to our task, as it is not formalised as ad-hoc retrieval (i.e., single query, multiple documents). Instead we compare multiple query models (MLE's of citing sentences) to a single document model (KPLM of the cited paper), making KLdivergence scores not comparable due to query specific entropy terms. See (Zhai, 2008) for a detailed analysis.

128

