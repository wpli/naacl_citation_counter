Optimizing Multivariate Performance Measures for Learning Relation Extraction Models
2

Gholamreza Haffari
Faculty of IT, Monash University

1

Ajay Nagesh1,2,3
IITB-Monash Research Academy

Ganesh Ramakrishnan
3

Dept. of CSE, IIT Bombay

gholamreza.haffari@monash.edu ajaynagesh@cse.iitb.ac.in

ganesh@cse.iitb.ac.in

Abstract
We describe a novel max-margin learning approach to optimize non-linear performance measures for distantly-supervised relation extraction models. Our approach can be generally used to learn latent variable models under multivariate non-linear performance measures, such as F -score. Our approach interleaves Concave-Convex Procedure (CCCP) for populating latent variables with dual decomposition to factorize the original hard problem into smaller independent sub-problems. The experimental results demonstrate that our learning algorithm is more effective than the ones commonly used in the literature for distant supervision of information extraction models. On several data conditions, we show that our method outperforms the baseline and results in up to 8.5% improvement in the F1 -score.

to learning graphical models that incorporate interdependencies among the output variables either directly, or indirectly through hidden variables. Large-margin methods have been shown to be a compelling approach to learn rich models detailing the inter-dependencies among the output variables, via optimizing loss functions decomposable over the training instances (Taskar et al., 2003; Tsochantaridis et al., 2004) or non-decompasable loss functions (Ranjbar et al., 2013; Tarlow and Zemel, 2012; Rosenfeld et al., 2014; Keshet, 2014). They have also been shown to be powerful when applied to latent variable models when optimizing for decomposable loss functions (Wang and Mori, 2011; Felzenszwalb et al., 2010; Yu and Joachims, 2009). Our large-margin method learns latent variable models via optimizing non-decomposable loss functions. It interleaves the Concave-Convex Procedure (CCCP) (Yuille and Rangarajan, 2001) for populating latent variables with dual decomposition (Komodakis et al., 2011; Rush and Collins, 2012). The latter factorizes the hard optimization problem (encountered in learning) into smaller independent sub-problems over the training instances. We then present linear programming and local search methods for effective optimization of the sub-problems encountered in the dual decomposition. Our local search algorithm leads to a speed up of 7,000 times compared to the exhaustive search used in the literature (Joachims, 2005; Ranjbar et al., 2012). Our work is the first to make use of max-margin training in distant supervision of relation extraction models. We demonstrate the effectiveness of our proposed method compared to two strong baseline systems which optimize for the error rate and conditional likelihood, including a state-of-the-art system by Hoffmann et al. (2011). On several data conditions, we show that our method outperforms the baseline and results in up to 8.5% improvement in the F1 -score.

1

Introduction

Rich models with latent variables are popular for many problems in natural language processing. In information extraction, for example, one needs to predict the relation labels y that an entity-pair x can have based on the hidden relation mentions h, i.e., the relation labels for occurrences of the entity-pair in a given corpus. However, these models are often trained by optimizing performance measures (such as conditional log-likelihood or error rate) that are not directly related to the task-specific non-linear performance measure, e.g., the F1 -score. However, better models may be trained by optimizing the taskspecific performance measure while allowing latent variables to adapt their values accordingly. We present a large-margin method to learn parameters of latent variable models for a wide range of non-linear multivariate performance measures such as F . Our method can be applied 892

Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 892­900, Denver, Colorado, May 31 ­ June 5, 2015. c 2015 Association for Computational Linguistics

