space of all possible extra edges is (n - 2)2 and revisiting them only adds more actions linearly, the total asymptotic runtime complexity of our algorithm is O(n2 ). In practice, however, the number of applications of the REATTACH action is much less than the worst case scenario due to the similarities between the dependency tree and the AMR graph of a sentence. Also, nodes with reentrancies in AMR only account for a small fraction of all the nodes, thus making the REENTRANCE action occur at constant times. These allow the tree-to-graph parser to parse a sentence in nearly linear time in practice. 3.2 Greedy Parsing Algorithm Algorithm 1 Parsing algorithm Input: sentence w = w0 . . . wn and its dependency tree Dw Output: parsed graph Gp 1: s  s0 (Dw , w ) 2: while s  / St do 3: T  all possible actions according to s 4: bestT  arg maxtT score(t, c) 5: s  apply bestT to s 6: end while 7: return Gp Our parsing algorithm is similar to the parser in (Sartorio et al., 2013). At each parsing state s  S , the algorithm greedily chooses the parsing action t  T that maximizes the score function score(). The score function is a linear model defined over parsing action t and parsing state s. score(t, s) =  · (t, s) (1)

As pointed out in (Bohnet and Nivre, 2012), constraints can be added to limit the number of possible actions to be evaluated at line 3. There could be formal constraints on states such as the constraint that the SWAP action should not be applied twice to the same pair of nodes. We could also apply soft constraints to filter out unlikely concept labels, relation labels and candidate nodes k for REATTACH and REENTRANCE. In our parser, we enforce the constraint that NEXT-NODE-lc can only choose from concept labels that co-occur with the current node's lemma in the training data. We also empirically set the constraint that REATTACHk could only choose k among 0 's grandparents and great grandparents. Additionally, REENTRANCEk could only choose k among its siblings. These constraints greatly reduce the search space, thus speeding up the parser.

4

Learning

4.1 Learning Algorithm As stated in section 3.2, the parameter of our model is weight vector  in the score function. To train the weight vector, we employ the averaged perceptron learning algorithm (Collins, 2002). Algorithm 2 Learning algorithm Input: sentence w = w0 . . . wn , Dw , Gw Output:  1: s  s0 (Dw , w ) 2: while s  / St do 3: T  all possible actions according to s 4: bestT  arg maxtT score(t, s) 5: goldT  oracle(s, Gw ) 6: if bestT = goldT then 7:    - (bestT, s) + (goldT, s) 8: end if 9: s  apply goldT to s 10: end while For each sentence w and its corresponding AMR annotation GAM R in the training corpus, we could get the dependency tree Dw of w with a dependency parser. Then we represent GAM R as span graph Gw , which serves as our learning target. The learning algorithm takes the training instances (w, Dw , Gw ), parses Dw according to Algorithm 1, and get the best action using current weight vector  . The

where  is the weight vector and  is a function that extracts the feature vector representation for one possible state-action pair t, s . First, the algorithm initializes the state s with the sentence w and its dependency tree Dw . At each iteration, it gets all the possible actions for current state s (line 3). Then, it chooses the action with the highest score given by function score() and applies it to s (line 4-5). When the current state reaches a terminal state, the parser stops and returns the parsed graph.

371

