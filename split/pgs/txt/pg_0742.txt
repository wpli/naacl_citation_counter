in the update equation 2 are set heuristically: the sense agnostic weight  is 1.0, and relations-specific weights r are 1.0 for synonyms and 0.5 for hypernyms and hyponyms. EM+RETRO vectors are the exception where we use a weight of  = 0.0 instead, as required by the derivation in section 2.2. For skip-gram vectors (SG) we use the following standard settings, and do not tune any of the values. We filter all words with frequency < 5, and prenormalize the corpus to replace all numeric tokens with a placeholder. We set the dimensionality of the vectors to 80, and the window size to 10 (5 context words to either side of a target). The learning rate is set to an initial value of 0.025 and diminished linearly throughout training. The negative sampling parameter is set to 5. Additionally for the EM variants (section 2.2) we set the Dirichlet concentration parameter  to 1000. We use 5 abstract senses for the EM vectors, and initialize the priors uniformly. For EM+RETRO, WordNet dictates the number of senses; also when available WordNet lemma counts are used to initialize the priors. Finally, we set the retrofitting period k to 50 million words. 3.2 Experimental Results We evaluate our models on 3 kinds of lexical semantic tasks: similarity scoring, synonym selection, and similarity scoring in context. Similarity Scoring: This task involves using a semantic model to assign a score to pairs of words. We use the following 4 standard datasets in this evaluation: WS-353 (Finkelstein et al., 2002), RG65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991) and MEN-3k (Bruni et al., 2014). Each dataset consists of pairs of words along with an averaged similarity score obtained from several human annotators. For example an item in the WS-353 dataset is "book, paper  7.46". We use standard cosine similarity to assign a score to word pairs in single-sense VSMs, and the following average similarity score to multi-sense variants, as proposed by Reisinger and Mooney (2010): avgSim(wi , wi ) = 1 ki kj cos(vij , vi j )
j,j

Synonym Selection: In this task, VSMs are used to select the semantically closest word to a target from a list of candidates. We use the following 3 standard datasets in this evaluation: ESL50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004) and TOEFL-80 (Landauer and Dumais, 1997). These datasets consist of a list of target words that appear with several candidate lexical items. An example from the TOEFL dataset is "rug  sofa, ottoman, carpet, hallway", with "carpet" being the most synonym-like candidate to the target. We begin by scoring all pairs composed of the target and one of the candidates. We use cosine similarity for single-sense VSMs, and max similarity for multisense models7 : maxSim(wi , wi ) = max cos(vij , vi j )
j,j

(9)

(8)

The output of systems is evaluated against the gold standard using Spearman's rank correlation coefficient. 688

These scores are then sorted in descending order, with the top-ranking score yielding the semantically closest candidate to the target. Systems are evaluated on the basis of their accuracy at discriminating the top-ranked candidate. The results for similarity scoring and synonym selection are presented in table 1. On both tasks and on all datasets, with the partial exception of WS-353 and MEN-3k, our vectors (RETRO & EM+RETRO) consistently yield better results than other VSMs. Notably, both our techniques perform better than preprocessing a corpus with WSD information in unsupervised or supervised fashion (SGWSD & SG-IMS). Simple EM without an ontological prior to ground the vectors (SG-EM) also performs poorly. We investigated the observed drop in performance on WS-353 and found that this dataset consists of two parts: a set of similar word pairs (e.g. "tiger" and "cat") and another set of related word pairs (e.g. "weather" and "forecast"). The synonym, hypernym and hyponym relations we use tend to encourage similarity to the detriment of relatedness. We ran an auxiliary experiment to show this. SGEM+RETRO training also learns vectors for context words ­ which can be thought of as a proxy for relatedness. Using this VSM we scored a word pair by the average similarity of all the sense vectors of
Here we are specifically looking for synonyms, so the max makes more sense than taking an average.
7

