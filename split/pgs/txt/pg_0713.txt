Figure 6: Correct-head accuracies over POS-tags (sorted in the descending order by frequency) of iterated reranking (IR) and its initializer (MS) on the training sentences in phase 1 ( 15 words). vised clustering method to tag words. Although their approach can distinguish polysemes (e.g., `cool' in `to cool the selling panic' and in `it is cool'), it is not able to make use of word meaning similarities (e.g., the meaning of `dog' is closer to `animal' than to `table'). Naseem and Barzilay (2011)'s system uses semantic cues from an out-of-domain annotated corpus, thus is not fully unsupervised. We have showed that IR with a generative reranker is an approximation of hard-EM (see Equation 4). Our system is thus related to the works of Spitkovsky et al. (2013) and Tu and Honavar (2012). However, what we have proposed is more than that: IR is a general framework that we can have more than one option for choosing k -best parser and reranker. For instance, we can make use of a generative k -best parser and a discriminative reranker that are used for supervised parsing. Our future work is to explore this. The experimental results reveal that starting point is very important to the iterated reranking with the -order generative model. On the one hand, that is a disadvantage compared to the other systems, which use uninformed or harmonic initialisers. But on the other hand, that is an innovation as our approach is capable of making use of existing systems. The results shown in Figure 4 suggest that if phase 0 uses a better parser which uses less expressive model and/or less external knowledge than our model, such as the one proposed by Spitkovsky et al. (2013), we can expect even a higher performance. The other systems, except Blunsom and Cohn (2010), however, might not benefit from using good existing 659 parsers as initializers because their models are not significantly more expressive than others 8 .

8 Conclusion
We have proposed a new framework, iterated reranking (IR), which trains supervised parsers without the need of manually annotated data by using a unsupervised parser as an initialiser. Our system, employing Marecek and Straka (2013)'s unsupervised parser as the initialiser, the k -best MSTParser, and Le and Zuidema (2014)'s reranker, achieved 1.8% DDA higher than the SOTA parser of Spitkovsky et al. (2013) on the WSJ corpus. Moreover, we also showed that unsupervised parsing benefits from lexical semantics through using word-embeddings. Our future work is to exploit other existing supervised parsers that fit our framework. Besides, taking into account the fast development of the word embedding research (Mikolov et al., 2013; Pennington et al., 2014), we will try different word embeddings.

Acknowledgments
We thank Remko Scha and three anonymous reviewers for helpful comments. Le thanks Milos  Stanojevic ´ for helpful discussion.

In an experiment, we used the Marecek and Straka (2013)'s parser as an initializer for the Gillenwater et al. (2011)'s parser. As we expected, the latter was not able to make use of this.

8

