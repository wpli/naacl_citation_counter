Unlabeled Collection

"can I have a cheap restaurant"
Semantic Knowledge Graph

Slot Induction
Frame-Semantic Parsing
Slot Candidates

Semantic Decoder Training
Slot Ranking Model

SLU Model

Syntactic Dependency Parsing

Lexical Knowledge Graph

Induced Slots

Semantic Representation

Figure 1: The proposed framework

can i have a cheap restaurant
Frame: expensiveness FT LU: cheap Frame: capability Frame: locale_by_use FT LU: can FE Filler: i FT/FE LU: restaurant

and restaurant), which we consider as possible slotfillers. 3.2 Independent Semantic Decoder With outputted semantic parses, we extract the frames with the top 50 highest frequency as our slot candidates for training SLU. The features for training are generated by word confusion network, where confusion network features are shown to be useful in developing more robust systems for SLU (HakkaniT¨ ur et al., 2006; Henderson et al., 2012). We build a vector representation of an utterance as u = [x1 , ..., xj , ...]. xj = E[Cu (n-gramj )]1/|n-gramj | , (1)

Figure 2: An example of probabilistic frame-semantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit.

more, 1976), which holds that the meaning of most words can be expressed on the basis of semantic frames, which encompass three major components: frame (F), frame elements (FE), and lexical units (LU). For example, the frame "food" contains words referring to items of food. A descriptor frame element within the food frame indicates the characteristic of the food. For example, the phrase "low fat milk" should be analyzed with "milk" evoking the food frame and "low fat" filling the descriptor FE of that frame. In our approach, we parse all ASR-decoded utterances in our corpus using SEMAFOR5 , a stateof-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates, where the LUs that correspond to the frames are extracted for slot filling. For example, Figure 2 shows an example of an ASR-decoded text output parsed by SEMAFOR. SEMAFOR generates three frames (capability, expensiveness, and locale by use) for the utterance, which we consider as slot candidates for training the SLU model. Note that for each slot candidate, SEMAFOR also includes the corresponding lexical unit (can i, cheap,
5

where Cu (n-gramj ) counts how many times ngramj occurs in the utterance u, E(Cu (n-gramj )) is the expected frequency of n-gramj in u, and |n-gramj | is the number of words in n-gramj . For each slot candidate si , we generate a pseudo training data Di to train a binary classifier Mi for predicting the existence of si given an utterance, i ) | u  R+ , li  {-1, +1}}K , Di = {(uk , lk k k k=1 i where lk = +1 when the utterance uk contains the i = -1 othslot candidate si in its semantic parse, lk erwise, and K is the number of utterances. 3.3 Adaptation Process and SLU Model Since SEMAFOR was trained on FrameNet annotation, which has a more generic frame-semantic context, not all the frames from the parsing results can be used as the actual slots in the domain-specific dialogue systems. For instance, in Figure 2, we see that the frames "expensiveness" and "locale by use" are essentially the key slots for the purpose of understanding in the restaurant query domain, whereas

http://www.ark.cs.cmu.edu/SEMAFOR/

621

