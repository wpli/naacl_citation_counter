Lexicon Glove +PPDB +WNsyn +WNall +FN SG +PPDB +WNsyn +WNall +FN GC +PPDB +WNsyn +WNall +FN Multi +PPDB +WNsyn +WNall +FN

MEN-3k 73.7 1.4 0.0 2.2 ­3.6 67.8 5.4 0.7 2.5 ­3.2 31.3 7.0 3.6 6.7 1.8 75.8 3.8 1.2 2.9 1.8

RG-65 76.7 2.9 2.7 7.5 ­1.0 72.8 3.5 3.9 5.0 2.6 62.8 6.1 6.4 10.2 4.0 75.5 4.0 0.2 8.5 4.0

WS-353 60.5 ­1.2 0.5 0.7 ­5.3 65.6 4.4 0.0 1.9 ­4.9 62.3 2.0 0.6 2.3 0.0 68.1 6.0 2.2 4.3 0.0

TOEFL 89.7 5.1 5.1 2.6 2.6 85.3 10.7 9.3 9.3 1.3 60.8 13.1 7.3 4.4 4.4 84.0 12.0 6.6 6.6 4.4

SYN-REL 67.0 ­0.4 ­12.4 ­8.4 ­7.0 73.9 ­2.3 ­13.6 ­10.7 ­7.3 10.9 5.3 ­1.7 ­0.6 ­0.6 45.5 4.3 ­12.3 ­10.6 ­0.6

SA 79.6 1.6 0.7 0.5 0.0 81.2 0.9 0.7 ­0.3 0.5 67.8 1.1 0.0 0.2 0.2 81.0 0.6 1.4 1.4 0.2

Table 2: Absolute performance changes with retrofitting. Spearman's correlation (3 left columns) and accuracy (3 right columns) on different tasks. Higher scores are always better. Bold indicates greatest improvement for a vector type. Method LBL (Baseline) LBL + Lazy LBL + Periodic LBL + Retrofitting k,  k = ,  = 0 =1  = 0.1  = 0.01 k = 100M k = 50M k = 25M ­ MEN-3k 58.0 ­0.4 0.7 0.7 3.8 3.4 0.5 5.7 RG-65 42.7 4.2 8.1 9.5 18.4 19.5 18.1 15.6 WS-353 53.6 0.6 0.4 1.7 3.6 4.4 2.7 5.5 TOEFL 66.7 ­0.1 ­1.4 2.6 12.0 18.6 21.3 18.6 SYN-REL 31.5 0.6 0.7 1.9 4.8 0.6 ­3.7 14.7 SA 72.5 1.2 0.8 0.4 1.3 1.9 0.8 0.9

Table 3: Absolute performance changes for including PPDB information while training LBL vectors. Spearman's correlation (3 left columns) and accuracy (3 right columns) on different tasks. Bold indicates greatest improvement.

We train vectors of length 100 on the WMT-2011 news corpus, which contains 360 million words, and use PPDB as the semantic lexicon as it performed reasonably well in the retrofitting experiments (§6.1). For the lazy method we update with respect to the prior every k = 100,000 words7 and test for different values of prior strength   {1, 0.1, 0.01}. For the periodic method, we update the word vectors using Eq. 1 every k  {25, 50, 100} million words.
7

Results. See Table 3. For lazy,  = 0.01 performs best, but the method is in most cases not highly sensitive to  's value. For periodic, which overall leads to greater improvements over the baseline than lazy, k = 50M performs best, although all other values of k also outperform the the baseline. Retrofitting, which can be applied to any word vectors, regardless of how they are trained, is competitive and sometimes better.

k = 10,000 or 50,000 yielded similar results.

1611

