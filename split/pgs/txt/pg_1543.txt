vestigate the impact of the phonetic features on the detection of various forms of persuasiveness. We compare three different sets of features, namely phonetic, n-grams and their combination to understand whether phonetic information can improve the performance of standard lexical approaches. Similarly to Danescu-Niculescu-Mizil et al. (2012) and Tan et al. (2014), we formulate a pairwise classification problem such that given a pair (s1 , s2 ) consisting of sentences s1 and s2 , the goal is to determine the more persuasive one (i.e., the one on the left or right). We can consider this as a binary classification task where for each instance (i.e., pair) the possible labels are left or right. 6.1 Dataset and preprocessing

filtering for bigrams and trigrams to capture longerrange usage patterns such as propositional phrases. The third feature set is simply the union of both phonetic and n-gram features. To find the best configuration for each dataset and feature set, we conducted a grid search over the degree of the polynomial kernel (1 or 2) and the number of features to be used (in the range between 1,000 and 20,000). Due to the low dimensionality of the phonetic feature set, feature selection was performed only for the feature sets including n-grams. The selection was performed based on the information gain of each feature.
Dataset CORPS Movie Slogan Twitter Phonetic 0.589 (-, 1) 0.600 (-, 2) 0.700 (-, 2) 0.563 (-, 2) N-Gram (4k, 1) 0.694 (1k, 1) 0.826 (3k, 1) 0.732 (5k, 1) 0.733 All 0.736 (2k, 1) 0.722 (1k, 1) 0.883 (5k, 1) 0.745 (4k, 1)

For the prediction experiments, we used the four datasets described in Section 4 (i.e., CORPS, Twitter, Slogan and Movie), all of which consist of a persuasive sentence P and its non-persuasive counterpart (¬P ) labeled as either left or right. To make the positions of the sentences in a pair irrelevant (i.e. to provide symmetry), for each instance occurring in the original datasets (e.g., (s1 , s2 ) with label left), we added another instance including the same sentence pair in reverse order (i.e., (s2 , s1 ) with label right). As a preprocessing step, all the sentences were tokenized by using Stanford CoreNLP (Manning et al., 2014). 6.2 Classifier and features

Table 5: Results of the within-dataset experiments.

6.3

Within-dataset experiments

We performed a 10-fold cross-validation on each dataset and experimented with three feature sets by using a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995). We preferred SVM as a classifier due to its characteristic property to especially perform well on high-dimensional data (Weichselbraun et al., 2011). The first feature set consists of the phonetic features (i.e. plosive, alliteration, rhyme and homogeneity scores as detailed in Section 3). The second feature set is a standard bag of word n-grams including unigrams, bigrams and trigrams. All the nonascii characters, punctuations and numbers were ignored. The URLs and mentions in Twitter data were replaced with tags (i.e. URL and MENTION respectively). In addition, for the unigram features, stop words were filtered out. We did not apply this 1489

For this set of experiments, we conducted a 10-fold cross validation on each dataset separately. In Table 5, for each dataset listed in the first column, in the subsequent columns we report the performance of the best model obtained with 10-fold cross validation using i) only phonetic features (Phonetic), ii) only n-grams (N-Gram), iii) both phonetic and ngram features (All). As mentioned previously, for each pair (s1 , s2 ) consisting of sentences s1 and s2 , our dataset contains another pair including the same sentences in reverse order (i.e., (s2 , s1 )), resulting in a symmetric and balanced dataset. Therefore, classification performance is measured in terms of accuracy (i.e., the percentage of pairs of which labels were correctly predicted). For each accuracy value, we also report in parenthesis the number of features selected and the kernel degree of the corresponding model. While the kernel degree did not make a big difference in the performance, the number of selected features had an important effect on the accuracy of the models. As can be observed from these values, the best performance on all the datasets is achieved with a relatively small number of features. Among the values reported in the table, the ones followed by  are significantly different (p < .001)

