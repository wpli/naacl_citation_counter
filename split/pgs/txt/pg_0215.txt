evant concrete scenes and situations. The together example illustrates how visual data might ground abstract notions in surprising ways. For all these cases, we can borrow what Howell et al. (2005) say about visual propagation to abstract words (p. 260):
Intuitively, this is something like trying to explain an abstract concept like love to a child by using concrete examples of scenes or situations that are associated with love. The abstract concept is never fully grounded in external reality, but it does inherit some meaning from the more concrete concepts to which it is related.

Model  W ORD FREQUENCY 0.22 K IELA ET AL . -0.65 S KIP - GRAM 0.05 MMS KIP - GRAM -B 0.04 MMS KIP - GRAM -A -0.75 MMS KIP - GRAM -B* -0.71

(a)

(b)

Figure 3: (a) Distribution of MMS KIP - GRAM -A vector activation for meat (blue) and hope (red). (b) Spearman  between concreteness and various measures on the Kiela et al. (2014) set. MMS KIP - GRAM -A representations and those generated by mapping MMS KIP - GRAM -B vectors onto visual space (MMS KIP - GRAM -B*) achieve very high correlation (but, interestingly, not MMS KIP GRAM -B). This is further evidence that multimodal learning is grounding the representations of both concrete and abstract words in meaningful ways.

Of course, not all examples are good: the last column of Figure 2 shows cases with no obvious relation between words and visual neighbours (subjects preferred the random images by a large margin). The multimodal vectors we induce also display an interesting intrinsic property related to the hypothesis that grounded representations of abstract words are more complex than for concrete ones, since abstract concepts relate to varied and composite situations (Barsalou and Wiemer-Hastings, 2005). A natural corollary of this idea is that visually-grounded representations of abstract concepts should be more diverse: If you think of dogs, very similar images of specific dogs will come to mind. You can also imagine the abstract notion of freedom, but the nature of the related imagery will be much more varied. Recently, Kiela et al. (2014) have proposed to measure abstractness by exploiting this very same intuition. However, they rely on manual annotation of pictures via Google Images and define an ad-hoc measure of image dispersion. We conjecture that the representations naturally induced by our models display a similar property. In particular, the entropy of our multimodal vectors, being an expression of how varied the information they encode is, should correlate with the degree of abstractness of the corresponding words. As Figure 3(a) shows, there is indeed a difference in entropy between the most concrete (meat) and most abstract (hope) words in the Kiela et al. set. To test the hypothesis quantitatively, we measure the correlation of entropy and concreteness on the 200 words in the Kiela et al. (2014) set.7 Figure 3(b) shows that the entropies of both the
7 Since the vector dimensions range over the real number line, we calculate entropy on vectors that are unit-normed after adding a small constant insuring all values are positive.

6

Conclusion

We introduced two multimodal extensions of S KIP GRAM . MMS KIP - GRAM -A is trained by directly optimizing the similarity of words with their visual representations, thus forcing maximum interaction between the two modalities. MMS KIP - GRAM -B includes an extra mediating layer, acting as a crossmodal mapping component. The ability of the models to integrate and propagate visual information resulted in word representations that performed well in both semantic and vision tasks, and could be used as input in systems benefiting from prior visual knowledge (e.g., caption generation). Our results with abstract words suggest the models might also help in tasks such as metaphor detection, or even retrieving/generating pictures of abstract concepts. Their incremental nature makes them well-suited for cognitive simulations of grounded language acquisition, an avenue of research we plan to explore further.

Acknowledgments
We thank Adam Liska, Tomas Mikolov, the reviewers and the NIPS 2014 Learning Semantics audience. We were supported by ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES).

161

