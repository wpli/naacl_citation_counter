Total dur. (min) # running words # utterances # speakers Avg. utt. dur. (s) WER

News 150 26,282 737 178 12.2 17.7

Legal 338 53,846 2,922 95 6.9 20.4

Weather 108 23,722 1,290 36 5.0 11.9

TED 340 41,545 2,245 28 9.1 22.9

Table 1: Some characteristics of the four domains.

News. We use the HUB45 corpus, which contains 104 hours of broadcasts from different television and radio networks. We selected the 1999 test set of the DARPA Hub-4 evaluation, consisting of two recordings acquired in TV studios and containing speech of professional speakers reading news. Legal. This audio database6 contains recordings of European Parliament members speaking in plenary sessions, as well as recordings of interpreters (non-native speakers). Speech is hence quite spontaneous, and a relevant level of reverberation is present due to the usage of table-mounted microphones. The data that we used for our experiments are both the English EPPS development (dev06) and evaluation (eval07) sets of the 2007 TC-STAR ASR evaluation campaign (Hamon et al., 2007). Weather. This dataset is formed by recordings of weather reports broadcasted by the BBC English TV channel, and contains both national and local weather forecasts. There are roughly 50 native speakers and the speech is delivered very quickly. Although the speakers are native and the recordings are performed in a controlled environment, there are some hesitations, grammar errors or lengthy formulations in the recordings which are corrected in the captions (which can thus be considered as loose reference transcripts (Mohr et al., 2013)). TED. This dataset contains audio recordings of English speakers (28 different talks) and was used within the IWSLT 2013 evaluation campaign (Cettolo et al., 2013). This domain presents large variability of topics (hence a large, unconstrained vocabulary), presence of non-native speakers, and a rather
5 distributed by the Linguistic Data Consortium and available at https://catalog.ldc.upenn.edu/docs/ LDC2000S88/ 6 http://catalog.elra.info/product_info. php?products_id=1032

informal speaking style. Given their diverse nature, the four domains present a big challenge both for ASR and QE systems. From Table 1 it is possible to grasp several differences among them. One aspect that reflects such differences is the WER of the ASR system we used to transcribe the utterances (described in Section 4.2). The lowest WER is for Weather, a domain in which the speech is planned. This is also the domain with the shortest average utterance duration (5 sec.), the lowest number of speakers (36) and the lowest number of running words (23,722). The higher WER achieved on the other domains is due to the more challenging conditions posed by each of them. TED and News include speeches about unconstrained topics, and their average utterance durations tend to be longer than for the other two domains. News is the shortest domain in duration and the smallest in number of utterances (150 min. for 737 utterances), but has the highest number of speakers. This means that there are very few utterances for each speaker, in average, and that both the ASR and the QE system must cope with the differences in speech for all these subjects. Legal presents the second largest number of speakers, both native and non-native, using a specific terminology on a varied number of topics. 4.2 ASR System The ASR engine used in our experiments makes use of Hidden Markov Models (HMMs) of triphone units and of 4-gram back-off language models (LMs). HMMs are trained on domain-specific sets of audio data. The HUB4 training corpus is released with "verbatim" transcriptions of the audio signals while, for the other three domains (i.e. Legal, Weather and TED), training data have only associated captions, which are not always exact transcriptions of the corresponding audio recordings. To extract audio segments with reliable transcriptions we hence applied a lightly supervised training procedure (Lamel et al., 2001). This resulted in 67 hours of recordings for the Weather domain, 144 hours for TED, 164 hours for News and 100 hours for Legal. For LM training, first, a general purpose LM is trained on the Gigaword text corpus (5th ed.) (Parker et al., 2011) then, it is adapted to all domains, using domain specific text data. Each auto-

718

