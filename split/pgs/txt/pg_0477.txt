 Contraction Replacements. Unrolling standard contractions (don't), common informal cases (wanna), and non-standard variations produced via apostrophe omission (dont).  Slang Replacements. Replacing slang terms, such as slang shortenings and word elongation.  Capitalization Replacements. Correcting the capitalization of words. The replaced word differs from its replacement by only capitalization.  Other Replacements. Correcting unintentional typographic mistakes, such as misspelling and word concatenation. When segmenting word additions, we note that words that need to be added in a normalization edit were often consciously dropped by the user in the original text. Our categorization reflects this by examining syntactic categories that are often dropped in informal writing:  Subject Addition. Adding in omitted subjects.  Determiner Addition. Adding in omitted determiners (e.g., "[The] front row is so close").  Be-verb Addition. Adding in omitted forms of the verb to be.  Other Addition. All word additions not covered by the other categories. Finally, word removals are subdivided into just two categories:  Dataset-specific Removals. Removing tokens that do not appear outside of the dataset in question (e.g., for Twitter: hashtags, @replies, and retweets).  Other Removals. Removing interjections, laughter words, and other expression of emotion (e.g., ugh). Note that we are not suggesting here that datasetspecific words should be removed in all cases. While in many cases they may be removed if they do not have a formal equivalent, they may also be replaced or retained as is, depending on the context. 423

3.3

Dataset

To facilitate our experiments, we collected and annotated a dataset of Twitter posts (tweets) from the TREC Twitter Corpus1 . The TREC Twitter corpus is a collection of 16 million tweets posted in January and February of 2011. The corpus is designed to be a representative sample of Twitter usage, and as such includes both regular and spam tweets. To build our dataset, we sampled 600 posts at random from the corpus. The tweets were then manually filtered such that tweets that were not in English were replaced with those in English. To produce our gold standard, two oDesk2 contractors were asked to manually normalize each tweet in the dataset to its fully grammatical form, as would be found in formal text. Annotation guidelines stipulated that twitter-specific tokens should be retained if important to understanding the sentence, but modified or removed otherwise. As noted, most previous work often stopped short of requiring full grammaticality. However, Zhang et al. (2013) argued that grammaticality should be the ideal end goal of normalization since the models used in downstream applications are typically trained on well-formed sentences. We adopt this methodology here both because we agree with this assertion and because a fully grammatical form is appropriate for all of the downstream applications of interest, allowing for a single unified gold standard that can aid comparison across applications. During gold standard creation, each normalization edit was labeled with its type, according to the above taxonomy. The distribution of normalization edits in the dataset is given in Table 1. As shown, normalization edits accounted for about 29% of all tokens. Token replacements accounted for just over half of all edits (53%), while token addition (29%) was more common than token removal (18%). One interesting observation is non-capitalization word replacement accounted for only 25% of all normalization edits, intuitively indicating potential drawbacks for the common definition of normalization as one of simple word replacement which ignores capitalization and punctuation.
1 2

http://trec.nist.gov/data/tweets/ https://www.odesk.com/

