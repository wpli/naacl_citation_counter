naive word-alignment based projection would map every word from a phrase extracted in English to the source sentence. This algorithm has two drawbacks: first, since the word alignments are many-to-many, each English word can be possibly mapped to more than one source word which leads to ambiguity in its projection; second, a word level mapping can produce non-contiguous phrases in the source sentence, which are hard to interpret semantically. To tackle these problems, we introduce a novel algorithm that incorporates a BLEU score (Papineni et al., 2002) based phrase similarity metric to perform cross-lingual projection of relations. Given a source sentence, its translation, and the wordto-word alignment, we first extract phrase-pairs P using the phrase-extract algorithm (Och and Ney, 2004). In each extracted phrase pair (phrs , phrt )  P , phrs and phrt are contiguous word sequences in s and t respectively. We next determine the translations of arg1, rel and arg2 from the extracted phrase-pairs. For each English phrase p  {arg1, rel, arg2}, we first obtain the phrase-pair (phrs , phrt )  P such that phrt has the highest BLEU score relative to p subject to the condition that p  phrt =  i.e, there is at least one word overlap between the two phrases. This condition is necessary since we use BLEU score with smoothing and may obtain a nonzero BLEU score even with zero word overlap. If there are multiple phrase-pairs in P that correspond to the same target phrase phrt , we select the shortest source phrase (phrs ). However, if there is no word overlap between the target phrase p and any of the target phrases in P , we project the phrase using the word-alignment based projection. The cross-lingual projection method is presented in Algorithm 1.

ple, in the sentence: "Michelle Obama, wife of Barack Obama was born in Chicago", the following are possible annotations: a) (Michelle Obama; born in; Chicago): 1, b) (Barack Obama; born in; Chicago): 0. Such binary annotations are not available for languages apart from English. Furthermore, a binary 1/0 label is a coarse annotation that could unfairly penalize an extracted relation which has the correct semantics but is slightly ungrammatical. This could occur either when prepositions are dropped from the relation phrase or when there is an ambiguity in the boundary of the relation phrase. Therefore to evaluate our multilingual relation extraction framework, we obtained annotations from professional linguists for three typologically different languages: French, Hindi, and Russian. The annotation task is as follows: Given a sentence and a pair of arguments (extracted automatically from the sentence), the annotator identifies the most relevant contiguous relation phrase from the sentence that establishes a plausible connection between the two arguments. If there is no meaningful contiguous relation phrase between the two arguments, the arguments are considered invalid and hence, the extracted relation tuple from the sentence is considered incorrect. Given the human annotated relation phrase and the automatically extracted relation phrase, we can measure the similarity between the two, thus alleviating the problem of coarse annotation in binary judgments. For evaluation, we first report the percentage of valid arguments. Then for sentences with valid arguments, we use smoothed sentence-level BLEU score (max n-gram order = 3) to measure the similarity of the automatically extracted relation relative to the human annotated relation.5 Results. We extracted relations from the entire Wikipedia6 corpus in Russian, French and Hindi from all sentences whose lengths are in the range of 10 - 30 words. We randomly selected 1, 000 relations for each of these languages and annotated them. The results are shown in table 1. The percentage of valid extractions is highest in French (81.6%)
We obtained two annotations for  300 Russian sentences. Between the two annotations, the perfect agreement rate was 74.5% and the average BLEU score was 0.85. 6 www.wikipedia.org
5

3

Experiments

Evaluation for open relations is a difficult task with no standard evaluation datasets. We first describe the construction of our multilingual relation extraction dataset and then present the experiments. Annotation. The current approach to evaluation for open relations (Fader et al., 2011; Mausam et al., 2012) is to extract relations from a sentence and manually annotate each relation as either valid (1) or invalid (0) for the sentence. For exam1353

